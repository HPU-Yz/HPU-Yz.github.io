<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  
    <meta name="keywords" content="Hexo,博客，python，C语言,机器学习，人工智能">
  
  
    <meta name="description" content="Life may sometimes have regret,but the future is still good.Since the choice of the distance,will only trials and hardships.">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    梯度下降法的三种形式BGD、SGD、MBGD及python实现 |
    
    Flaneur</title>
  
    <link rel="shortcut icon" href="/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  
    <link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">
  
  <script src="/js/pace.min.js"></script>
</head>
</html>
<body>
<main class="content">
  <section class="outer">
  <article id="post-梯度下降法的三种形式BGD、SGD、MBGD及python实现" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      梯度下降法的三种形式BGD、SGD、MBGD及python实现
    </h1>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2019/07/18/梯度下降法的三种形式BGD、SGD、MBGD及python实现/" class="article-date">
  <time datetime="2019-07-17T16:00:00.000Z" itemprop="datePublished">2019-07-18</time>
</a>
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/梯度下降/">梯度下降</a> / <a class="article-category-link" href="/categories/梯度下降/机器学习/">机器学习</a> / <a class="article-category-link" href="/categories/梯度下降/机器学习/python/">python</a>
  </div>

      </div>
    

    <div class="article-entry" itemprop="articleBody">
      

      

      
        <meta name="referrer" content="no-referrer">

<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;梯度下降法作为机器学习中较常使用的优化算法，其有着三种不同的形式：批量梯度下降（Batch Gradient Descent）、随机梯度下降（Stochastic Gradient Descent）以及小批量梯度下降（Mini-Batch Gradient Descent）。其中小批量梯度下降法也常用在深度学习中进行模型的训练。接下来，我们将对这三种不同的梯度下降法进行理解。<br><a id="more"></a><br>为了便于理解，这里我们将使用只含有一个特征的线性回归来展开。</p>
<p>此时线性回归的假设函数为：<br><img src="https://img-blog.csdnimg.cn/20190725100634816.png" alt><br>对应的<strong>目标函数（代价函数）</strong>即为：<br> <img src="https://img-blog.csdnimg.cn/20190725085658136.png" alt><br>下图为 J(θ0,θ1)与参数 θ0,θ1 的关系的图：<br><img src="https://img-blog.csdnimg.cn/20190725090156368.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt></p>
<h2 id="1、批量梯度下降（Batch-Gradient-Descent，BGD）"><a href="#1、批量梯度下降（Batch-Gradient-Descent，BGD）" class="headerlink" title="1、批量梯度下降（Batch Gradient Descent，BGD）"></a>1、批量梯度下降（Batch Gradient Descent，BGD）</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>批量梯度下降法</strong>是最原始的形式，它是指在<strong>每一次迭代时</strong>使用<strong>所有样本</strong>来进行梯度的更新。从数学上理解如下：<br>  （1）对目标函数求偏导：<br>  <img src="https://img-blog.csdnimg.cn/20190725085839458.png" alt><br>  其中 i=1,2,…,m 表示样本数， j=0,1 表示特征数，这里我们使用了偏置项 x(i)0=1。<br>  （2）每次迭代对参数进行更新：<br>  <img src="https://img-blog.csdnimg.cn/2019072508594192.png" alt><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>注意这里更新时存在一个求和函数，即为对所有样本进行计算处理，可与下文SGD法进行比较。</strong><br>  伪代码形式为：<br>  <img src="https://img-blog.csdnimg.cn/201907250859587.png" alt><br>  <strong>优点</strong>：<br>  （1）一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。<br>  （2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。<br>  <strong>缺点</strong>：<br>  （1）当样本数目 mm 很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。<br>  从迭代的次数上来看，BGD迭代的次数相对较少。其迭代的收敛曲线示意图可以表示如下：<br>  <img src="https://img-blog.csdnimg.cn/20190725090052713.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt></p>
<h2 id="批量梯度下降法的python实现"><a href="#批量梯度下降法的python实现" class="headerlink" title="批量梯度下降法的python实现"></a>批量梯度下降法的python实现</h2><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">import</span> matplotlib.pyplot as plt</span><br><span class="line"><span class="built_in">import</span> random</span><br><span class="line"><span class="comment">##样本数据</span></span><br><span class="line"><span class="attr">x_train</span> = [<span class="number">150</span>,<span class="number">200</span>,<span class="number">250</span>,<span class="number">300</span>,<span class="number">350</span>,<span class="number">400</span>,<span class="number">600</span>]</span><br><span class="line"><span class="attr">y_train</span> = [<span class="number">6450</span>,<span class="number">7450</span>,<span class="number">8450</span>,<span class="number">9450</span>,<span class="number">11450</span>,<span class="number">15450</span>,<span class="number">18450</span>]</span><br><span class="line"><span class="comment">#样本个数</span></span><br><span class="line"><span class="attr">m</span> = len(x_train)</span><br><span class="line"><span class="comment">#步长</span></span><br><span class="line"><span class="attr">alpha</span> = <span class="number">0.00001</span></span><br><span class="line"><span class="comment">#循环次数</span></span><br><span class="line"><span class="attr">cnt</span> = <span class="number">0</span></span><br><span class="line"><span class="comment">#假设函数为 y=theta0+theta1*x</span></span><br><span class="line">def h(x):</span><br><span class="line">    return theta0 + theta1*x</span><br><span class="line"><span class="attr">theta0</span> = <span class="number">0</span></span><br><span class="line"><span class="attr">theta1</span> = <span class="number">0</span></span><br><span class="line"><span class="comment">#导数</span></span><br><span class="line"><span class="attr">diff0=0</span></span><br><span class="line"><span class="attr">diff1=0</span></span><br><span class="line"><span class="comment">#误差</span></span><br><span class="line"><span class="attr">error0=0</span>           </span><br><span class="line"><span class="attr">error1=0</span>          </span><br><span class="line"><span class="comment">#每次迭代theta的值</span></span><br><span class="line"><span class="attr">retn0</span> = []         </span><br><span class="line"><span class="attr">retn1</span> = []         </span><br><span class="line"></span><br><span class="line"><span class="comment">#退出迭代的条件</span></span><br><span class="line"><span class="attr">epsilon=0.00001</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#批量梯度下降</span></span><br><span class="line">while <span class="number">1</span>:</span><br><span class="line">    <span class="attr">cnt=cnt+1</span></span><br><span class="line">    <span class="attr">diff0=0</span></span><br><span class="line">    <span class="attr">diff1=0</span></span><br><span class="line">    <span class="comment">#梯度下降</span></span><br><span class="line">    for i <span class="keyword">in</span> range(m):</span><br><span class="line">        diff0+=h(x_train[i])-y_train[i]</span><br><span class="line">        diff1+=(h(x_train[i])-y_train[i])*x_train[i]</span><br><span class="line">    <span class="attr">theta0=theta0-alpha/m*diff0</span></span><br><span class="line">    <span class="attr">theta1=theta1-alpha/m*diff1</span></span><br><span class="line">    retn0.append(theta0)</span><br><span class="line">    retn1.append(theta1)</span><br><span class="line">    <span class="attr">error1=0</span></span><br><span class="line">    <span class="comment">#计算迭代误差</span></span><br><span class="line">    for i <span class="keyword">in</span> range(len(x_train)):</span><br><span class="line">        error1 += ((theta0 + theta1 * x_train[i])-y_train[i]) ** <span class="number">2</span> / <span class="number">2</span></span><br><span class="line">    <span class="comment">#判断是否已收敛</span></span><br><span class="line">    <span class="keyword">if</span> abs(error1 - error0) &lt; epsilon:</span><br><span class="line">        break</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="attr">error0</span> = error1</span><br><span class="line"><span class="comment"># 画图表现</span></span><br><span class="line">plt.title('BGD')</span><br><span class="line">plt.plot(range(len(retn0)),retn0,<span class="attr">label='theta0')</span></span><br><span class="line">plt.plot(range(len(retn1)),retn1,<span class="attr">label='theta1')</span></span><br><span class="line">plt.legend()          <span class="comment">#显示上面的label</span></span><br><span class="line">plt.xlabel('time')</span><br><span class="line">plt.ylabel('theta')</span><br><span class="line">plt.show()</span><br><span class="line">plt.plot(x_train,y_train,'bo')</span><br><span class="line">plt.plot(x_train,[h(x) for x <span class="keyword">in</span> x_train],<span class="attr">color='k',label='BGD')</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel('area')</span><br><span class="line">plt.ylabel('price')</span><br><span class="line">print(<span class="string">"批量梯度下降法：theta0=&#123;&#125;,theta1=&#123;&#125;"</span>.format(theta0,theta1))</span><br><span class="line">print(<span class="string">"批量梯度下降法循环次数：&#123;&#125;"</span>.format(cnt))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="2、随机梯度下降（Stochastic-Gradient-Descent，SGD）"><a href="#2、随机梯度下降（Stochastic-Gradient-Descent，SGD）" class="headerlink" title="2、随机梯度下降（Stochastic Gradient Descent，SGD）"></a>2、随机梯度下降（Stochastic Gradient Descent，SGD）</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&amp;nbsp<strong>;随机梯度下降法</strong>不同于批量梯度下降，随机梯度下降是<strong>每次迭代</strong>使用<strong>一个样本</strong>来对参数进行更新。使得训练速度加快。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于一个样本的目标函数为：<br>  <img src="https://img-blog.csdnimg.cn/20190725090216938.png" alt><br>  （1）对目标函数求偏导：<br>  <img src="https://img-blog.csdnimg.cn/20190725090246509.png" alt><br>  （2）参数更新：<br>  <img src="https://img-blog.csdnimg.cn/20190725090315940.png" alt><br>  <strong>注意，这里不再有求和符号</strong><br>  伪代码形式为：<br>  <img src="https://img-blog.csdnimg.cn/20190725090345289.png" alt><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>优点</strong>：<br>  （1）由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>缺点</strong>：<br>  （1）准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛。<br>  （2）可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势。<br>  （3）不易于并行实现。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>解释一下为什么SGD收敛速度比BGD要快：</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;答：这里我们假设有30W个样本，对于BGD而言，每次迭代需要计算30W个样本才能对参数进行一次更新，需要求得最小值可能需要多次迭代（假设这里是10）；而对于SGD，每次更新参数只需要一个样本，因此若使用这30W个样本进行参数更新，则参数会被更新（迭代）30W次，而这期间，SGD就能保证能够收敛到一个合适的最小值上了。也就是说，在收敛时，BGD计算了 10×30W 次，而SGD只计算了 1×30W 次。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从迭代的次数上来看，SGD迭代的次数较少，在解空间的搜索过程看起来很盲目。其迭代的收敛曲线示意图可以表示如下：<br>  <img src="https://img-blog.csdnimg.cn/20190725090459507.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="随机梯度下降法的python实现"><a href="#随机梯度下降法的python实现" class="headerlink" title="随机梯度下降法的python实现"></a>随机梯度下降法的python实现</h2><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">import</span> matplotlib.pyplot as plt</span><br><span class="line"><span class="built_in">import</span> random</span><br><span class="line"><span class="comment">##样本数据</span></span><br><span class="line"><span class="attr">x_train</span> = [<span class="number">150</span>,<span class="number">200</span>,<span class="number">250</span>,<span class="number">300</span>,<span class="number">350</span>,<span class="number">400</span>,<span class="number">600</span>]</span><br><span class="line"><span class="attr">y_train</span> = [<span class="number">6450</span>,<span class="number">7450</span>,<span class="number">8450</span>,<span class="number">9450</span>,<span class="number">11450</span>,<span class="number">15450</span>,<span class="number">18450</span>]</span><br><span class="line"><span class="comment">#样本个数</span></span><br><span class="line"><span class="attr">m</span> = len(x_train)</span><br><span class="line"><span class="comment">#步长</span></span><br><span class="line"><span class="attr">alpha</span> = <span class="number">0.00001</span></span><br><span class="line"><span class="comment">#循环次数</span></span><br><span class="line"><span class="attr">cnt</span> = <span class="number">0</span></span><br><span class="line"><span class="comment">#假设函数为 y=theta0+theta1*x</span></span><br><span class="line">def h(x):</span><br><span class="line">    return theta0 + theta1*x</span><br><span class="line"><span class="attr">theta0</span> = <span class="number">0</span></span><br><span class="line"><span class="attr">theta1</span> = <span class="number">0</span></span><br><span class="line"><span class="comment">#导数</span></span><br><span class="line"><span class="attr">diff0=0</span></span><br><span class="line"><span class="attr">diff1=0</span></span><br><span class="line"><span class="comment">#误差</span></span><br><span class="line"><span class="attr">error0=0</span>           </span><br><span class="line"><span class="attr">error1=0</span>          </span><br><span class="line"><span class="comment">#每次迭代theta的值</span></span><br><span class="line"><span class="attr">retn0</span> = []         </span><br><span class="line"><span class="attr">retn1</span> = []         </span><br><span class="line"></span><br><span class="line"><span class="comment">#退出迭代的条件</span></span><br><span class="line"><span class="attr">epsilon=0.00001</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#随机梯度下降</span></span><br><span class="line">for i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    <span class="attr">cnt=cnt+1</span></span><br><span class="line">    <span class="attr">diff0=0</span></span><br><span class="line">    <span class="attr">diff1=0</span></span><br><span class="line">    <span class="attr">j</span> = random.randint(<span class="number">0</span>, m - <span class="number">1</span>)</span><br><span class="line">    <span class="attr">diff0=h(x_train[j])-y_train[j]</span></span><br><span class="line">    <span class="attr">diff1=(h(x_train[j])-y_train[j])*x_train[j]</span></span><br><span class="line">    <span class="attr">theta0=theta0-alpha/m*diff0</span></span><br><span class="line">    <span class="attr">theta1=theta1-alpha/m*diff1</span></span><br><span class="line">    retn0.append(theta0)</span><br><span class="line">    retn1.append(theta1)</span><br><span class="line">    <span class="attr">error1=0</span></span><br><span class="line">    <span class="comment">#计算迭代的误差</span></span><br><span class="line">    for i <span class="keyword">in</span> range(len(x_train)):</span><br><span class="line">        error1 += ((theta0 + theta1 * x_train[i])-y_train[i]) ** <span class="number">2</span> / <span class="number">2</span></span><br><span class="line">    <span class="comment">#判断是否已收敛</span></span><br><span class="line">    <span class="keyword">if</span> abs(error1 - error0) &lt; epsilon:</span><br><span class="line">        break</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="attr">error0</span> = error1</span><br><span class="line"><span class="comment"># 画图表现        </span></span><br><span class="line">plt.title('SGD')</span><br><span class="line">plt.plot(range(len(retn0)),retn0,<span class="attr">label='theta0')</span></span><br><span class="line">plt.plot(range(len(retn1)),retn1,<span class="attr">label='theta1')</span></span><br><span class="line">plt.legend()          <span class="comment">#显示上面的label</span></span><br><span class="line">plt.xlabel('time')</span><br><span class="line">plt.ylabel('theta')</span><br><span class="line">plt.show()</span><br><span class="line">plt.plot(x_train,y_train,'bo')</span><br><span class="line">plt.plot(x_train,[h(x) for x <span class="keyword">in</span> x_train],<span class="attr">color='k',label='SGD')</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel('area')</span><br><span class="line">plt.ylabel('price')</span><br><span class="line">print(<span class="string">"随机梯度下降法：theta0=&#123;&#125;,theta1=&#123;&#125;"</span>.format(theta0,theta1))</span><br><span class="line">print(<span class="string">"随机梯度下降法循环次数：&#123;&#125;"</span>.format(cnt))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="3、小批量梯度下降（Mini-Batch-Gradient-Descent-MBGD）"><a href="#3、小批量梯度下降（Mini-Batch-Gradient-Descent-MBGD）" class="headerlink" title="3、小批量梯度下降（Mini-Batch Gradient Descent, MBGD）"></a>3、小批量梯度下降（Mini-Batch Gradient Descent, MBGD）</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>小批量梯度下降</strong>，是对批量梯度下降以及随机梯度下降的一个折中办法。其思想是：<strong>每次迭代</strong> 使用 <strong>batch_size 个样本</strong>来对参数进行更新。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里我们假设 batchsize=10，样本数 m=1000 。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;伪代码形式为：<br>  <img src="https://img-blog.csdnimg.cn/20190725090543904.png" alt><br>  <strong>优点</strong>：<br>  （1）通过矩阵运算，每次在一个batch上优化神经网络参数并不会比单个数据慢太多。<br>  （2）每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果。(比如上例中的30W，设置batch_size=100时，需要迭代3000次，远小于SGD的30W次)<br>  （3）可实现并行化。<br>  <strong>缺点</strong>：<br>  （1）batch_size的不当选择可能会带来一些问题。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>batcha_size的选择带来的影响：</strong><br>  （1）在合理地范围内，增大batch_size的好处：<br>    a. 内存利用率提高了，大矩阵乘法的并行化效率提高。<br>    b. 跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。<br>    c. 在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。<br>  （2）盲目增大batch_size的坏处：<br>    a. 内存利用率提高了，但是内存容量可能撑不住了。<br>    b. 跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。<br>    c. Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下图显示了三种梯度下降算法的收敛过程：<br>  <img src="https://img-blog.csdnimg.cn/20190725090635209.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt></p>
<h2 id="4、总结"><a href="#4、总结" class="headerlink" title="4、总结"></a>4、总结</h2><p><strong>Batch gradient descent:</strong> Use all examples in each iteration；</p>
<p><strong>Stochastic gradient descent:</strong> Use 1 example in each iteration；</p>
<p><strong>Mini-batch gradient descent:</strong> Use b examples in each iteration.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://hpu-yz.github.io/2019/07/18/梯度下降法的三种形式BGD、SGD、MBGD及python实现/" data-id="ck0256sc1003zy0uh3kyz3s5k" class="article-share-link">分享</a>
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/梯度下降/">梯度下降</a></li></ul>

    </footer>

  </div>

  
    
  <nav class="article-nav">
    
      <a href="/2019/07/19/K-means聚类算法原理及python实现/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            K-means算法及python实现
          
        </div>
      </a>
    
    
      <a href="/2019/07/18/数据集的划分--训练集、验证集和测试集/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">数据集的划分--训练集、验证集和测试集</div>
      </a>
    
  </nav>


  

  
    
  

</article>



</section>
  <footer class="footer">
  <div class="outer">
    <div class="float-right">
      <ul class="list-inline">
  
    <li><i class="fe fe-smile-alt"></i> <span id="busuanzi_value_site_uv"></span></li>
  
</ul>
    </div>
    <ul class="list-inline">
      <li>&copy; 2019 Flaneur</li>
      <li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
      <li>Theme  <a href="https://github.com/zhwangart/hexo-theme-ocean">Ocean</a></li>
    </ul>
  </div>
</footer>

</main>
<aside class="sidebar">
  <button class="navbar-toggle"></button>

<nav class="navbar">
  
    <div class="logo">
      <a href="/"><img src="/images/favicon.ico" alt="Flaneur"></a>
    </div>
  
  <ul class="nav nav-main">
    
      <li class="nav-item">
        <a class="nav-item-link" href="/">主页</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/archives">归档</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/gallery">相册</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/about">关于</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/tags">友链</a>
      </li>
    
    <li class="nav-item">
      <a class="nav-item-link nav-item-search" title="搜索">
        <i class="fe fe-search"></i>
        搜索
      </a>
    </li>
  </ul>
</nav>

<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      <div class="totop" id="totop">
  <i class="fe fe-rocket"></i>
</div>
    </li>
    <li class="nav-item">
      
        <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
          <i class="fe fe-feed"></i>
        </a>
      
    </li>
  </ul>
</nav>

<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
</aside>
<script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/lazyload.min.js"></script>
<script src="/js/snap.svg-min.js"></script>
<script src="/js/busuanzi-2.3.pure.min.js"></script>


  <script src="/fancybox/jquery.fancybox.min.js"></script>



  <script src="/js/search.js"></script>


<script src="/js/ocean.js"></script>

</body>
</html>