<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Best-yz"><title>梯度下降法 · Best-yz</title><meta name="description" content="&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;梯度下降法作为机器学习中较常使用的优化算法，其有着三种不同的形式：批量梯度下降（Batch Gradient Descent）、随机梯度下降（Stochastic Gradient Descent）以及小"><meta name="keywords" content="Flaneur博客，Best-yz博客"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script src="/js/jquery.js"></script><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:127px;"><h3 title><a href="/">Best-yz</a></h3><div class="description"><p>三分热度 <br> Nothing is impossible to a willing heart.</p></div></div></div><ul class="social-links"><li><a href="https://github.com/HPU-Yz"><i class="fa fa-github"></i></a></li><li><a href="mailto:2426545812@qq.com"><i class="fa fa-envelope"></i></a></li><li><a href="http://sighttp.qq.com/authd?IDKEY=da97a3288889b7c35a1e6d914e0cb28bda9a83ec1e77cf7b"><i class="fa fa-qq"></i></a></li></ul><div class="footer"><div class="p"> <span>© 2019 - 2020 </span><i class="fa fa-star"></i><span> Best-yz</span></div><div class="by_farbox"><span>Powered by </span><a href="https://hexo.io/zh-cn/" target="_blank">Hexo </a><span> </span><a href="http://www.beian.miit.gov.cn/" target="_blank">&nbsp;</a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">首页</a></li><li><a href="/archives">归档</a></li><li><a href="/tags">标签</a></li><li><a href="/about">关于</a></li><li><a href="/guestbook">留言</a></li><li><a href="/links">友链</a></li><li><a href="/shuoshuo">说说</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>梯度下降法</a></h3></div><div class="post-content"><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;梯度下降法作为机器学习中较常使用的优化算法，其有着三种不同的形式：批量梯度下降（Batch Gradient Descent）、随机梯度下降（Stochastic Gradient Descent）以及小批量梯度下降（Mini-Batch Gradient Descent）。其中小批量梯度下降法也常用在深度学习中进行模型的训练。接下来，我们将对这三种不同的梯度下降法进行理解。</p>
<p>为了便于理解，这里我们将使用只含有一个特征的线性回归来展开。</p>
<p>此时线性回归的假设函数为：<br><img src="https://img-blog.csdnimg.cn/20190725100634816.png" alt><br>对应的<strong>目标函数（代价函数）</strong>即为：<br> <img src="https://img-blog.csdnimg.cn/20190725085658136.png" alt><br>下图为 J(θ0,θ1)与参数 θ0,θ1 的关系的图：<br><img src="https://img-blog.csdnimg.cn/20190725090156368.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt></p>
<h2 id="1、批量梯度下降（Batch-Gradient-Descent，BGD）"><a href="#1、批量梯度下降（Batch-Gradient-Descent，BGD）" class="headerlink" title="1、批量梯度下降（Batch Gradient Descent，BGD）"></a>1、批量梯度下降（Batch Gradient Descent，BGD）</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>批量梯度下降法</strong>是最原始的形式，它是指在<strong>每一次迭代时</strong>使用<strong>所有样本</strong>来进行梯度的更新。从数学上理解如下：<br>  （1）对目标函数求偏导：<br>  <img src="https://img-blog.csdnimg.cn/20190725085839458.png" alt><br>  其中 i=1,2,…,m 表示样本数， j=0,1 表示特征数，这里我们使用了偏置项 x(i)0=1。<br>  （2）每次迭代对参数进行更新：<br>  <img src="https://img-blog.csdnimg.cn/2019072508594192.png" alt><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>注意这里更新时存在一个求和函数，即为对所有样本进行计算处理，可与下文SGD法进行比较。</strong><br>  伪代码形式为：<br>  <img src="https://img-blog.csdnimg.cn/201907250859587.png" alt><br>  <strong>优点</strong>：<br>  （1）一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。<br>  （2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。<br>  <strong>缺点</strong>：<br>  （1）当样本数目 mm 很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。<br>  从迭代的次数上来看，BGD迭代的次数相对较少。其迭代的收敛曲线示意图可以表示如下：<br>  <img src="https://img-blog.csdnimg.cn/20190725090052713.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt></p>
<h2 id="批量梯度下降法的python实现"><a href="#批量梯度下降法的python实现" class="headerlink" title="批量梯度下降法的python实现"></a>批量梯度下降法的python实现</h2><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">import</span> matplotlib.pyplot as plt</span><br><span class="line"><span class="built_in">import</span> random</span><br><span class="line"><span class="comment">##样本数据</span></span><br><span class="line"><span class="attr">x_train</span> = [<span class="number">150</span>,<span class="number">200</span>,<span class="number">250</span>,<span class="number">300</span>,<span class="number">350</span>,<span class="number">400</span>,<span class="number">600</span>]</span><br><span class="line"><span class="attr">y_train</span> = [<span class="number">6450</span>,<span class="number">7450</span>,<span class="number">8450</span>,<span class="number">9450</span>,<span class="number">11450</span>,<span class="number">15450</span>,<span class="number">18450</span>]</span><br><span class="line"><span class="comment">#样本个数</span></span><br><span class="line"><span class="attr">m</span> = len(x_train)</span><br><span class="line"><span class="comment">#步长</span></span><br><span class="line"><span class="attr">alpha</span> = <span class="number">0.00001</span></span><br><span class="line"><span class="comment">#循环次数</span></span><br><span class="line"><span class="attr">cnt</span> = <span class="number">0</span></span><br><span class="line"><span class="comment">#假设函数为 y=theta0+theta1*x</span></span><br><span class="line">def h(x):</span><br><span class="line">    return theta0 + theta1*x</span><br><span class="line"><span class="attr">theta0</span> = <span class="number">0</span></span><br><span class="line"><span class="attr">theta1</span> = <span class="number">0</span></span><br><span class="line"><span class="comment">#导数</span></span><br><span class="line"><span class="attr">diff0=0</span></span><br><span class="line"><span class="attr">diff1=0</span></span><br><span class="line"><span class="comment">#误差</span></span><br><span class="line"><span class="attr">error0=0</span>           </span><br><span class="line"><span class="attr">error1=0</span>          </span><br><span class="line"><span class="comment">#每次迭代theta的值</span></span><br><span class="line"><span class="attr">retn0</span> = []         </span><br><span class="line"><span class="attr">retn1</span> = []         </span><br><span class="line"></span><br><span class="line"><span class="comment">#退出迭代的条件</span></span><br><span class="line"><span class="attr">epsilon=0.00001</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#批量梯度下降</span></span><br><span class="line">while <span class="number">1</span>:</span><br><span class="line">    <span class="attr">cnt=cnt+1</span></span><br><span class="line">    <span class="attr">diff0=0</span></span><br><span class="line">    <span class="attr">diff1=0</span></span><br><span class="line">    <span class="comment">#梯度下降</span></span><br><span class="line">    for i <span class="keyword">in</span> range(m):</span><br><span class="line">        diff0+=h(x_train[i])-y_train[i]</span><br><span class="line">        diff1+=(h(x_train[i])-y_train[i])*x_train[i]</span><br><span class="line">    <span class="attr">theta0=theta0-alpha/m*diff0</span></span><br><span class="line">    <span class="attr">theta1=theta1-alpha/m*diff1</span></span><br><span class="line">    retn0.append(theta0)</span><br><span class="line">    retn1.append(theta1)</span><br><span class="line">    <span class="attr">error1=0</span></span><br><span class="line">    <span class="comment">#计算迭代误差</span></span><br><span class="line">    for i <span class="keyword">in</span> range(len(x_train)):</span><br><span class="line">        error1 += ((theta0 + theta1 * x_train[i])-y_train[i]) ** <span class="number">2</span> / <span class="number">2</span></span><br><span class="line">    <span class="comment">#判断是否已收敛</span></span><br><span class="line">    <span class="keyword">if</span> abs(error1 - error0) &lt; epsilon:</span><br><span class="line">        break</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="attr">error0</span> = error1</span><br><span class="line"><span class="comment"># 画图表现</span></span><br><span class="line">plt.title('BGD')</span><br><span class="line">plt.plot(range(len(retn0)),retn0,<span class="attr">label='theta0')</span></span><br><span class="line">plt.plot(range(len(retn1)),retn1,<span class="attr">label='theta1')</span></span><br><span class="line">plt.legend()          <span class="comment">#显示上面的label</span></span><br><span class="line">plt.xlabel('time')</span><br><span class="line">plt.ylabel('theta')</span><br><span class="line">plt.show()</span><br><span class="line">plt.plot(x_train,y_train,'bo')</span><br><span class="line">plt.plot(x_train,[h(x) for x <span class="keyword">in</span> x_train],<span class="attr">color='k',label='BGD')</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel('area')</span><br><span class="line">plt.ylabel('price')</span><br><span class="line">print(<span class="string">"批量梯度下降法：theta0=&#123;&#125;,theta1=&#123;&#125;"</span>.format(theta0,theta1))</span><br><span class="line">print(<span class="string">"批量梯度下降法循环次数：&#123;&#125;"</span>.format(cnt))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="2、随机梯度下降（Stochastic-Gradient-Descent，SGD）"><a href="#2、随机梯度下降（Stochastic-Gradient-Descent，SGD）" class="headerlink" title="2、随机梯度下降（Stochastic Gradient Descent，SGD）"></a>2、随机梯度下降（Stochastic Gradient Descent，SGD）</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&amp;nbsp<strong>;随机梯度下降法</strong>不同于批量梯度下降，随机梯度下降是<strong>每次迭代</strong>使用<strong>一个样本</strong>来对参数进行更新。使得训练速度加快。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于一个样本的目标函数为：<br>  <img src="https://img-blog.csdnimg.cn/20190725090216938.png" alt><br>  （1）对目标函数求偏导：<br>  <img src="https://img-blog.csdnimg.cn/20190725090246509.png" alt><br>  （2）参数更新：<br>  <img src="https://img-blog.csdnimg.cn/20190725090315940.png" alt><br>  <strong>注意，这里不再有求和符号</strong><br>  伪代码形式为：<br>  <img src="https://img-blog.csdnimg.cn/20190725090345289.png" alt><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>优点</strong>：<br>  （1）由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>缺点</strong>：<br>  （1）准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛。<br>  （2）可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势。<br>  （3）不易于并行实现。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>解释一下为什么SGD收敛速度比BGD要快：</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;答：这里我们假设有30W个样本，对于BGD而言，每次迭代需要计算30W个样本才能对参数进行一次更新，需要求得最小值可能需要多次迭代（假设这里是10）；而对于SGD，每次更新参数只需要一个样本，因此若使用这30W个样本进行参数更新，则参数会被更新（迭代）30W次，而这期间，SGD就能保证能够收敛到一个合适的最小值上了。也就是说，在收敛时，BGD计算了 10×30W 次，而SGD只计算了 1×30W 次。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从迭代的次数上来看，SGD迭代的次数较少，在解空间的搜索过程看起来很盲目。其迭代的收敛曲线示意图可以表示如下：<br>  <img src="https://img-blog.csdnimg.cn/20190725090459507.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="随机梯度下降法的python实现"><a href="#随机梯度下降法的python实现" class="headerlink" title="随机梯度下降法的python实现"></a>随机梯度下降法的python实现</h2><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">import</span> matplotlib.pyplot as plt</span><br><span class="line"><span class="built_in">import</span> random</span><br><span class="line"><span class="comment">##样本数据</span></span><br><span class="line"><span class="attr">x_train</span> = [<span class="number">150</span>,<span class="number">200</span>,<span class="number">250</span>,<span class="number">300</span>,<span class="number">350</span>,<span class="number">400</span>,<span class="number">600</span>]</span><br><span class="line"><span class="attr">y_train</span> = [<span class="number">6450</span>,<span class="number">7450</span>,<span class="number">8450</span>,<span class="number">9450</span>,<span class="number">11450</span>,<span class="number">15450</span>,<span class="number">18450</span>]</span><br><span class="line"><span class="comment">#样本个数</span></span><br><span class="line"><span class="attr">m</span> = len(x_train)</span><br><span class="line"><span class="comment">#步长</span></span><br><span class="line"><span class="attr">alpha</span> = <span class="number">0.00001</span></span><br><span class="line"><span class="comment">#循环次数</span></span><br><span class="line"><span class="attr">cnt</span> = <span class="number">0</span></span><br><span class="line"><span class="comment">#假设函数为 y=theta0+theta1*x</span></span><br><span class="line">def h(x):</span><br><span class="line">    return theta0 + theta1*x</span><br><span class="line"><span class="attr">theta0</span> = <span class="number">0</span></span><br><span class="line"><span class="attr">theta1</span> = <span class="number">0</span></span><br><span class="line"><span class="comment">#导数</span></span><br><span class="line"><span class="attr">diff0=0</span></span><br><span class="line"><span class="attr">diff1=0</span></span><br><span class="line"><span class="comment">#误差</span></span><br><span class="line"><span class="attr">error0=0</span>           </span><br><span class="line"><span class="attr">error1=0</span>          </span><br><span class="line"><span class="comment">#每次迭代theta的值</span></span><br><span class="line"><span class="attr">retn0</span> = []         </span><br><span class="line"><span class="attr">retn1</span> = []         </span><br><span class="line"></span><br><span class="line"><span class="comment">#退出迭代的条件</span></span><br><span class="line"><span class="attr">epsilon=0.00001</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#随机梯度下降</span></span><br><span class="line">for i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    <span class="attr">cnt=cnt+1</span></span><br><span class="line">    <span class="attr">diff0=0</span></span><br><span class="line">    <span class="attr">diff1=0</span></span><br><span class="line">    <span class="attr">j</span> = random.randint(<span class="number">0</span>, m - <span class="number">1</span>)</span><br><span class="line">    <span class="attr">diff0=h(x_train[j])-y_train[j]</span></span><br><span class="line">    <span class="attr">diff1=(h(x_train[j])-y_train[j])*x_train[j]</span></span><br><span class="line">    <span class="attr">theta0=theta0-alpha/m*diff0</span></span><br><span class="line">    <span class="attr">theta1=theta1-alpha/m*diff1</span></span><br><span class="line">    retn0.append(theta0)</span><br><span class="line">    retn1.append(theta1)</span><br><span class="line">    <span class="attr">error1=0</span></span><br><span class="line">    <span class="comment">#计算迭代的误差</span></span><br><span class="line">    for i <span class="keyword">in</span> range(len(x_train)):</span><br><span class="line">        error1 += ((theta0 + theta1 * x_train[i])-y_train[i]) ** <span class="number">2</span> / <span class="number">2</span></span><br><span class="line">    <span class="comment">#判断是否已收敛</span></span><br><span class="line">    <span class="keyword">if</span> abs(error1 - error0) &lt; epsilon:</span><br><span class="line">        break</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="attr">error0</span> = error1</span><br><span class="line"><span class="comment"># 画图表现        </span></span><br><span class="line">plt.title('SGD')</span><br><span class="line">plt.plot(range(len(retn0)),retn0,<span class="attr">label='theta0')</span></span><br><span class="line">plt.plot(range(len(retn1)),retn1,<span class="attr">label='theta1')</span></span><br><span class="line">plt.legend()          <span class="comment">#显示上面的label</span></span><br><span class="line">plt.xlabel('time')</span><br><span class="line">plt.ylabel('theta')</span><br><span class="line">plt.show()</span><br><span class="line">plt.plot(x_train,y_train,'bo')</span><br><span class="line">plt.plot(x_train,[h(x) for x <span class="keyword">in</span> x_train],<span class="attr">color='k',label='SGD')</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel('area')</span><br><span class="line">plt.ylabel('price')</span><br><span class="line">print(<span class="string">"随机梯度下降法：theta0=&#123;&#125;,theta1=&#123;&#125;"</span>.format(theta0,theta1))</span><br><span class="line">print(<span class="string">"随机梯度下降法循环次数：&#123;&#125;"</span>.format(cnt))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="3、小批量梯度下降（Mini-Batch-Gradient-Descent-MBGD）"><a href="#3、小批量梯度下降（Mini-Batch-Gradient-Descent-MBGD）" class="headerlink" title="3、小批量梯度下降（Mini-Batch Gradient Descent, MBGD）"></a>3、小批量梯度下降（Mini-Batch Gradient Descent, MBGD）</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>小批量梯度下降</strong>，是对批量梯度下降以及随机梯度下降的一个折中办法。其思想是：<strong>每次迭代</strong> 使用 <strong>batch_size 个样本</strong>来对参数进行更新。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里我们假设 batchsize=10，样本数 m=1000 。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;伪代码形式为：<br>  <img src="https://img-blog.csdnimg.cn/20190725090543904.png" alt><br>  <strong>优点</strong>：<br>  （1）通过矩阵运算，每次在一个batch上优化神经网络参数并不会比单个数据慢太多。<br>  （2）每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果。(比如上例中的30W，设置batch_size=100时，需要迭代3000次，远小于SGD的30W次)<br>  （3）可实现并行化。<br>  <strong>缺点</strong>：<br>  （1）batch_size的不当选择可能会带来一些问题。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>batcha_size的选择带来的影响：</strong><br>  （1）在合理地范围内，增大batch_size的好处：<br>    a. 内存利用率提高了，大矩阵乘法的并行化效率提高。<br>    b. 跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。<br>    c. 在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。<br>  （2）盲目增大batch_size的坏处：<br>    a. 内存利用率提高了，但是内存容量可能撑不住了。<br>    b. 跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。<br>    c. Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下图显示了三种梯度下降算法的收敛过程：<br>  <img src="https://img-blog.csdnimg.cn/20190725090635209.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt></p>
<h2 id="4、总结"><a href="#4、总结" class="headerlink" title="4、总结"></a>4、总结</h2><p><strong>Batch gradient descent:</strong> Use all examples in each iteration；</p>
<p><strong>Stochastic gradient descent:</strong> Use 1 example in each iteration；</p>
<p><strong>Mini-batch gradient descent:</strong> Use b examples in each iteration.</p>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2019-07-18</span><i class="fa fa-tag"></i><a class="tag" href="/tags/机器学习-梯度下降/" title="机器学习 - 梯度下降">机器学习 - 梯度下降 </a><span class="leancloud_visitors"></span></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href onclick="javascript:join_favorite()" ref="sidebar"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" href="http://twitter.com/home?status=,https://hpu-yz.github.io/2019/07/18/ti-du-xia-jiang-fa-de-san-chong-xing-shi-bgd-sgd-mbgd-ji-python-shi-xian/,Best-yz,梯度下降法,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2019/07/18/shu-ju-ji-de-hua-fen-xun-lian-ji-yan-zheng-ji-he-ce-shi-ji/" title="数据集的划分">上一篇</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2019/04/02/python-chi-san-shu-xue-luo-ji-yan-suan/" title="Python+离散数学→逻辑演算">下一篇</a></li></ul></div><script src="/js/visitors.js"></script><a id="comments"></a><div id="vcomments" style="margin:0 30px;"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//cdn.jsdelivr.net/gh/xcss/valine@latest/dist/Valine.min.js"></script><script>var valine = new Valine({
  el:'#vcomments',
  notify:false || false, 
  verify:false|| false, 
  app_id:'ScFNhrKDxbwGDnwPjE6xGCTm-gzGzoHsz',
  app_key:'Lwnq5xfD0M2LosaGSeDSFeqi',
  placeholder:'念念不忘，必有回响...',
  path: window.location.pathname,
  serverURLs: '',
  visitor:true,
  recordIP:true,
  avatar:'monsterid'
})</script></div></div></div></div><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/add-bookmark.js"></script><script src="/js/baidu-tongji.js"></script></body></html>