<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  
    <meta name="keywords" content="Hexo,博客，python，C语言,机器学习，人工智能">
  
  
    <meta name="description" content="走最远的路，爬最高的山，看最美的风景。">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    K-means算法及python实现 |
    
    Flaneur</title>
  
    <link rel="shortcut icon" href="/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  
    <link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">
  
  <script src="/js/pace.min.js"></script>
</head>
</html>
<body>
<main class="content">
  <section class="outer">
  <article id="post-K-means聚类算法原理及python实现" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      K-means算法及python实现
    </h1>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2019/07/19/K-means聚类算法原理及python实现/" class="article-date">
  <time datetime="2019-07-18T16:00:00.000Z" itemprop="datePublished">2019-07-19</time>
</a>
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a> / <a class="article-category-link" href="/categories/机器学习/K-means/">K-means</a> / <a class="article-category-link" href="/categories/机器学习/K-means/python/">python</a>
  </div>

      </div>
    

    <div class="article-entry" itemprop="articleBody">
      

      

      
        <meta name="referrer" content="no-referrer">

<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;K-means(Thek-meansalgorithm)是机器学习十大经典算法之一，同时也是最为经典的无监督聚类（Unsupervised Clustering）算法。接触聚类算法，首先需要了解k-means算法的实现原理和步骤。本文将对k-means算法的基本原理和实现实例进行分析。<br><a id="more"></a></p>
<h1 id="一-聚类算法的简介"><a href="#一-聚类算法的简介" class="headerlink" title="一.聚类算法的简介"></a>一.聚类算法的简介</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于”<strong>监督学习</strong>“(supervised learning)，其训练样本<strong>是带有标记信息</strong>的，并且监督学习的<strong>目的是</strong>：对带有标记的数据集进行模型学习，从而便于对新的样本进行分类。而在“<strong>无监督学习</strong>”(unsupervised learning)中，训练样本的标记信息是未知的，<strong>目标是</strong>通过对无标记训练样本的学习来揭示数据的内在性质及规律，为进一步的数据分析提供基础。对于无监督学习，应用最广的便是”<strong>聚类</strong>“(clustering)。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;”<strong>聚类算法</strong>“试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个“簇”(cluster)，通过这样的划分，每个簇可能对应于一些潜在的概念或类别。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们可以通过下面这个图来理解：<br>  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="https://img-blog.csdnimg.cn/20190724144117607.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上图是未做标记的样本集，通过他们的分布，我们很容易对上图中的样本做出以下几种划分。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当需要将其划分为两个簇时，即 k=2时：<br><img src="https://img-blog.csdnimg.cn/20190726155350351.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当需要将其划分为四个簇时，即 k=4 时：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="https://img-blog.csdnimg.cn/20190724144431802.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt></p>
<h1 id="二-K-means聚类算法"><a href="#二-K-means聚类算法" class="headerlink" title="二.K-means聚类算法"></a>二.K-means聚类算法</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kmeans算法又名k均值算法,K-means算法中的k表示的是聚类为k个簇，means代表取每一个聚类中数据值的均值作为该簇的中心，或者称为质心，即用每一个的类的质心对该簇进行描述。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其算法思想大致为：先从样本集中随机选取 k个样本作为簇中心，并计算所有样本与这 k个“簇中心”的距离，对于每一个样本，将其划分到与其距离最近的“簇中心”所在的簇中，对于新的簇计算各个簇的新的“簇中心”。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;根据以上描述，我们大致可以猜测到实现kmeans算法的主要四点：<br>  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（1）簇个数 k 的选择<br>  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（2）各个样本点到“簇中心”的距离<br>  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（3）根据新划分的簇，更新“簇中心”<br>  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（4）重复上述2、3过程，直至”簇中心”没有移动<br>    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;优缺点：</p>
<ul>
<li>优点：容易实现</li>
<li>缺点：可能收敛到局部最小值，在大规模数据上收敛较慢</li>
</ul>
<h1 id="三-K-means算法步骤详解"><a href="#三-K-means算法步骤详解" class="headerlink" title="三.K-means算法步骤详解"></a>三.K-means算法步骤详解</h1><h2 id="Step1-K值的选择"><a href="#Step1-K值的选择" class="headerlink" title="Step1.K值的选择"></a>Step1.K值的选择</h2><p> k 的选择一般是按照实际需求进行决定，或在实现算法时直接给定 k 值。</p>
<blockquote>
<p>说明：<br> <strong>A</strong>.质心数量由用户给出，记为k，k-means最终得到的簇数量也是k<br> <strong>B</strong>.后来每次更新的质心的个数都和初始k值相等<br> <strong>C</strong>.k-means最后聚类的簇个数和用户指定的质心个数相等，一个质心对应一个簇，每个样本只聚类到一个簇里面<br> <strong>D</strong>.初始簇为空</p>
</blockquote>
<h2 id="Step2-距离度量"><a href="#Step2-距离度量" class="headerlink" title="Step2.距离度量"></a>Step2.距离度量</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;将对象点分到距离聚类中心最近的那个簇中需要最近邻的度量策略，在欧式空间中采用的是欧式距离，在处理文档中采用的是余弦相似度函数，有时候也采用曼哈顿距离作为度量，不同的情况实用的度量公式是不同的。 </p>
<h3 id="2-1-欧式距离"><a href="#2-1-欧式距离" class="headerlink" title="2.1.欧式距离"></a>2.1.欧式距离</h3><p><img src="https://img-blog.csdnimg.cn/20190724141524460.png" alt></p>
<h3 id="2-2-曼哈顿距离"><a href="#2-2-曼哈顿距离" class="headerlink" title="2.2.曼哈顿距离"></a>2.2.曼哈顿距离</h3><p><img src="https://img-blog.csdnimg.cn/20190724141601149.png" alt></p>
<h3 id="2-3-余弦相似度"><a href="#2-3-余弦相似度" class="headerlink" title="2.3.余弦相似度"></a>2.3.余弦相似度</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A与B表示向量(x1,y1)，(x2,y2)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分子为A与B的点乘，分母为二者各自的L2相乘，即将所有维度值的平方相加后开方。<br><img src="https://img-blog.csdnimg.cn/2019072611455719.png" alt></p>
<blockquote>
<p>说明：<br><strong>A</strong>.经过step2，得到k个新的簇，每个样本都被分到k个簇中的某一个簇<br><strong>B</strong>.得到k个新的簇后，当前的质心就会失效，需要计算每个新簇的自己的新质心</p>
</blockquote>
<h2 id="Step3-新质心的计算"><a href="#Step3-新质心的计算" class="headerlink" title="Step3.新质心的计算"></a>Step3.新质心的计算</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于分类后的产生的k个簇，分别计算到簇内其他点距离均值最小的点作为质心（对于拥有坐标的簇可以计算每个簇坐标的均值作为质心）</p>
<blockquote>
<p>说明：<br><strong>A</strong>.比如一个新簇有3个样本：[[1,4], [2,5], [3,6]]，得到此簇的新质心=[(1+2+3)/3,   (4+5+6)/3]<br><strong>B</strong>.经过step3，会得到k个新的质心，作为step2中使用的质心</p>
</blockquote>
<h2 id="Step4-是否停止K-means"><a href="#Step4-是否停止K-means" class="headerlink" title="Step4.是否停止K-means"></a>Step4.是否停止K-means</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;质心不再改变，或给定loop最大次数loopLimit  </p>
<blockquote>
<p>说明：<br><strong>A</strong>当每个簇的质心，不再改变时就可以停止k-menas<br><strong>B</strong>.当loop次数超过looLimit时，停止k-means<br><strong>C</strong>.只需要满足两者的其中一个条件，就可以停止k-means<br><strong>C</strong>.如果Step4没有结束k-means，就再执行step2-step3-step4<br><strong>D</strong>.如果Step4结束了k-means，则就打印(或绘制)簇以及质心</p>
</blockquote>
<h1 id="四-python实现-代码详解"><a href="#四-python实现-代码详解" class="headerlink" title="四.python实现+代码详解"></a>四.python实现+代码详解</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以下是python得实例代码以及代码的详解，应该可以理解的。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算欧拉距离</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcDis</span><span class="params">(dataSet, centroids, k)</span>:</span></span><br><span class="line">    clalist=[]</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dataSet:</span><br><span class="line">        diff = np.tile(data, (k, <span class="number">1</span>)) - centroids  <span class="comment">#相减   (np.tile(a,(2,1))就是把a先沿x轴复制1倍，即没有复制，仍然是 [0,1,2]。 再把结果沿y方向复制2倍得到array([[0,1,2],[0,1,2]]))</span></span><br><span class="line">        squaredDiff = diff ** <span class="number">2</span>     <span class="comment">#平方</span></span><br><span class="line">        squaredDist = np.sum(squaredDiff, axis=<span class="number">1</span>)   <span class="comment">#和  (axis=1表示行)</span></span><br><span class="line">        distance = squaredDist ** <span class="number">0.5</span>  <span class="comment">#开根号</span></span><br><span class="line">        clalist.append(distance) </span><br><span class="line">    clalist = np.array(clalist)  <span class="comment">#返回一个每个点到质点的距离len(dateSet)*k的数组</span></span><br><span class="line">    <span class="keyword">return</span> clalist</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算质心</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(dataSet, centroids, k)</span>:</span></span><br><span class="line">    <span class="comment"># 计算样本到质心的距离</span></span><br><span class="line">    clalist = calcDis(dataSet, centroids, k)</span><br><span class="line">    <span class="comment"># 分组并计算新的质心</span></span><br><span class="line">    minDistIndices = np.argmin(clalist, axis=<span class="number">1</span>)    <span class="comment">#axis=1 表示求出每行的最小值的下标</span></span><br><span class="line">    newCentroids = pd.DataFrame(dataSet).groupby(minDistIndices).mean() <span class="comment">#DataFramte(dataSet)对DataSet分组，groupby(min)按照min进行统计分类，mean()对分类结果求均值</span></span><br><span class="line">    newCentroids = newCentroids.values</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 计算变化量</span></span><br><span class="line">    changed = newCentroids - centroids</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> changed, newCentroids</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用k-means分类</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kmeans</span><span class="params">(dataSet, k)</span>:</span></span><br><span class="line">    <span class="comment"># 随机取质心</span></span><br><span class="line">    centroids = random.sample(dataSet, k)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 更新质心 直到变化量全为0</span></span><br><span class="line">    changed, newCentroids = classify(dataSet, centroids, k)</span><br><span class="line">    <span class="keyword">while</span> np.any(changed != <span class="number">0</span>):</span><br><span class="line">        changed, newCentroids = classify(dataSet, newCentroids, k)</span><br><span class="line"> </span><br><span class="line">    centroids = sorted(newCentroids.tolist())   <span class="comment">#tolist()将矩阵转换成列表 sorted()排序</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 根据质心计算每个集群</span></span><br><span class="line">    cluster = []</span><br><span class="line">    clalist = calcDis(dataSet, centroids, k) <span class="comment">#调用欧拉距离</span></span><br><span class="line">    minDistIndices = np.argmin(clalist, axis=<span class="number">1</span>)  </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        cluster.append([])</span><br><span class="line">    <span class="keyword">for</span> i, j <span class="keyword">in</span> enumerate(minDistIndices):   <span class="comment">#enymerate()可同时遍历索引和遍历元素</span></span><br><span class="line">        cluster[j].append(dataSet[i])</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> centroids, cluster</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 创建数据集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">1</span>], [<span class="number">6</span>, <span class="number">4</span>], [<span class="number">6</span>, <span class="number">3</span>], [<span class="number">5</span>, <span class="number">4</span>]]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>: </span><br><span class="line">    dataset = createDataSet()</span><br><span class="line">    centroids, cluster = kmeans(dataset, <span class="number">2</span>)</span><br><span class="line">    print(<span class="string">'质心为：%s'</span> % centroids)</span><br><span class="line">    print(<span class="string">'集群为：%s'</span> % cluster)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(dataset)):</span><br><span class="line">      plt.scatter(dataset[i][<span class="number">0</span>],dataset[i][<span class="number">1</span>], marker = <span class="string">'o'</span>,color = <span class="string">'green'</span>, s = <span class="number">40</span> ,label = <span class="string">'原始点'</span>)</span><br><span class="line">                                                    <span class="comment">#  记号形状       颜色      点的大小      设置标签</span></span><br><span class="line">      <span class="keyword">for</span> j <span class="keyword">in</span> range(len(centroids)):</span><br><span class="line">        plt.scatter(centroids[j][<span class="number">0</span>],centroids[j][<span class="number">1</span>],marker=<span class="string">'x'</span>,color=<span class="string">'red'</span>,s=<span class="number">50</span>,label=<span class="string">'质心'</span>)</span><br><span class="line">        plt.show</span><br></pre></td></tr></table></figure></p>
<h1 id="五-K-means算法补充"><a href="#五-K-means算法补充" class="headerlink" title="五.K-means算法补充"></a>五.K-means算法补充</h1><p>1.对初始化敏感，初始质点k给定的不同，可能会产生不同的聚类结果。如下图所示，右边是k=2的结果，这个就正好，而左图是k=3的结果，可以看到右上角得这两个簇应该是可以合并成一个簇的。</p>
<p>改进：<br>对k的选择可以先用一些算法分析数据的分布，如重心和密度等，然后选择合适的k<br><img src="https://img-blog.csdnimg.cn/20190726154810167.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt><br>2.<strong>使用存在局限性</strong>，如下面这种非球状的数据分布就搞不定了：<br><img src="https://img-blog.csdnimg.cn/20190724161221112.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzQxMzEy,size_16,color_FFFFFF,t_70" alt><br>3.数据集比较大的时候，<strong>收敛会比较慢</strong>。</p>
<p>4.最终会收敛。不管初始点如何选择，最终都会收敛。可是是全局收敛，也可能是局部收敛。</p>
<h1 id="六-小结"><a href="#六-小结" class="headerlink" title="六.小结"></a>六.小结</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1. 聚类是一种无监督的学习方法。聚类区别于分类，即事先不知道要寻找的内容，没有预先设定好的目标变量。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2. 聚类将数据点归到多个簇中，其中相似的数据点归为同一簇，而不相似的点归为不同的簇。相似度的计算方法有很多，具体的应用选择合适的相似度计算方法</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3. K-means聚类算法，是一种广泛使用的聚类算法，其中k是需要指定的参数，即需要创建的簇的数目，K-means算法中的k个簇的质心可以通过随机的方式获得，但是这些点需要位于数据范围内。在算法中，计算每个点到质心得距离，选择距离最小的质心对应的簇作为该数据点的划分，然后再基于该分配过程后更新簇的质心。重复上述过程，直至各个簇的质心不再变化为止。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4. K-means算法虽然有效，但是容易受到初始簇质心的情况而影响，有可能陷入局部最优解。为了解决这个问题，可以使用另外一种称为二分K-means的聚类算法。二分K-means算法首先将所有数据点分为一个簇；然后使用K-means（k=2）对其进行划分；下一次迭代时，选择使得SSE下降程度最大的簇进行划分；重复该过程，直至簇的个数达到指定的数目为止。实验表明，二分K-means算法的聚类效果要好于普通的K-means聚类算法。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://hpu-yz.github.io/2019/07/19/K-means聚类算法原理及python实现/" data-id="ck7wo57l20000z4uhyk7ng61b" class="article-share-link">分享</a>
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/K-means/">K-means</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/机器学习/">机器学习</a></li></ul>

    </footer>

  </div>

  
    
  <nav class="article-nav">
    
      <a href="/2019/07/20/K-最近邻分类算法(KNN)及python实现/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            KNN算法及python实现
          
        </div>
      </a>
    
    
      <a href="/2019/07/18/数据集的划分--训练集、验证集和测试集/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">数据集的划分--训练集、验证集和测试集</div>
      </a>
    
  </nav>


  

  
    
  

</article>



</section>
  <footer class="footer">
  <div class="outer">
    <div class="float-right">
      <ul class="list-inline">
  
    <li><i class="fe fe-smile-alt"></i> <span id="busuanzi_value_site_uv"></span></li>
  
</ul>
    </div>
    <ul class="list-inline">
      <li>&copy; 2020 Flaneur</li>
      <li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
      <li>Theme  <a href="https://github.com/zhwangart/hexo-theme-ocean">Ocean</a></li>
    </ul>
  </div>
</footer>

</main>
<aside class="sidebar">
  <button class="navbar-toggle"></button>

<nav class="navbar">
  
    <div class="logo">
      <a href="/"><img src="/images/favicon.ico" alt="Flaneur"></a>
    </div>
  
  <ul class="nav nav-main">
    
      <li class="nav-item">
        <a class="nav-item-link" href="/">主页</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/archives">归档</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/gallery">相册</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/about">关于</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/tags">友链</a>
      </li>
    
    <li class="nav-item">
      <a class="nav-item-link nav-item-search" title="搜索">
        <i class="fe fe-search"></i>
        搜索
      </a>
    </li>
  </ul>
</nav>

<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      <div class="totop" id="totop">
  <i class="fe fe-rocket"></i>
</div>
    </li>
    <li class="nav-item">
      
        <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
          <i class="fe fe-feed"></i>
        </a>
      
    </li>
  </ul>
</nav>

<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
</aside>
<script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/lazyload.min.js"></script>
<script src="/js/snap.svg-min.js"></script>
<script src="/js/busuanzi-2.3.pure.min.js"></script>


  <script src="/fancybox/jquery.fancybox.min.js"></script>



  <script src="/js/search.js"></script>


<script src="/js/ocean.js"></script>

</body>
</html>