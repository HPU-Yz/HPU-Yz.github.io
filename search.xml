<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[多种相似度计算的python实现]]></title>
    <url>%2F2019%2F07%2F24%2F%E5%A4%9A%E7%A7%8D%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97%E7%9A%84python%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在机器学习中有很多地方要计算相似度，比如聚类分析和协同过滤。计算相似度的有许多方法，其中有欧几里德距离(欧式距离)、曼哈顿距离、Jaccard系数和皮尔逊相关度等等。我们这里把一些常用的相似度计算方法，用python进行实现以下。大家都是初学者，我认为把公式先写下来，然后再写代码去实现比较好。 欧几里德距离(欧式距离)几个数据集之间的相似度一般是基于每对对象间的距离计算。最常用的当然是欧几里德距离，其公式为：12345678910111213#-*-coding:utf-8 -*-#计算欧几里德距离：def euclidean(p,q):#如果两数据集数目不同，计算两者之间都对应有的数same = 0for i in p: if i in q: same +=1#计算欧几里德距离,并将其标准化e = sum([(p[i] - q[i])**2 for i in range(same)])return 1/(1+e**.5) 我们用数据集可以去算一下： 123p = [1,3,2,3,4,3]q = [1,3,4,3,2,3,4,3]print euclidean(p,q) 得出结果是：0.261203874964 皮尔逊相关度几个数据集中出现异常值的时候，欧几里德距离就不如皮尔逊相关度‘稳定’，它会在出现偏差时倾向于给出更好的结果。其公式为： 123456789101112131415161718192021222324252627-*-coding:utf-8 -*-#计算皮尔逊相关度：def pearson(p,q):#只计算两者共同有的 same = 0 for i in p: if i in q: same +=1 n = same #分别求p，q的和 sumx = sum([p[i] for i in range(n)]) sumy = sum([q[i] for i in range(n)]) #分别求出p，q的平方和 sumxsq = sum([p[i]**2 for i in range(n)]) sumysq = sum([q[i]**2 for i in range(n)]) #求出p，q的乘积和 sumxy = sum([p[i]*q[i] for i in range(n)]) # print sumxy #求出pearson相关系数 up = sumxy - sumx*sumy/n down = ((sumxsq - pow(sumxsq,2)/n)*(sumysq - pow(sumysq,2)/n))**.5 #若down为零则不能计算，return 0 if down == 0 :return 0 r = up/down return r 用同样的数据集去计算： 123p = [1,3,2,3,4,3]q = [1,3,4,3,2,3,4,3]print pearson(p,q) 得出结果是：0.00595238095238 曼哈顿距离曼哈顿距离是另一种相似度计算方法，不是经常需要，但是我们仍然学会如何用python去实现，其公式为： 1234567891011121314#-*-coding:utf-8 -*-#计算曼哈顿距离：def manhattan(p,q):#只计算两者共同有的 same = 0 for i in p: if i in q: same += 1#计算曼哈顿距离 n = same vals = range(n) distance = sum(abs(p[i] - q[i]) for i in vals) return distance 用以上的数据集去计算： 123p = [1,3,2,3,4,3]q = [1,3,4,3,2,3,4,3]print manhattan(p,q) 得出结果为4 小结&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里只讲述了三种相似度的计算方法，事实上还有很多种，由于我也是刚学，其他的方法还不是很了解，以后碰到了再补上。一般情况下，这三种方法是最常用的，足够我们使用了。]]></content>
      <categories>
        <category>机器学习</category>
        <category>相似度</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>机器学习</tag>
        <tag>相似度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐系统之矩阵分解(MF)及其python实现]]></title>
    <url>%2F2019%2F07%2F23%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B9%8B%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3(MF)%E5%8F%8A%E5%85%B6python%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[前言目前推荐系统中用的最多的就是矩阵分解方法，在Netflix Prize推荐系统大赛中取得突出效果。以用户-项目评分矩阵为例，矩阵分解就是预测出评分矩阵中的缺失值，然后根据预测值以某种方式向用户推荐。今天以“用户-项目评分矩阵R（M×N）”说明矩阵分解方式的原理以及python实现。 一、矩阵分解1.案例引入有如下R(5,4)的打分矩阵：（“-”表示用户没有打分） 其中打分矩阵R(n,m)是n行和m列，n表示user个数，m行表示item个数那么，如何根据目前的矩阵R（5,4）如何对未打分的商品进行评分的预测（如何得到分值为0的用户的打分值）？ ——矩阵分解的思想可以解决这个问题，其实这种思想可以看作是有监督的机器学习问题（回归问题）。 矩阵分解的过程中，,矩阵R可以近似表示为矩阵P与矩阵Q的乘积：矩阵P(n,k)表示n个user和k个特征之间的关系矩阵，这k个特征是一个中间变量，矩阵Q(k,m)的转置是矩阵Q(m,k)，矩阵Q(m,k)表示m个item和K个特征之间的关系矩阵，这里的k值是自己控制的，可以使用交叉验证的方法获得最佳的k值。为了得到近似的R(n,m)，必须求出矩阵P和Q，如何求它们呢？ 2.推导步骤 首先令： 对于式子1的左边项，表示的是r^ 第i行，第j列的元素值，对于如何衡量，我们分解的好坏呢，式子2，给出了衡量标准，也就是损失函数，平方项损失，最后的目标，就是每一个元素(非缺失值)的e(i,j)的总和最小值 使用梯度下降法获得修正的p和q分量： 求解损失函数的负梯度： 根据负梯度的方向更新变量： 不停迭代直到算法最终收敛（直到sum(e^2) &lt;=阈值，即梯度下降结束条件：f(x)的真实值和预测值小于自己设定的阈值） 为了防止过拟合，增加正则化项 3.加入正则项的损失函数求解 通常在求解的过程中，为了能够有较好的泛化能力，会在损失函数中加入正则项，以对参数进行约束，加入正则L2范数的损失函数为：对正则化不清楚的，公式可化为： 使用梯度下降法获得修正的p和q分量：-求解损失函数的负梯度： 根据负梯度的方向更新变量：4.预测预测利用上述的过程，我们可以得到矩阵和，这样便可以为用户 i 对商品 j 进行打分：二、python代码实现 以下是根据上文的评分例子做的一个矩阵分解算法，并且附有代码详解。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from math import *import numpyimport matplotlib.pyplot as pltdef matrix_factorization(R,P,Q,K,steps=5000,alpha=0.0002,beta=0.02): #矩阵因子分解函数，steps：梯度下降次数；alpha：步长；beta：β。 Q=Q.T # .T操作表示矩阵的转置 result=[] for step in range(steps): #梯度下降 for i in range(len(R)): for j in range(len(R[i])): eij=R[i][j]-numpy.dot(P[i,:],Q[:,j]) # .DOT表示矩阵相乘 for k in range(K): if R[i][j]&gt;0: #限制评分大于零 P[i][k]=P[i][k]+alpha*(2*eij*Q[k][j]-beta*P[i][k]) #增加正则化，并对损失函数求导，然后更新变量P Q[k][j]=Q[k][j]+alpha*(2*eij*P[i][k]-beta*Q[k][j]) #增加正则化，并对损失函数求导，然后更新变量Q eR=numpy.dot(P,Q) e=0 for i in range(len(R)): for j in range(len(R[i])): if R[i][j]&gt;0: e=e+pow(R[i][j]-numpy.dot(P[i,:],Q[:,j]),2) #损失函数求和 for k in range(K): e=e+(beta/2)*(pow(P[i][k],2)+pow(Q[k][j],2)) #加入正则化后的损失函数求和 result.append(e) if e&lt;0.001: #判断是否收敛，0.001为阈值 break return P,Q.T,resultif __name__ == '__main__': #主函数 R=[ #原始矩阵 [5,3,0,1], [4,0,0,1], [1,1,0,5], [1,0,0,4], [0,1,5,4] ] R=numpy.array(R) N=len(R) #原矩阵R的行数 M=len(R[0]) #原矩阵R的列数 K=3 #K值可根据需求改变 P=numpy.random.rand(N,K) #随机生成一个 N行 K列的矩阵 Q=numpy.random.rand(M,K) #随机生成一个 M行 K列的矩阵 nP,nQ,result=matrix_factorization(R,P,Q,K) print(R) #输出原矩阵 R_MF=numpy.dot(nP,nQ.T) print(R_MF) #输出新矩阵 #画图 plt.plot(range(len(result)),result) plt.xlabel("time") plt.ylabel("loss") plt.show()]]></content>
      <categories>
        <category>机器学习</category>
        <category>人工智能</category>
        <category>推荐系统</category>
        <category>矩阵分解</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>机器学习</tag>
        <tag>人工智能</tag>
        <tag>推荐系统</tag>
        <tag>矩阵分解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[暑期培训第一次测试题总结]]></title>
    <url>%2F2019%2F07%2F22%2F%E6%9A%91%E6%9C%9F%E5%9F%B9%E8%AE%AD%E7%AC%AC%E4%B8%80%E6%AC%A1%E6%B5%8B%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[前言这里是一些暑期培训第一次测试题的部分解释，经过这次测试的摧残，总结备录一下，方便日后回顾复习。 Feeling&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;经过几天的学习，7月22日下午，进行了第一次检测。开始以为会让我们推导一些公式什么的，结果当拿到测试题的时候，一首凉凉送给自己……开始的选择题和填空题还能接受，看到简答题，这都是什么，有种似曾相识的感觉，但就是写不出来。为时四十分钟的考试结束后，不到半个小时成绩就出来了，虽然成绩不那么好，但排名还行，然后学长给我们进行了讲解答疑，发现自己学的有点粗糙，没有注意那些细节性问题和概念，学以致用这方面也是有点差的。 Test 1为什么一般需要划分出额外的校验集(validation set)用于超参数调整，而不选择直接使用测试集(test set)?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;校验集是用于调整超参数的，从而更好的优化训练模型。测试集是用于在完成神经网络训练过程后,为了客观评价模型在其未见过(未曾影响普通参数和超参数选择)的数据上的性能,而且测试集不能提出对参数或者超参数的修改意见,只能作为评价网络性能的一个指标。 （备注：不清楚这三个数据集概念及其作用的，可看下我以前写的关于这些的一篇文章。传送门：） Test 2批量梯度下降(Batch Gradient Descent)和随机梯度下降(Stochastic Gradient Descent)在应对鞍点时有何不同表现？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们要先知道什么是BGD和SGD，从两者的运算方法上，我们就可以得知不同之处。 （1）批量梯度下降法（Batch Gradient Descent） ：在更新参数时都使用所有的样本来进行更新。 优点：全局最优解，能保证每一次更新权值，都能降低损失函数；易于并行实现。 缺点：当样本数目很多时，训练过程会很慢。 （2）随机梯度下降法（Stochastic Gradient Descent）：在更新参数时都使用一个样本来进行更新。每一次跟新参数都用一个样本，更新很多次。如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将参数迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次，这种方式计算复杂度太高。 优点：训练速度快； 缺点：准确度下降，并不是全局最优；不易于并行实现。从迭代的次数上来看，随机梯度下降法迭代的次数较多，在解空间的搜索过程看起来很盲目。噪音很多，使得它并不是每次迭代都向着整体最优化方向。 Test 3当一个模型训练完后若在训练集上的loss非常高，请问如何在不对代码进行全面排查的前提下，以最快速度定位是模型本身的拟合能力不足还是代码的实现存在某种错误？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;废话不多说，直接上图： Test 4假设我们在训练一个使用Sigmoid激活函数的全连接神经网络。在对其权重进行初始化时，为什么一般会倾向于让初始值的绝对值偏小？如果需要这样，为何不直接使用0进行初始化？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于逻辑回归，把权重初始化为0当然也是可以的，但是对于一个神经网络，如果你把权重或者参数都初始化为0，那么梯度下降将不会起作用。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果权值全初始化为0，则无法更新权值。这是由于前向传播中，所有节点输出值均相同，由于此处使用了sigmod激活函数，所以此处所有神经节点输出都为1/2，而在反向传播每个节点输出值对损失函数的偏导时，涉及到对权值相乘后的求和，该项永远为0，故所乘的结果也必然为0，这样在计算权值对算是函数的偏导时，其偏导必然为0，所有权值偏导都为0，那么就不要指望使用梯度下降法能更新权值了，自然神经网络的训练也就无法进行下去了。 Test 5在CNN中梯度不稳定指的是什么？在神经网络训练过程中，为什么会出现梯度消失的问题？如何解决？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;神经网络中的梯度不稳定指的是梯度消失和梯度爆炸问题。（备注：对于这两种问题的具体解释和为什么会出现这种问题，以及解决方法，这里不具体讨论了，我会在以后的文章中具体解释到的。） Test 6为什么在神经网络中使用交叉熵而不是均方差作为误差函数？ 神经网络中如果预测值与实际值的误差越大，那么在反向传播训练的过程中，各种参数调整的幅度就要更大，从而使训练更快收敛，如果预测值与实际值的误差小，各种参数调整的幅度就要小，从而减少震荡。 使用平方误差损失函数，误差增大参数的梯度会增大，但是当误差很大时，参数的梯度就会又减小了。 使用交叉熵损失是函数，误差越大参数的梯度也越大，能够快速收敛。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（备注：对于结论的推导过程会总结在另一篇文章里。）]]></content>
      <categories>
        <category>测试题，机器学习</category>
      </categories>
      <tags>
        <tag>测试题，机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K-means算法原理及python实现]]></title>
    <url>%2F2019%2F07%2F19%2FK-means%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8Apython%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;K-Means(Thek-meansalgorithm)是机器学习十大经典算法之一，同时也是最为经典的无监督聚类（Unsupervised Clustering）算法。接触聚类算法，首先需要了解k-means算法的实现原理和步骤。本文将对k-means算法的基本原理和实现实例进行分析。 一.聚类算法的简介&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于”监督学习“(supervised learning)，其训练样本是带有标记信息的，并且监督学习的目的是：对带有标记的数据集进行模型学习，从而便于对新的样本进行分类。而在“无监督学习”(unsupervised learning)中，训练样本的标记信息是未知的，目标是通过对无标记训练样本的学习来揭示数据的内在性质及规律，为进一步的数据分析提供基础。对于无监督学习，应用最广的便是”聚类“(clustering)。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;”聚类算法“试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个“簇”(cluster)，通过这样的划分，每个簇可能对应于一些潜在的概念或类别。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们可以通过下面这个图来理解： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上图是未做标记的样本集，通过他们的分布，我们很容易对上图中的样本做出以下几种划分。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当需要将其划分为两个簇时，即 k=2时：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当需要将其划分为四个簇时，即 k=4 时：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 二.K-means聚类算法&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kmeans算法又名k均值算法,K-means算法中的k表示的是聚类为k个簇，means代表取每一个聚类中数据值的均值作为该簇的中心，或者称为质心，即用每一个的类的质心对该簇进行描述。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其算法思想大致为：先从样本集中随机选取 k个样本作为簇中心，并计算所有样本与这 k个“簇中心”的距离，对于每一个样本，将其划分到与其距离最近的“簇中心”所在的簇中，对于新的簇计算各个簇的新的“簇中心”。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;根据以上描述，我们大致可以猜测到实现kmeans算法的主要四点： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（1）簇个数 k 的选择 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（2）各个样本点到“簇中心”的距离 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（3）根据新划分的簇，更新“簇中心” &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（4）重复上述2、3过程，直至”簇中心”没有移动 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;优缺点： 优点：容易实现 缺点：可能收敛到局部最小值，在大规模数据上收敛较慢 三.K-means算法步骤详解Step1.K值的选择 k 的选择一般是按照实际需求进行决定，或在实现算法时直接给定 k 值。 说明： A.质心数量由用户给出，记为k，k-means最终得到的簇数量也是k B.后来每次更新的质心的个数都和初始k值相等 C.k-means最后聚类的簇个数和用户指定的质心个数相等，一个质心对应一个簇，每个样本只聚类到一个簇里面 D.初始簇为空 Step2.距离度量&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;将对象点分到距离聚类中心最近的那个簇中需要最近邻的度量策略，在欧式空间中采用的是欧式距离，在处理文档中采用的是余弦相似度函数，有时候也采用曼哈顿距离作为度量，不同的情况实用的度量公式是不同的。 2.1.欧式距离 2.2.曼哈顿距离 2.3.余弦相似度&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A与B表示向量(x1,y1)，(x2,y2)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分子为A与B的点乘，分母为二者各自的L2相乘，即将所有维度值的平方相加后开方。 说明：A.经过step2，得到k个新的簇，每个样本都被分到k个簇中的某一个簇B.得到k个新的簇后，当前的质心就会失效，需要计算每个新簇的自己的新质心 Step3.新质心的计算&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于分类后的产生的k个簇，分别计算到簇内其他点距离均值最小的点作为质心（对于拥有坐标的簇可以计算每个簇坐标的均值作为质心） 说明：A.比如一个新簇有3个样本：[[1,4], [2,5], [3,6]]，得到此簇的新质心=[(1+2+3)/3, (4+5+6)/3]B.经过step3，会得到k个新的质心，作为step2中使用的质心 Step4.是否停止K-means&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;质心不再改变，或给定loop最大次数loopLimit 说明：A当每个簇的质心，不再改变时就可以停止k-menas B.当loop次数超过looLimit时，停止k-meansC.只需要满足两者的其中一个条件，就可以停止k-meansC.如果Step4没有结束k-means，就再执行step2-step3-step4D.如果Step4结束了k-means，则就打印(或绘制)簇以及质心 四.python实现+代码详解&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以下是python得实例代码以及代码的详解，应该可以理解的。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import randomimport pandas as pdimport numpy as npimport matplotlib.pyplot as plt# 计算欧拉距离def calcDis(dataSet, centroids, k): clalist=[] for data in dataSet: diff = np.tile(data, (k, 1)) - centroids #相减 (np.tile(a,(2,1))就是把a先沿x轴复制1倍，即没有复制，仍然是 [0,1,2]。 再把结果沿y方向复制2倍得到array([[0,1,2],[0,1,2]])) squaredDiff = diff ** 2 #平方 squaredDist = np.sum(squaredDiff, axis=1) #和 (axis=1表示行) distance = squaredDist ** 0.5 #开根号 clalist.append(distance) clalist = np.array(clalist) #返回一个每个点到质点的距离len(dateSet)*k的数组 return clalist# 计算质心def classify(dataSet, centroids, k): # 计算样本到质心的距离 clalist = calcDis(dataSet, centroids, k) # 分组并计算新的质心 minDistIndices = np.argmin(clalist, axis=1) #axis=1 表示求出每行的最小值的下标 newCentroids = pd.DataFrame(dataSet).groupby(minDistIndices).mean() #DataFramte(dataSet)对DataSet分组，groupby(min)按照min进行统计分类，mean()对分类结果求均值 newCentroids = newCentroids.values # 计算变化量 changed = newCentroids - centroids return changed, newCentroids# 使用k-means分类def kmeans(dataSet, k): # 随机取质心 centroids = random.sample(dataSet, k) # 更新质心 直到变化量全为0 changed, newCentroids = classify(dataSet, centroids, k) while np.any(changed != 0): changed, newCentroids = classify(dataSet, newCentroids, k) centroids = sorted(newCentroids.tolist()) #tolist()将矩阵转换成列表 sorted()排序 # 根据质心计算每个集群 cluster = [] clalist = calcDis(dataSet, centroids, k) #调用欧拉距离 minDistIndices = np.argmin(clalist, axis=1) for i in range(k): cluster.append([]) for i, j in enumerate(minDistIndices): #enymerate()可同时遍历索引和遍历元素 cluster[j].append(dataSet[i]) return centroids, cluster # 创建数据集def createDataSet(): return [[1, 1], [1, 2], [2, 1], [6, 4], [6, 3], [5, 4]]if __name__=='__main__': dataset = createDataSet() centroids, cluster = kmeans(dataset, 2) print('质心为：%s' % centroids) print('集群为：%s' % cluster) for i in range(len(dataset)): plt.scatter(dataset[i][0],dataset[i][1], marker = 'o',color = 'green', s = 40 ,label = '原始点') # 记号形状 颜色 点的大小 设置标签 for j in range(len(centroids)): plt.scatter(centroids[j][0],centroids[j][1],marker='x',color='red',s=50,label='质心') plt.show 五.K-means算法补充1.对初始化敏感，初始质点k给定的不同，可能会产生不同的聚类结果。如下图所示，右边是k=2的结果，这个就正好，而左图是k=3的结果，可以看到右上角得这两个簇应该是可以合并成一个簇的。 改进：对k的选择可以先用一些算法分析数据的分布，如重心和密度等，然后选择合适的k2.使用存在局限性，如下面这种非球状的数据分布就搞不定了：3.数据集比较大的时候，收敛会比较慢。 4.最终会收敛。不管初始点如何选择，最终都会收敛。可是是全局收敛，也可能是局部收敛。 六.小结&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1. 聚类是一种无监督的学习方法。聚类区别于分类，即事先不知道要寻找的内容，没有预先设定好的目标变量。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2. 聚类将数据点归到多个簇中，其中相似的数据点归为同一簇，而不相似的点归为不同的簇。相似度的计算方法有很多，具体的应用选择合适的相似度计算方法 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3. K-means聚类算法，是一种广泛使用的聚类算法，其中k是需要指定的参数，即需要创建的簇的数目，K-means算法中的k个簇的质心可以通过随机的方式获得，但是这些点需要位于数据范围内。在算法中，计算每个点到质心得距离，选择距离最小的质心对应的簇作为该数据点的划分，然后再基于该分配过程后更新簇的质心。重复上述过程，直至各个簇的质心不再变化为止。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4. K-means算法虽然有效，但是容易受到初始簇质心的情况而影响，有可能陷入局部最优解。为了解决这个问题，可以使用另外一种称为二分K-means的聚类算法。二分K-means算法首先将所有数据点分为一个簇；然后使用K-means（k=2）对其进行划分；下一次迭代时，选择使得SSE下降程度最大的簇进行划分；重复该过程，直至簇的个数达到指定的数目为止。实验表明，二分K-means算法的聚类效果要好于普通的K-means聚类算法。]]></content>
      <categories>
        <category>K-means</category>
        <category>python</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>机器学习</tag>
        <tag>K-means</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据集的划分--训练集、验证集和测试集]]></title>
    <url>%2F2019%2F07%2F18%2F%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%88%92%E5%88%86--%E8%AE%AD%E7%BB%83%E9%9B%86%E3%80%81%E9%AA%8C%E8%AF%81%E9%9B%86%E5%92%8C%E6%B5%8B%E8%AF%95%E9%9B%86%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在机器学习中，经常提到训练集和测试集，验证集似有似无。感觉挺好奇的，就仔细查找了文献。以下谈谈训练集、验证集和测试集。 为什么要划分数据集为训练集、验证集和测试集？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;做科研，就要提出问题，找到解决方法，并证明其有效性。这里的工作有3个部分，一个是提出问题，一个是找到解决方法，另一个是证明有效性。每一个部分都可以作为科研的对象，研究的越靠前，则越偏向科学，越靠后，则越偏向技术，因此叫做科学与技术。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在人工智能领域，证明一个模型的有效性，就是对于某一问题，有一些数据，而我们提出的模型可以（部分）解决这个问题，那如何来证明呢？这和我们平时的考试也是一样的，证明我们掌握了某类知识，就是去参加考试。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;好，那么如何设计考试，让这个考试可以较为客观的考察出每个人的能力呢（注意，不是让每个人都得最高分）？回想我们的高中阶段，有一些教材，让我们平时学习其基本知识（训练集），有一些模拟考试，让我们知道我们到底掌握的怎么样，然后再改进我们的学习（验证集），最后的高考决定我们的去向（测试集）。这样的类比，是不是就很清楚了。 训练集、验证集和测试集 训练集：顾名思义指的是用于训练的样本集合,主要用来训练神经网络中的参数。 验证集：从字面意思理解即为用于验证模型性能的样本集合.不同神经网络在训练集上训练结束后,通过验证集来比较判断各个模型的性能.这里的不同模型主要是指对应不同超参数的神经网络,也可以指完全不同结构的神经网络。 测试集：对于训练完成的神经网络,测试集用于客观的评价神经网络的性能。 如何划分训练集、验证集和测试集&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这个问题其实非常基础，也非常明确，在Scikit-learn里提供了各种各样的划分方法。无论是单一的训练集、验证集和测试集，还是进行交叉验证，你都会找到调用的方法，因此我们这里主要介绍两种常见的方法。 前人给出训练集、验证集和测试集对于这种情况，那么只能跟随前人的数据划分进行，一般的比赛也是如此。一定不要使用测试集来调整性能（测试集已知的情况下），尽管存在使用这种方法来提升模型的性能的行为，但是我们并不推荐这么做。最正常的做法应当是使用训练集来学习，并使用验证集来调整超参数。当在验证集上取得最优的模型时，此时就可以使用此模型的超参数来重新训练（训练集+验证集），并用测试集评估最终的性能。我们首先说明加入验证集重新训练和不加有啥区别，从理论上讲，一方面学习的样本增多，应当是会提升模型性能的，第二，其在验证集上取得最优的模型与验证集的分布的契合度是最高的，因此最终的模型会更接近验证集的分布。其次再说明验证集和测试集上的性能差异。事实上，在验证集上取得最优的模型，未必在测试集上取得最优。其原因就是训练的模型是否对于该问题有着较好的泛化能力，即没有对验证集产生过拟合现象。正因为有这种情况的发生，才会有人使用测试集的最优值作为最终的结果（而不管验证集的好坏）。 前人没有明确给出数据集的划分这时候可以采取第一种划分方法，对于样本数较小的数据集，同样可以采取交叉验证的方法。交叉验证的方法的使用场景有很多，我们这里是针对不同的模型的性能好坏进行评估。使用交叉验证，可以获得更为客观的性能差异。当使用第一种方法时，我们更建议使用P值来做显著性检验，从而保证性能差异的客观性。而使用第二种方法，即交叉验证时，我们选取其性能表现的均值作为最终的结果，更能体现该模型的泛化能力。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.存在验证集&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里五倍交叉验证是用于进行调参，此时不接触测试集。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据集首先划分出训练集与测试集（可以是4:1或者9:1）。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其次，在训练集中，再划分出验证集（通常也是4:1或者9：1）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;然后对于训练集和验证集进行5折交叉验证，选取出最优的超参数，然后把训练集和验证集一起训练出最终的模型。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.不存在验证集&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该情况通常是对比不同的模型，如自己的模型和别人的模型的性能好坏。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;只需要把数据集划分为训练集和测试集即可，然后选取5次试验的平均值作为最终的性能评价。 验证集和测试集的区别&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那么,训练集、校验集和测试集之间又有什么区别呢?一般而言,训练集与后两者之间较易分辨,校验集和测试集之间的概念较易混淆.个人是从下面的角度来理解的: 神经网络在网络结构确定的情况下,有两部分影响模型最终的性能,一是普通参数(比如权重w和偏置b),另一个是超参数(例如学习率,网络层数).普通参数我们在训练集上进行训练,超参数我们一般人工指定(比较不同超参数的模型在校验集上的性能).那为什么我们不像普通参数一样在训练集上训练超参数呢?(花书给出了解答)一是：超参数一般难以优化(无法像普通参数一样通过梯度下降的方式进行优化).二是：超参数很多时候不适合在训练集上进行训练,例如,如果在训练集上训练能控制模型容量的超参数,这些超参数总会被训练成使得模型容量最大的参数(因为模型容量越大,训练误差越小),所以训练集上训练超参数的结果就是模型绝对过拟合. 正因为超参数无法在训练集上进行训练,因此我们单独设立了一个验证集,用于选择(人工训练)最优的超参数.因为验证集是用于选择超参数的,因此校验集和训练集是独立不重叠的. 测试集是用于在完成神经网络训练过程后,为了客观评价模型在其未见过(未曾影响普通参数和超参数选择)的数据上的性能,因此测试与验证集和训练集之间也是独立不重叠的,而且测试集不能提出对参数或者超参数的修改意见,只能作为评价网络性能的一个指标. 为了方便清楚直观的了解，上一个表格： 综述&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;至此,我们可以将神经网络完整的训练过程归结为一下两个步骤: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.训练普通参数.在训练集(给定超参数)上利用学习算法,训练普通参数,使得模型在训练集上的误差降低到可接受的程度(一般接近人类的水平). &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.’训练’超参数.在验证集上验证网络的generalization error(泛化能力),并根据模型性能对超参数进行调整. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;重复1和2两个步骤,直至网络在验证集上取得较低的generalization error.此时完整的训练过程结束.在完成参数和超参数的训练后,在测试集上测试网络的性能. 附言说到底： 验证集是一定需要的； 如果验证集具有足够泛化代表性，是不需要再整出什么测试集的； 整个测试集往往就是为了在验证集只是非训练集一个小子集的情况下，好奇一下那个靠训练集（训练）和验证集（调参）多次接力训练出来的模型是不是具有了泛化性能，因而加试一下图个确定。]]></content>
      <categories>
        <category>机器学习，数据集</category>
      </categories>
      <tags>
        <tag>机器学习，数据集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[梯度下降法的三种形式BGD、SGD、MBGD及python实现]]></title>
    <url>%2F2019%2F07%2F18%2F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E7%9A%84%E4%B8%89%E7%A7%8D%E5%BD%A2%E5%BC%8FBGD%E3%80%81SGD%E3%80%81MBGD%E5%8F%8Apython%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;梯度下降法作为机器学习中较常使用的优化算法，其有着三种不同的形式：批量梯度下降（Batch Gradient Descent）、随机梯度下降（Stochastic Gradient Descent）以及小批量梯度下降（Mini-Batch Gradient Descent）。其中小批量梯度下降法也常用在深度学习中进行模型的训练。接下来，我们将对这三种不同的梯度下降法进行理解。为了便于理解，这里我们将使用只含有一个特征的线性回归来展开。 此时线性回归的假设函数为：对应的目标函数（代价函数）即为： 下图为 J(θ0,θ1)与参数 θ0,θ1 的关系的图： 1、批量梯度下降（Batch Gradient Descent，BGD）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;批量梯度下降法是最原始的形式，它是指在每一次迭代时使用所有样本来进行梯度的更新。从数学上理解如下： （1）对目标函数求偏导： 其中 i=1,2,…,m 表示样本数， j=0,1 表示特征数，这里我们使用了偏置项 x(i)0=1。 （2）每次迭代对参数进行更新： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;注意这里更新时存在一个求和函数，即为对所有样本进行计算处理，可与下文SGD法进行比较。 伪代码形式为： 优点： （1）一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。 （2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。 缺点： （1）当样本数目 mm 很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。 从迭代的次数上来看，BGD迭代的次数相对较少。其迭代的收敛曲线示意图可以表示如下： 批量梯度下降法的python实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import matplotlib.pyplot as pltimport random##样本数据x_train = [150,200,250,300,350,400,600]y_train = [6450,7450,8450,9450,11450,15450,18450]#样本个数m = len(x_train)#步长alpha = 0.00001#循环次数cnt = 0#假设函数为 y=theta0+theta1*xdef h(x): return theta0 + theta1*xtheta0 = 0theta1 = 0#导数diff0=0diff1=0#误差error0=0 error1=0 #每次迭代theta的值retn0 = [] retn1 = [] #退出迭代的条件epsilon=0.00001#批量梯度下降while 1: cnt=cnt+1 diff0=0 diff1=0 #梯度下降 for i in range(m): diff0+=h(x_train[i])-y_train[i] diff1+=(h(x_train[i])-y_train[i])*x_train[i] theta0=theta0-alpha/m*diff0 theta1=theta1-alpha/m*diff1 retn0.append(theta0) retn1.append(theta1) error1=0 #计算迭代误差 for i in range(len(x_train)): error1 += ((theta0 + theta1 * x_train[i])-y_train[i]) ** 2 / 2 #判断是否已收敛 if abs(error1 - error0) &lt; epsilon: break else: error0 = error1# 画图表现plt.title('BGD')plt.plot(range(len(retn0)),retn0,label='theta0')plt.plot(range(len(retn1)),retn1,label='theta1')plt.legend() #显示上面的labelplt.xlabel('time')plt.ylabel('theta')plt.show()plt.plot(x_train,y_train,'bo')plt.plot(x_train,[h(x) for x in x_train],color='k',label='BGD')plt.legend()plt.xlabel('area')plt.ylabel('price')print("批量梯度下降法：theta0=&#123;&#125;,theta1=&#123;&#125;".format(theta0,theta1))print("批量梯度下降法循环次数：&#123;&#125;".format(cnt))plt.show() 2、随机梯度下降（Stochastic Gradient Descent，SGD）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&amp;nbsp;随机梯度下降法不同于批量梯度下降，随机梯度下降是每次迭代使用一个样本来对参数进行更新。使得训练速度加快。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于一个样本的目标函数为： （1）对目标函数求偏导： （2）参数更新： 注意，这里不再有求和符号 伪代码形式为： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;优点： （1）由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;缺点： （1）准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛。 （2）可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势。 （3）不易于并行实现。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;解释一下为什么SGD收敛速度比BGD要快：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;答：这里我们假设有30W个样本，对于BGD而言，每次迭代需要计算30W个样本才能对参数进行一次更新，需要求得最小值可能需要多次迭代（假设这里是10）；而对于SGD，每次更新参数只需要一个样本，因此若使用这30W个样本进行参数更新，则参数会被更新（迭代）30W次，而这期间，SGD就能保证能够收敛到一个合适的最小值上了。也就是说，在收敛时，BGD计算了 10×30W 次，而SGD只计算了 1×30W 次。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从迭代的次数上来看，SGD迭代的次数较少，在解空间的搜索过程看起来很盲目。其迭代的收敛曲线示意图可以表示如下： 随机梯度下降法的python实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import matplotlib.pyplot as pltimport random##样本数据x_train = [150,200,250,300,350,400,600]y_train = [6450,7450,8450,9450,11450,15450,18450]#样本个数m = len(x_train)#步长alpha = 0.00001#循环次数cnt = 0#假设函数为 y=theta0+theta1*xdef h(x): return theta0 + theta1*xtheta0 = 0theta1 = 0#导数diff0=0diff1=0#误差error0=0 error1=0 #每次迭代theta的值retn0 = [] retn1 = [] #退出迭代的条件epsilon=0.00001#随机梯度下降for i in range(1000): cnt=cnt+1 diff0=0 diff1=0 j = random.randint(0, m - 1) diff0=h(x_train[j])-y_train[j] diff1=(h(x_train[j])-y_train[j])*x_train[j] theta0=theta0-alpha/m*diff0 theta1=theta1-alpha/m*diff1 retn0.append(theta0) retn1.append(theta1) error1=0 #计算迭代的误差 for i in range(len(x_train)): error1 += ((theta0 + theta1 * x_train[i])-y_train[i]) ** 2 / 2 #判断是否已收敛 if abs(error1 - error0) &lt; epsilon: break else: error0 = error1# 画图表现 plt.title('SGD')plt.plot(range(len(retn0)),retn0,label='theta0')plt.plot(range(len(retn1)),retn1,label='theta1')plt.legend() #显示上面的labelplt.xlabel('time')plt.ylabel('theta')plt.show()plt.plot(x_train,y_train,'bo')plt.plot(x_train,[h(x) for x in x_train],color='k',label='SGD')plt.legend()plt.xlabel('area')plt.ylabel('price')print("随机梯度下降法：theta0=&#123;&#125;,theta1=&#123;&#125;".format(theta0,theta1))print("随机梯度下降法循环次数：&#123;&#125;".format(cnt))plt.show() 3、小批量梯度下降（Mini-Batch Gradient Descent, MBGD）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;小批量梯度下降，是对批量梯度下降以及随机梯度下降的一个折中办法。其思想是：每次迭代 使用 batch_size 个样本来对参数进行更新。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里我们假设 batchsize=10，样本数 m=1000 。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;伪代码形式为： 优点： （1）通过矩阵运算，每次在一个batch上优化神经网络参数并不会比单个数据慢太多。 （2）每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果。(比如上例中的30W，设置batch_size=100时，需要迭代3000次，远小于SGD的30W次) （3）可实现并行化。 缺点： （1）batch_size的不当选择可能会带来一些问题。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;batcha_size的选择带来的影响： （1）在合理地范围内，增大batch_size的好处： a. 内存利用率提高了，大矩阵乘法的并行化效率提高。 b. 跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。 c. 在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。 （2）盲目增大batch_size的坏处： a. 内存利用率提高了，但是内存容量可能撑不住了。 b. 跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。 c. Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下图显示了三种梯度下降算法的收敛过程： 4、总结Batch gradient descent: Use all examples in each iteration； Stochastic gradient descent: Use 1 example in each iteration； Mini-batch gradient descent: Use b examples in each iteration.]]></content>
      <categories>
        <category>梯度下降</category>
        <category>机器学习</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>机器学习</tag>
        <tag>梯度下降</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[送给她]]></title>
    <url>%2F2019%2F04%2F13%2F%E9%80%81%E7%BB%99%E5%A5%B9%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;此篇献给傻乎乎的小强，收录了小强最喜爱的华晨宇的歌曲，希望小强傻强有傻福。😄&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 凭数据说话&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年，武音有史以来在校学生个人节目上春晚第一人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年，出道一年十大代言。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年，继在《花儿与少年》的出色表现火爆银屏后，相继受邀登上《我们都爱笑》《天天向上》《快乐大本营》等众多湖南卫视王牌节目，因他的亮相节目收视率飙升同时段第一名。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年，首张个人专辑《卡西莫多的礼物》出专辑速度打破了往届纪录。 2014年，刚出道以最快速度开万人演唱会的第一人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年，华语乐坛在千人演出场馆召开发布会的第一人，全部采用演唱会配置。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年，芒果TV和QQ音乐两平台共计线上演唱会直播门票购买次数超过12万，刷新此前演唱会的在线付费观看人数。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年，Why nobody fights 开启了新网络媒体音乐版权付费时代，成为当代音乐第一人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年，首创音乐故事录音带，时长达到40分钟。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年，新浪微博客户端首创下拉出现Q版明星形象第一人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年，第一位广告打在了“北京世贸天阶的大LED、亚洲超大LED”世贸天阶的艺人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年，华晨宇火星北京演唱会门票1分35秒全部抢光！票务网站个人演唱会预定同时在线人数达到75312人的最高记录。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年成为天娱传媒旗下第一位进行O2O线上付费演唱会直播的艺人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年，首位荣登L’Uomo Vogue的内地男歌手。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年，韩国知名ALO眼镜首位中国代言人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2015年，2015最具潜力青春男偶TOP10内唯一的纯歌手。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2015年，受邀观礼格莱美。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2015年，个人演唱会连开三场，成为内地第一人！5月23日第一场开票，16万人在线，35秒万张票抢空，创线上售票新记录；5月30日第二场开票，在线人数16万，1分12秒再度秒杀！今日宣布7.31加开第三场。（获大麦网麦盘点2015年度售罄时间最快演唱会） &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2015年，成为华语乐坛在大型游轮上举办发布会第一人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2015年，上海“火星”演唱会8.1日打破LiveMusic直播264万在线人数记录，成为腾讯视频历史在线人数第一，相当于坐满220个上海大舞台。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2015年，《蜉蝣》蝉联了亚洲榜13周周榜冠军，成为亚洲新歌榜创榜以来第一位连续十周歌曲位居首位的歌手。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2015年，首张数字专辑《终极双主打》上线仅10分钟就以迅猛之势突破10万销量，刷新内地数字专辑上线10分钟的最高销售纪录。一小时后，售卖成绩正式突破30万，以绝对优势打破平台预售的历史纪录。当日24小时购买数量突破35万，再次缔造内地数字专辑数据传奇。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2015年，在数字专发行状态下又发行了个人实体大碟《异类》，有胆量同步发行实体专辑和数字专辑的艺人当属华晨宇第一人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2015年，国内首位入驻二次元空间A站的艺人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2015年，福布斯中国名人榜排91名！ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2016年，90后男艺人新媒体人气指数前十内唯一纯歌手（艺人新媒体指数＝参演电视剧每日播放量×A+微博数据×B+贴吧数据×C+豆瓣数据×D+搜索量×E+其他×F）华晨宇A零分的情况下进了前十。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2016年，凭借《异类》《蜉蝣》《国王与乞丐》等五首新歌在亚洲新歌榜共夺得12周周榜冠军，成为获得周榜冠军次数最多的歌手，并荣获““亚洲新歌榜2015冠军王”。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2016年，成为内地乐坛第一位启用四面台开唱的歌手。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2016年，上海站演唱会缔造乐视全屏终端最高9万人付费在线观看纪录。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2016年，出道3年八场万人演唱会！无赞助不赠票，场场爆满。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2016年，三首歌曲荣获亚洲新歌榜年度十大金曲，风头无两。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2016年，《我的滑板鞋2016》占据香港和台湾的热搜NO.1，在Youtube热门排行榜中，《我的滑板鞋2016》排名台湾热门第一，香港热门前三。谷歌、脸书、Instagram、推特、YouTube等境外网络媒体上全面刮起“滑板鞋”飓风，数据惊人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2016年，成为了MAMA亚洲最佳艺人大奖迄今为止最年轻的获奖者。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2017年1月12日，百龄坛特醇X华晨宇首次合作，成为其代言人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2017年3月23日，成为六神公布品牌形象代言人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2017年7月13日，雅诗兰黛确认华晨宇为其品牌大使。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2017年7月，成为珠江纯生最新代言人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2017年9月27日，为最火手游王者荣耀创作鲁班七号角色主题曲《智商二五零》。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2017年10月13日-14日，2017火星演唱会在北京五棵松体育馆连开两场。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2017年是入选福布斯30岁以下精英榜唯一纯歌手。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2018年1月23日，确认作为首位补位歌手加盟《歌手》。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2018年4月13日，歌手总决赛歌王之夜以优异表现拿下第二名，整合六季投票率排名华人第一！ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2018年6月，受邀成为三星Galaxy A9 Star系列代言人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2018年9月8日～9月9日，华晨宇火星演唱会在北京鸟巢举办，成为首位在鸟巢连开两场演唱会的内地歌手。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2018年9月成为美拍全新代言人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;截止2018年11月，华晨宇在15年即三年前发行的个人第二张专辑《异类》在台湾五大金榜华语榜连续进榜50周，其中有30+周排名前三，销售额占20%+。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2019年凭借2015年专辑异类荣获台湾五大金榜18年度销量冠军，也是首位内地歌手获此殊荣。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2019年1月参加2018微博之夜，获得年度最佳歌手奖项！ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2019年1月公布#2018腾讯娱乐白皮书#中 ：2018年现场演唱会热度#2018华晨宇火星演唱会# Top1，2018年华语歌手专业技术口碑榜，@华晨宇yu Top7成为唯一上榜的90后歌手。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2019年开年重磅电子刊#华晨宇华氏度#屡创佳绩，最终销量高达505797本，创单人电子刊销量最高纪录。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 他对音乐的态度 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;08042号选手华晨宇，黑框眼镜满脸痘坑人字拖，是我对他的第一印象了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;记得拿一首没有歌词的歌，点燃了评委尚雯婕的神经，坚持让华晨宇晋级。最后，华晨宇获得了那一届的冠军。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;到后来一段时间里，在综艺节目中《花儿与少年》，不善于沟通的华晨宇被称为“巨婴”，一时间网上批判声音不断。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;二十岁出头的年轻人，在娱乐圈中或者张狂或者乖巧，都正常的很。却总是会被贴上一个标签，或坏或好。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于不了解的人，第一印象格外重要，作为明星，一点点的细节都会被镜头凸显放大。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;华晨宇被贴上的标签：“巨婴”、“怪异”、“孤独”。怪和不讨喜一度成为了华晨宇的代名词，我也一度这样认为。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;直到某一天，认真的听完了华晨宇的一首歌才觉得，一个年轻人怪一点，又有什么错。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;不讨好这个世界，才是他音乐最大的特点。华晨宇性格、音乐、人生态度——就是一个艺术品。华晨宇总能演绎出不一样的歌曲，每一首歌都能给人带来最大的惊喜。而《天籁之战》更是让这种惊喜翻倍。;每一首华晨宇改编过的歌曲，都赋予了一种新的灵魂，如果没有听过原唱，不知道名字，当把两首歌放在一起，你绝对不会想到这是同一首歌。认认真真听完华晨宇改编的所有歌曲，想要用千言万语来形容，最终却只汇聚为两个字——鬼才。不管是古典、饶舌、深情款款的情歌、暗黑系的摇滚，只要经过了华晨宇的改编，都只有一种名字——华晨宇的音乐。;而听过华晨宇演绎出来的版本，我瞬间爱上了这首歌词简单到不需要背词的歌。华晨宇打碎了原曲的音乐结构构造了一首新生的歌曲，加入的艾米纳rap元素，更是让听者炸裂，疯狂。面对困难度极高的挑战，不带半点敷衍，释放出的巨大能量，这是华晨宇对音乐的态度。每一首改编歌曲中都深深烙印上了华晨宇的影子。​ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;华晨宇的独特之处就在于，你认识他的时候就认为他的作品已经100分了，可后来每一次呈现出来的作品都要在加1分。每一次的作品，每一次的演唱，都能富裕一首歌新的生命。也许，只有华晨宇自己才能超越华晨宇。2018年《歌手》中，我仿佛看见了五年前那个懵懵懂懂的大男孩的影子，他变了，他没变。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;他的脑袋里藏着一个令人琢磨不透却又异常纯粹的宇宙， 那些奇异的旋律诞生在此，始终和着内心的声音。 不需要太多话语，音乐就是华晨宇最精确的语言， 是他与世界交流的最佳方式。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;或许很多人说他不是走大众音乐路线的。哥哥（张国荣）当初在的时候，多少人也是看不起他的才华，我认识哥哥的作品的时候，太难过了，一边看电影听歌一边心痛得要死。所以当听到华晨宇的歌，以及看到他对音乐的态度以后，我只是不想错过这样一个认认真真对待音乐又有自己独到见解的歌手而已。华晨宇是很尊敬哥哥的，我觉得很好啊，你看他做音乐的态度和那个时候哥哥唱歌拍电影的态度多像啊。我作为一个普通人只是希望自己能看到更多好听好看的作品，所以才不想错过认识每一个我觉得在这些方面有天赋的人的机会。如果错过了，就真的太让人难过了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;等大家都看透了流量明星，这个娱乐圈浮躁的时代都过去了，华晨宇肯定会火的，而且是爆火的那种火。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;他的音乐绝对不小众相信我。包括他的颜值绝对是在线的，只是说能不能欣赏他的长相。其实现在华语乐坛挺缺他这个风格和性格的歌手的。而且他的现场绝对没得讲，多少人都是看live或者live的视频入坑的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;华晨宇，他教会了我对音乐的理解，和对人生的态度。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;音乐方面：以前我听歌是走歌词路线的，只要曲调不难听，歌词写的感同身受的歌曲是我的偏爱类型，也听摇滚，但是只是为了追求一种撕裂与绝望的宣泄。后来“不幸”遇上华晨宇，整个颠覆了我的听歌逻辑，我第一次只通过旋律和音色就感受到情绪，歌词仿佛只是成为了一首歌曲的注释。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我从花花翻唱的《我》里竟然能感受到我之前从不了解的哥哥的那份孑然和孤傲，我也会在《齐天》中为终成大圣虽身有傲骨却心生迷茫的悟空而落泪。在歌词之外，我更懂了怎样去感受音乐的其他魅力，而这正是因华晨宇而开始的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 一个坚持自我不忘初心，对音乐十分专注较真的人是不会被音乐抛弃的。他天赋使然，加上后天努力，这种魅力和态度真的能把人圈的死死的！就是喜欢这样的他，清楚自己想要的，然后坚决地去执行。坚持初心这种东西说起来很容易，能够做到的人真没几个，然而华晨宇做到了，这才是一个优质偶像应该具备的品质。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;【如今这个时代，互联网很发达，时不时唱一首歌，会很容易火遍全网络，这种传播速度太快了，会让人觉得成功太容易了。】清醒且不浮躁，是艺人最难得的品质之一。我庆幸你能在复杂纷乱的圈子始终保持着通透清澈的心，也感动你能在这个快餐时代依然坚持自己的步调。其多不易，唯有你知。​​ 歌单 嘿嘿&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;小强最喜欢中的最爱]]></content>
      <categories>
        <category>音乐</category>
      </categories>
      <tags>
        <tag>音乐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python+离散数学→逻辑演算]]></title>
    <url>%2F2019%2F04%2F02%2F%E9%80%BB%E8%BE%91%E6%BC%94%E7%AE%97%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本篇探讨的是一道逻辑演算推理题。有两种方法，一种是常规的离散数学逻辑演算，另一种则是用python程序来解决。本篇将探究两种方法： python+离散数学→逻辑演算。 问题&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在某次研讨会的中间休息时间，3名与会者根据王教授的口音对他是哪个省市的人判断如下：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;甲:王教授不是苏州人，是上海人&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;乙:王教授不是上海人，是苏州人&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;丙:王教授既不是上海人，也不是杭州人&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;听完这3人的判断后，王教授笑着说，你们3人中有一人说得全对，有一人说对了一半，另一人全不对。试用逻辑演算分析王教授到底是哪里人. 常规推理 设命题&nbsp;&nbsp;&nbsp;&nbsp;p:王教授是苏州人；&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;q:王教授是上海人；&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;r:王教授是杭州人； 用p,q,r表示甲乙丙的观点如下： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;甲：￢p∧q &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;乙：p∧￢q &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;丙：￢q∧￢r 其中一人全对，一人对一半，另一人全错。 即其中一个真命题，两个假命题。先找真命题 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;甲全对：B1=￢p∧q &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;甲对一半：B2=(￢p∧￢q) ∨(p∧q) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;甲全错：B3=p∧￢q &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;乙全对：C1= p∧￢q &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;乙对一半：C2=(￢p∧￢q) ∨(p∧q) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;乙全错：C3=￢p∧q &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;丙全对：D1=￢q∧￢r &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;丙对一半：D2=(q∧￢r) ∨( ￢q∧r) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;丙全错：D3=q∧r 有王教授那句话可以写： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;E=（B1∧C2∧D3）∨(B1∧C3∧D2) ∨(B2∧C1∧D3) ∨(B2∧C3∧D1) ∨(B3∧C1∧D2) ∨(B3∧C2∧D1) 是真命题 而B1∧C2∧D3⇔(￢p∧q) ∧((￢p∧￢q) ∨(p∧q)) ∧(q∧r) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;⇔(￢p∧q) ∧((￢p∧￢q) ∧(q∧r) ∨ (p∧q) ∧(q∧r)) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;⇔(￢p∧q) ∧(0∨(p∧q∧r)) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;⇔(￢p∧q)∧(p∧q∧r) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;⇔0 其他同理类似可得： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B1C3D2⇔￢p∧q∧￢r &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B2C1D3⇔0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B2C3D1⇔0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B3C1D2⇔p∧￢q∧r &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B3C2D1⇔0 所以E⇔(￢p∧q∧￢r) ∨(p∧￢q∧r) 而pqr中只能有一个是真的，所以p∧q⇔0，p∧r⇔0，q∧r⇔0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;E⇔(￢p∧q∧￢r) ∨0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;⇔￢p∧q∧￢r &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;⇔1 所以p为假，q为真，r为假，王教授是上海人。 python程序实现&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以下是上述过程的代码实现，并附有代码详解，有兴趣的小伙伴可以看一看。 1234567891011121314151617181920212223242526272829303132333435for p in range(2): #p:王教授是苏州人 for q in range(2): #q:王教授是上海人 for r in range(2): #r:王教授是杭州人 #以下三行表示p,q,r不可能同时为真的情况 if (p == 1 and q == 1): continue if (p == 1 and r == 1): continue if (q == 1 and r == 1): continue Jia = (not p) and q #甲的判断 Yi = p and (not q) #乙的判断 Bing = (not q) and (not r) #丙的判断 B1 = (not p) and q #甲的判断全对 B2 = ((not p) and (not q))or (p and q) #甲的判断一半对 B3 = p and (not q) #甲的判断全错 C1 = p and (not q) #乙的判断全对 C2 = (p and q) or ((not p) and (not q)) #乙的判断一半对 C3 = (not p) and q #乙的判断全错 D1 = (not q) and (not r) #丙的判断全对 D2 = ((not q) and r) or (q and(not r)) #丙的判断一半对 D3 = q and r #丙的判断全错 #王教授所说的话 E = (B1 and C2 and D3) \ or(B1 and C3 and D2) \ or(B2 and C1 and D3) \ or(B2 and C3 and D1) \ or(B3 and C1 and D2) \ or(B3 and C2 and D1) #符合王教授所的E值 if E==1 : print("%d,%d,%d E=%d,Jia=%d,Yi=%d,Bing=%d"%(p,q,r,E,Jia,Yi,Bing)) 总结&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其实用python解决离散数学的逻辑推理题比直接推理效率更高而且准确性也高，并且在遇到复杂的逻辑推理时，很容易把自己绕晕，半天也解不出来，倒不如我们可以让计算机帮我们解决，只需写几十行代码，就可以罗列出复杂的关系，何乐而不为呢，所以我们可以多学学这种解题方法！😁]]></content>
      <categories>
        <category>python</category>
        <category>离散数学</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>离散数学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OJ刷题-while(scanf("%d",&n)!=EOF)]]></title>
    <url>%2F2019%2F03%2F21%2Fwhile-1%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;“测试输入包含若干测试实例。当N为0时，输入结束，该实例不被处理。”这句话 是最早我对OJ的印象 以前也没见过这种输入要求， 做第一道题的时候就卡住了 上网看别人的代码 都有一句 while(scanf(“%d”,&amp;n)!=EOF)”scanf 函数还能放while里啊… EOF是什么玩意儿呢…” 什么是OJ？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先先介绍下什么是OJ吧(知道的同学可以跳过此部分)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Online Judge系统（简称OJ）是一个在线的判题系统。用户可以在线提交程序多种程序（如C、C++）源代码，系统对源代码进行编译和执行，并通过预先设计的测试数据来检验程序源代码的正确性。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一个用户提交的程序在Online Judge系统下执行时将受到比较严格的限制，包括运行时间限制，内存使用限制和安全限制等。用户程序执行的结果将被Online Judge系统捕捉并保存，然后再转交给一个裁判程序。该裁判程序或者比较用户程序的输出数据和标准输出样例的差别，或者检验用户程序的输出数据是否满足一定的逻辑条件。最后系统返回给用户一个状态：通过（Accepted,AC）、答案错误(Wrong Answer,WA)、超时(Time Limit Exceed,TLE)、超过输出限制（Output Limit Exceed,OLE)、超内存（Memory Limit Exceed,MLE）、运行时错误（Runtime Error,RE）、格式错误（Presentation Error,PE)、或是无法编译（Compile Error,CE），并返回程序使用的内存、运行时间等信息。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Online Judge系统最初使用于ACM-ICPC国际大学生程序设计竞赛和OI信息学奥林匹克竞赛中的自动判题和排名。现广泛应用于世界各地高校学生程序设计的训练、参赛队员的训练和选拔、各种程序设计竞赛以及数据结构和算法的学习和作业的自动提交判断中。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;知名的OJ有：RQNOJ,URAL,SPOJ,vijos,tyvj,USACO,sgu,pku(poj),zju(toj),tju,uva，HDU(HDOJ)等。 EOF是什么东东？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;EOF 是一个宏 ，一般定义为-1。1.EOF用来判断文件结束的标记(end of file) 用在文件操作中,可以查下msdn 看它的定义:EOF is returned by an I/O routine when the end-of-file (or in some cases, an error) is encountered2.EOF表示输入流的结束。3.在发送端套接字关闭后，接收端读套接字的read函数也会返回EOF. while(scanf()!=EOF)流程图 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们学过的有三种循环：for，while，do-while。但while (scanf(“%d”, &amp;n) != EOF)不像以上的任何一种，接下来看一下它的流程图： OnlineJuge的评判时，该语句的作用 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;OJ评判的原理应该是这样的：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;输入：通过管道命令，将一个包含若干测试用例的文件作&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为【标准输入流】，所以需要while(scanf() != EOF)来判断测试文件是否读完。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;输出：通过管道命令，将【标准输出流】，输出到一个文件中。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;评判：将程序的【输出文件】与【正确答案文件】进行比对：如果一样，则返回程序正确提示；否则返回程序错误的提示。 使用原理 有人说 EOF等于-1 其实就是scanf函数的返回值 不等于-1时 继续进行循环 有人说 EOF是处理到文件结束 不用EOF也可以 ………. 很多天后终于把这个弄明白了 scanf的返回值由后面的参数决定 scanf(“%d%d”, &amp;a, &amp;b); 如果a和b都被成功读入，那么scanf的返回值就是2 如果只有a被成功读入，返回值为1 如果a和b都未被成功读入，返回值为0 如果遇到错误或遇到end of file，返回值为EOF，且返回值为int型. 总结 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以上是我个人所述，若有什么不对的地方或者有补充的地方，还希望各位指点指点，大家可以一起学习，一起进步！]]></content>
      <categories>
        <category>OJ</category>
        <category>C语言</category>
      </categories>
      <tags>
        <tag>OJ</tag>
        <tag>C语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[盐于律己，甜以待人]]></title>
    <url>%2F2019%2F03%2F21%2F%E8%87%AA%E6%88%91%E4%BB%8B%E7%BB%8D%20(1)%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这是一篇关于我的自我介绍，感谢各位的支持，小弟初来乍到，请各位江湖大佬多多关照。😊 自我介绍 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2000年中国智造，长176cm，净重60kg。采用人工智能，各部分零件齐全，运转稳定，经十八年的运行，属质量信得过产品。 学习方向&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在是计算机专业的一名学生，所属计算机科学与技术人工智能方向，已学习python和C语言，以及一些其他的计算机学科，学习道路还甚远，让我们一起学习进步！ 兴趣爱好&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;普通男生一名（已有主人），平常喜欢看一些关于计算机方面的东西，也喜欢搞一些稀奇古怪的软件和程序，逛逛别人的博客（互相学习）。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;平常也喜欢到处溜达溜达，跑跑步，喜欢接触学习那些我好奇的东西（偶尔也会思考思考人生😄） 建站目的 记录美好时光 写写博客文章 分享一些使用资源 结识更多的朋友 联系我 QQ：2426545812 奋斗&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;《热爱生命》&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;–汪国真&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我不去想是否能够成功&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;既然选择了远方 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;便只顾风雨兼程 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我不去想能否赢得爱情 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;既然钟情于玫瑰 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;就勇敢地吐露真诚 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我不去想身后会不会袭来寒风冷雨 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;既然目标是地平线 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;留给世界的只能是背影 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我不去想未来是平坦还是泥泞 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;只要热爱生命 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一切，都在意料之中]]></content>
      <categories>
        <category>关于我</category>
      </categories>
      <tags>
        <tag>关于我</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python判断离散数学的合式公式]]></title>
    <url>%2F2019%2F03%2F17%2F%E5%88%A4%E6%96%AD%E5%90%88%E5%BC%8F%E5%85%AC%E5%BC%8F%20(1)%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;你没有听错，用python程序来解决离散数学的逻辑推理问题，我当我第一次听老师说的时候也很吃惊（再说上学期的Python学的也不咋地…..😩)，但经老师讲解后才知道，使用python解题不仅效率高，而且准确性也很强的，所以还是有必要学习以下的。 判断是否为合式公式&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第一个题目就是判断一个公式是否为合式公式，这也是最基础的，因为只有当在输入的合式公式正确的情况下，才能进一步的运算解题，所以先讲解一下怎么判断合式公式。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先要知道什么是合式公式？(1)原子命题常项或变项是合式公式；(2)如果A是合式公式，则（-A）也是合式公式（- 表示非）；(3)如果A，B是合式公式，则（AB）、（A+B）、（A &lt; B）、（ A ~ B）也是合式公式；(此处 合取 + 析取 &lt; 代表条件 ~ 代表双条件)(4)只有有限次地应用(1)～(3)所包含的命题变元，联结词和括号的符号串才是合式公式。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;个人思路：输入字符串，扫描字符串，把所含的各关联词分区出来，在判断每个关联词使用是否正确比如不合规则的情况：(1) 关联词所处位置不对(2) 关联词的连续使用(3) 括号不匹配(4) ……..程序中可能会有bug，希望大佬们多多指教 这道题应该有很多好的方法，但我不太会用，我这里只能暴力判断了。废话不多说，直接上代码😎12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788import reprint("定义‘!’为否定联结词")print("定义‘&amp;’为合取联结词")print("定义‘-’为析取联结词")print("定义‘&gt;’为蕴含联结词")print("定义‘~’为等价联结词")print("如果公式错误，会指出哪里错误；如果公式正确，则什么也不输出")S =input("请输入需要判断的公式：")T = list(S)if S[0] in ['&amp;','-',')','~','&gt;'] : print("不是合式公式")a ,b,c,d,f ="!" , "&amp;" ,"-" , "&gt;" , "~"if a in T: Q = [m.start() for m in re.finditer(a, S)] for i in Q : if S[-1] == '!' : print ("‘!’联结词使用错误，不是合式公式") break if S[i+1] in ['&amp;','!','-',')','~','&gt;']: print("‘!’联结词使用错误，不是合式公式") break if S[i-1] in ['!']: print("‘!’联结词使用错误，不是合式公式！") breakif b in T: W = [m.start() for m in re.finditer(b, S)] for y in W : if S[-1] == '&amp;' : print ("‘&amp;’联结词使用错误，不是合式公式") break if S[y+1] in ['-',')','~','&gt;','&amp;'] : print("‘&amp;’联结词使用错误，不是合式公式") break if S[y-1] in ['-','(','~','&gt;','&amp;'] : print("‘&amp;’联结词使用错误，不是合式公式") breakif c in T: E = [m.start() for m in re.finditer(c, S)] for h in E : if S[-1] == '-' : print("‘-’联结词使用错误，不是合式公式") break if S[h+1] in (')','&gt;','~','-') : print("‘-’联结词使用错误，不是合式公式") break if S[h-1] in ('&amp;','-','&gt;','~') : print("‘-’联结词使用错误，不是合式公式") breakif d in T: R = [m.start() for m in re.finditer(d, S)] for k in R : if S[-1] == '&gt;' : print ("‘&gt;’联结词使用错误，不是合式公式") break if S[k+1] in [')','&gt;'] : print("‘&gt;’联结词使用错误，不是合式公式") break if S[k-1] in ['(','&gt;'] : print("‘&gt;’联结词使用错误，不是合式公式") breakif f in T: O = [m.start() for m in re.finditer(f, S)] for v in O : if S[-1] == '~' : print("‘~’联结词使用错误，不是合式公式") break if S[v+1] in [')','~'] : print("‘~’联结词使用错误，不是合式公式") break if S[v-1] in ['(','~'] : print("‘~’联结词使用错误，不是合式公式") breakif '(' or ')' in S : e = 0 klb =[] for i in S : if i == '(': klb.append(i) if i == ')': if len(klb)==0: e=1 break else : klb.pop() if len(klb)!=0 : e = 1 if e==1: print("括号使用错误，不是合式公式") &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我也是小白，程序哪里可能有些不足之处，请多多指教，如果你有更好的解题方法或思路，可以联系我，大家可以一起学习，一起进步的！😊]]></content>
      <categories>
        <category>python</category>
        <category>离散数学</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>离散数学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客搭建]]></title>
    <url>%2F2019%2F03%2F15%2F%E6%90%AD%E5%BB%BA(2)%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本篇文章用于介绍Hexo个人博客的搭建过程，这也是我搭建本博客后的第一篇文章，分享一下搭建方法，有兴趣的小伙伴也可以自主搭建一个属于自己的博客！ 首先感谢大家的来访支持！&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;自己本来开始做的是html网页，网页做完之后需要购买服务器和域名（价格不便宜）才能运营，还要定期维护它，对于我个人只是感兴趣做个自己的网页，单单做个html网页就很费力气了，实在是没有精力和时间运营它，所以也没有必要去购买服务器和域名（性价比低），但我做的html网页在我自己的电脑上还是可以运营的（感兴趣的小伙伴可以联系我，一起探讨做html网页的方法）。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其次，现在市面上的博客有很多，如CSDN，博客园等平台，可以直接在上面发表，也有很多优点，但缺点是比较不自由，会受到各种限制和恶心的广告。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所以我选择了直接在github page平台上托管我的博客，这样可以安心写作，也不需要定期维护，而且hexo作为一个快速简洁的博客框架，用它来搭建博客也很简单。我在两天内完成了Hexo-Github个人博客的基本搭建（其实搭建不难，自己脑子笨，做的慢些了……）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在这里记录一下我的第搭建一个博客，并给大家分享一下搭建的流程，以及在搭建过程中遇到的问题和解决方案。如果你也有搭建个人博客的想法，希望能给你一点经验帮助。 大家都是新手，可以分享经验，互相交流学习的。 搭建经历 ❤2019-3-13：初次看到Hexo-Github搭建个人博客，很感兴趣，便开始了自己博客的搭建； ❤2019-3-14：完成了博客的基本搭建（可以运营），并上传了第一篇文章； ❤2019-3-17：博客搭建工作全部完结（主题优化，程序添加，分类管理……）; ❤未完待续…… 搭建步骤 安装Git 安装Node.js 安装Hexo 注册Github账号并创建新仓库 生产SSH添加到Github 将Hexo部署到Github 修改主题 优化主题 1.安装Git&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;直接到Git官网下载就行了，右键点击软件使用Git Bash的命令行工具，以后就用这个工具来使用Git 2.安装Node.js&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;安装同Git一样，也是到Node.js官网下载就可以了。 3.安装Hexo&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;使用下载好的Git Bash来下载Hexo，代码如下： 1npm install hexo-cli -g 4.注册Github账号并创建一个新的仓库&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Github是一个很好的开源项目托管平台，大部分人应该都注册的有账号（有账号的小伙伴可以跳过这步），即使没有注册过也没有关系，只需现在注册下就行了，请参考Github基础设置及使用详解，里面的有详细的注册过程,过程也很简单。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;注册过账号后，先创建一个新的仓库（创建的具体方法上面的那个网站也有详细过程，但这里需要注意的是，创建的仓库名一定要是 name.github.io,其中name为你注册的Github的用户名，修改成你自己的。 5.生成SSH添加到Github&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先配置SSH，同样使用Git Bash,输入一下代码： 1ssh-keygen -t rsa -C "邮件地址" &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中”邮件地址”是你注册Github时填写的邮箱（输入时双引号要带上），输入后要连续按回车键，再等待回应。它会回复： 1Enter file in which to save the key (/c/Users/lenovo/.ssh/id_rsa): &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;然后进入(/c/Users/lenovo/.ssh/id_rsa.pub),将里面的内容复制下来，再进入Github官网，到GitHub设置-&gt;SSH and GPG keys-&gt;New SSH key，粘贴此处并确定。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;再回到Git Bash,输入一下代码： 1ssh -T git@github.com &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用于测试SSH是否配置好，看到You&#39;ve successfully authenticated, but GitHub does not provide shell access.则说明配置好了，否则无法使用hexo d。 6. 将Hexo部署到Github&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;初始化Hexo12hexo init Blog cd Blog &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Blog可修该城其他名字；cd Blog 指打开Blog文件夹。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;安装Hexo扩展12npm install hexo-deployer-git --savenpm install &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本地调试123hexo cleanhexo g hexo s &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hexo s是开启本地预览服务，打开浏览器访问 http://localhost:4000 即可看到内容。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上传到Github Pages&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上传之前，打开站点配置文件_config.yml，位于站点根目录下，修改最后一部分为如下部分。 1234deploy: type: git repository: git@github.com:name/name.github.io.git #name修改为你的Github用户名 branch: master &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上传到Github 123hexo cleanhexo g hexo d 7.修改主题&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主题位置在themes目录下，默认为landscape主题。可从官网https://hexo.io/themes/选择各种下载（得看个人喜好了），推荐NexT主题，以下为安装方法。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;安装过程就一行代码，站点根目录下运行。 1git clone https://github.com/theme-next/hexo-theme-next themes/next &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;然后打开站点配置文件_config.yml，找到theme： landscape，把landscape修改为next就可以了。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;到现在你就已经搭建好了一个属于你自己的博客平台了，赶紧去熟悉熟悉它吧。👍 8.主题优化&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其实刚才我们已经搭建好了，并且也可以使用了，但我们仍可以使我们的博客变得更加好看吸引人，以及添加更多的小功能，使我们有更好的体验，也就是进行主题优化。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;但由于主题优化内容比较多，所以在这就先不讲解了，不用担心，我会在后续的博客文章中持续更新关于如何进行主题优化，使我们的博客变得“高大上”。🤞]]></content>
      <categories>
        <category>Hexo</category>
        <category>博客搭建</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>博客搭建</tag>
      </tags>
  </entry>
</search>
