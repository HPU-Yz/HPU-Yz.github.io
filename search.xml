<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[浅谈机器学习-分类和聚类的区别]]></title>
    <url>%2F2019%2F08%2F01%2F%E6%B5%85%E8%B0%88%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%88%86%E7%B1%BB%E5%92%8C%E8%81%9A%E7%B1%BB%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;机器学习中有两类的大问题，一个是分类，一个是聚类。在我们的生活中，我们常常没有过多的去区分这两个概念，觉得聚类就是分类，分类也差不多就是聚类，下面，我们就具体来研究下分类与聚类之间在数据挖掘中本质的区别。 分类分类有如下几种说法，但表达的意思是相同的。 分类（classification）:分类任务就是通过学习得到一个目标函数f，把每个属性集x映射到一个预先定义的类标号y中。 分类是根据一些给定的已知类别标号的样本，训练某种学习机器（即得到某种目标函数），使它能够对未知类别的样本进行分类。这属于supervised learning（监督学习）。 分类：通过学习来得到样本属性与类标号之间的关系。用自己的话来说，就是我们根据已知的一些样本（包括属性与类标号）来得到分类模型（即得到样本属性与类标号之间的函数），然后通过此目标函数来对只包含属性的样本数据进行分类。 分类算法的局限 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分类作为一种监督学习方法，要求必须事先明确知道各个类别的信息，并且断言所有待分类项都有一个类别与之对应。但是很多时候上述条件得不到满足，尤其是在处理海量数据的时候，如果通过预处理使得数据满足分类算法的要求，则代价非常大，这时候可以考虑使用聚类算法。 聚类聚类的相关的一些概念如下 聚类指事先并不知道任何样本的类别标号，希望通过某种算法来把一组未知类别的样本划分成若干类别，聚类的时候，我们并不关心某一类是什么，我们需要实现的目标只是把相似的东西聚到一起，这在机器学习中被称作 unsupervised learning （无监督学习） 通常，人们根据样本间的某种距离或者相似性来定义聚类，即把相似的（或距离近的）样本聚为同一类，而把不相似的（或距离远的）样本归在其他类。 聚类的目标：组内的对象相互之间时相似的（相关的），而不同组中的对象是不同的（不相关的）。组内的相似性越大，组间差别越大，聚类就越好。 分类与聚类的比较 分类：有训练数据，且训练数据包含输入和输出（有监督学习），已知分类的类别（即训练数据的输出）。学习出一个模型，用该模型对未分好类（预测数据）的数据进行预测分类（已知的类别中）。 聚类：训练数据只有输入（无监督学习）。训练过程即预测过程（聚类过程），且不知道类别，甚至不知道有多少个类别，类别的数量需要指定（K-means）,也可以直接通过算法学习出来（DBSCAN）。只能通过特征的相似性对样本分类。该过程即聚类。 聚类分析是研究如何在没有训练的条件下把样本划分为若干类。 在分类中，对于目标数据库中存在哪些类是知道的，要做的就是将每一条记录分别属于哪一类标记出来。 聚类需要解决的问题是将已给定的若干无标记的模式聚集起来使之成为有意义的聚类，聚类是在预先不知道目标数据库到底有多少类的情况下，希望将所有的记录组成不同的类或者说聚类，并且使得在这种分类情况下，以某种度量（例如：距离）为标准的相似性，在同一聚类之间最小化，而在不同聚类之间最大化。 与分类不同，无监督学习不依赖预先定义的类或带类标记的训练实例，需要由聚类学习算法自动确定标记，而分类学习的实例或数据样本有类别标记。]]></content>
      <categories>
        <category>机器学习</category>
        <category>分类</category>
        <category>聚类</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>分类</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈机器学习-回归与分类的区别]]></title>
    <url>%2F2019%2F08%2F01%2F%E6%B5%85%E8%B0%88%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%9B%9E%E5%BD%92%E4%B8%8E%E5%88%86%E7%B1%BB%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;机器学习的主要任务便是聚焦于两个问题：分类和回归。本文将浅谈下两者的区别。 区别&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;回归会给出一个具体的结果，例如房价的数据，根据位置、周边、配套等等这些维度，给出一个房价的预测。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分类相信大家都不会陌生，生活中会见到很多的应用，比如垃圾邮件识别、信用卡发放等等，就是基于数据集，作出二分类或者多分类的选择。 浅层： 两者的的预测目标变量类型不同，回归问题是连续变量，分类问题离散变量。中层： 回归问题是定量问题，分类问题是定性问题。高层： 回归与分类的根本区别在于输出空间是否为一个度量空间。 解释分类和回归的区别在于输出变量的类型。 定量输出称为回归，或者说是连续变量预测；定性输出称为分类，或者说是离散变量预测。 举个例子：预测明天的气温是多少度，这是一个回归任务；预测明天是阴、晴还是雨，就是一个分类任务。 应用场景不同1.回归问题的应用场景 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;回归问题通常是用来预测一个值，如预测房价、未来的天气情况等等，例如一个产品的实际价格为500元，通过回归分析预测值为499元，我们认为这是一个比较好的回归分析。一个比较常见的回归算法是线性回归算法（LR）。另外，回归分析用在神经网络上，其最上层是不需要加上softmax函数的，而是直接对前一层累加即可。回归是对真实值的一种逼近预测。 2.分类问题的应用场景 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分类问题是用于将事物打上一个标签，通常结果为离散值。例如判断一幅图片上的动物是一只猫还是一只狗，分类通常是建立在回归之上，分类的最后一层通常要使用softmax函数进行判断其所属类别。分类并没有逼近的概念，最终正确结果只有一个，错误的就是错误的，不会有相近的概念。最常见的分类方法是逻辑回归，或者叫逻辑分类。 本质&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分类模型和回归模型本质一样，都是要建立映射关系。在实际操作中，我们确实常常将回归问题和分类问题互相转化，即分类模型可将回归模型的输出离散化，回归模型也可将分类模型的输出连续化。（分类问题回归化：逻辑回归；回归问题分类化：年龄预测问题——&gt;年龄段分类问题）]]></content>
      <categories>
        <category>机器学习</category>
        <category>回归</category>
        <category>分类</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>分类</tag>
        <tag>回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于协同过滤（CF）算法的推荐系统]]></title>
    <url>%2F2019%2F07%2F28%2F%E5%9F%BA%E4%BA%8E%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%EF%BC%88CF%EF%BC%89%E7%AE%97%E6%B3%95%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 随着计算机领域技术的高速发展，电子商务时代的普及，个性化的推荐系统深入生活应用的各个方面。个性化推荐算法是推荐系统中最核心的技术，在很大程度上决定了电子商务推荐系统性能的优劣。而协同过滤推荐是个性化推荐系统应用最为广泛的技术，协同过滤推荐主要分为基于用户的协同过滤推荐、基于项目的协同过滤推荐和基于模型的协同过滤推荐。 一、协同过滤算法描述&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;大数据时代产生了海量的数据，里面蕴含了丰富的价值。但是，大数据体量之大、种类之繁以及产生速率之快，海量的数据并不都是有价值的，用户从海量的数据中提取有用的、针对性的信息需要花费很大的时间成本。比如，当你面对如此多的电影列表，你想找到一部最符合自己兴趣的电影，因为电影数量之多，你不可能把所有的电影简介都看一遍。那么怎么解决这个问题呢？电影平台搜集你过去看过的全部电影，分析了解你对什么类型电影感兴趣，然后针对性的把你感兴趣的电影主动的罗列给你，你不用花费太多的精力便可快速找到满足自己需求的电影。推荐算法便是为解决这类实际需求而诞生了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 推荐系统应用数据分析技术，找出用户最可能喜欢的东西推荐给用户，现在很多电子商务网站都有这个应用。目前用的比较多、比较成熟的推荐算法是协同过滤（Collaborative Filtering，简称CF）推荐算法，CF的基本思想是根据用户之前的喜好以及其他兴趣相近的用户的选择来给用户推荐物品。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如图所示，在CF中，用m×n的矩阵表示用户对物品的喜好情况，一般用打分表示用户对物品的喜好程度，分数越高表示越喜欢这个物品，0表示没有买过该物品。图中行表示一个用户，列表示一个物品，Uij表示用户i对物品j的打分情况。CF分为两个过程，一个为预测过程，另一个为推荐过程。预测过程是预测用户对没有购买过的物品的可能打分值，推荐是根据预测阶段的结果推荐用户最可能喜欢的一个或Top-N个物品。 二、协同过滤的实现要实现协同过滤的推荐算法，要进行以下三个步骤： 收集数据 找到相似用户或物品 进行推荐 1、收集数据&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里的数据指的都是用户的历史行为数据，比如用户的购买历史，关注，收藏行为，或者发表了某些评论，给某个物品打了多少分等等，这些都可以用来作为数据供推荐算法使用，服务于推荐算法。需要特别指出的在于，不同的数据准确性不同，在使用时需要考虑到噪音所带来的影响。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;要从用户的行为和偏好中发现规律，并基于此给予推荐，如何收集用户的偏好信息成为系统推荐效果最基础的决定因素。用户有很多方式向系统提供自己的偏好信息，而且不同的应用也可能大不相同，如下图： 2、找到相似用户或物品&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这一步其实就是计算用户间以及物品间的相似度。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关于相似度的计算，现有的几种基本方法都是基于向量（Vector）的，其实也就是计算两个向量的距离，距离越近相似度越大。在推荐的场景中，在用户 - 物品偏好的二维矩阵中，我们可以将一个用户对所有物品的偏好作为一个向量来计算用户之间的相似度，或者将所有用户对某个物品的偏好作为一个向量来计算物品之间的相似度。下面我们详细介绍几种常用的相似度计算方法： 下一步是作出预测。你可以运用下面的公式做一个预测： 3、进行推荐3.1、基于用户的协同过滤推荐(User-based Collaborative Filtering Recommendation) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于用户的协同过滤推荐算法先使用统计技术寻找与目标用户有相同喜好的邻居，然后根据目标用户的邻居的喜好产生向目标用户的推荐。基本原理就是利用用户访问行为的相似性来互相推荐用户可能感兴趣的资源，如图所示： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上图示意出基于用户的协同过滤推荐机制的基本原理，假设用户 A 喜欢物品 A、物品 C，用户 B 喜欢物品 B，用户 C 喜欢物品 A 、物品 C 和物品 D；从这些用户的历史喜好信息中，我们可以发现用户 A 和用户 C 的口味和偏好是比较类似的，同时用户 C 还喜欢物品 D，那么我们可以推断用户 A 可能也喜欢物品 D，因此可以将物品 D 推荐给用户 A。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于用户的协同过滤推荐机制和基于人口统计学的推荐机制都是计算用户的相似度，并基于“邻居”用户群计算推荐，但它们所不同的是如何计算用户的相似度，基于人口统计学的机制只考虑用户本身的特征，而基于用户的协同过滤机制可是在用户的历史偏好的数据上计算用户的相似度，它的基本假设是，喜欢类似物品的用户可能有相同或者相似的口味和偏好。 3.2、基于项目的协同过滤推荐(Item-based Collaborative Filtering Recommendation)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;根据所有用户对物品或者信息的评价，发现物品和物品之间的相似度，然后根据用户的历史偏好信息将类似的物品推荐给该用户，如图所示：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上图表明基于项目的协同过滤推荐的基本原理，用户A喜欢物品A和物品C，用户B喜欢物品A、物品B和物品C，用户C喜欢物品A，从这些用户的历史喜好中可以认为物品A与物品C比较类似，喜欢物品A的都喜欢物品C，基于这个判断用户C可能也喜欢物品C，所以推荐系统将物品C推荐给用户C。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 基于项目的协同过滤推荐和基于内容的协同过滤推荐都是基于物品相似度预测推荐，只是相似度度量的方法不一样，前者是从用户历史的偏好推断，而后者是基于物品本身的属性特征信息。 3.3、UserCF和ItemCF的区别 三、算法实例以ItemCF为例子具体介绍下整个算法流程。算法流程： 构建用户–&gt;物品的倒排； 构建物品与物品的同现矩阵； 计算物品之间的相似度，即计算相似矩阵； 根据用户的历史记录，给用户推荐物品； 算法流程1构建用户–&gt;物品的倒排如下表，行表示用户，列表示物品，1表示用户喜欢该物品 12345678例如python构建的数据格式如下&#123;'A': &#123;'a': '1', 'b': '1', 'd': '1'&#125;, 'B': &#123;'c': '1', 'b': '1', 'e': '1'&#125;, 'C': &#123;'c': '1', 'd': '1'&#125;, 'D': &#123;'c': '1', 'b': '1', 'd': '1'&#125;,'E': &#123;'a': '1', 'd': '1'&#125;&#125; 算法流程2构建物品与物品的同现矩阵 共现矩阵C表示同时喜欢两个物品的用户数，是根据用户物品倒排表计算出来的。如根据上面的用户物品倒排表可以计算出如下的共现矩阵C： 算法流程3计算物品之间的相似度，即计算相似矩阵 其中两个物品之间的相似度如何计算？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;设|N(i)|表示喜欢物品i的用户数，|N(i)⋂N(j)|表示同时喜欢物品i，j的用户数，则物品i与物品j的相似度为： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1)式有一个问题，当物品j是一个很热门的商品时，人人都喜欢，那么wijwij就会很接近于1，即(1)式会让很多物品都和热门商品有一个很大的相似度，所以可以改进一下公式： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;算法流程2中的共现矩阵C其实就是式(2)的分子，矩阵N（用于计算分母）表示喜欢某物品的用户数（是总的用户数），则(2)式中的分母便很容易求解出来了。 矩阵N如下所示： 利用式（2）便能计算物品之间的余弦相似矩阵如下： 算法流程4根据用户的历史记录，给用户推荐物品； 最终推荐的是什么物品，是由预测兴趣度决定的。 物品j预测兴趣度=用户喜欢的物品i的兴趣度×物品i和物品j的相似度 例如：A用户喜欢a，b，d ，兴趣度分别为1,1,1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;推荐c的预测兴趣度=1X0.67+1X0.58=1.25&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;推荐e的预测兴趣度=1X0.58=0.58 四、python实现案例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566from math import sqrtimport operator#1.构建用户--&gt;物品的倒排def loadData(files): data =&#123;&#125; for line in files: user,score,item=line.split(",") #按","将每组数据单独分开,并将每组的数据赋予对应变量 data.setdefault(user,&#123;&#125;) data[user][item]=score print ("----1.用户：物品的倒排----") print (data) return data#2.计算# 2.1 构造物品--&gt;物品的共现矩阵# 2.2 计算物品与物品的相似矩阵def similarity(data): # 2.1 构造物品：物品的共现矩阵 N=&#123;&#125;;#喜欢物品i的总人数 C=&#123;&#125;;#喜欢物品i也喜欢物品j的人数 for user,item in data.items(): for i,score in item.items(): N.setdefault(i,0) N[i]+=1 C.setdefault(i,&#123;&#125;) for j,scores in item.items(): if j not in i: C[i].setdefault(j,0) C[i][j]+=1 print ("---2.构造的共现矩阵---") print ('N:',N) print ('C:',C) #2.2 计算物品与物品的相似矩阵 W=&#123;&#125;; for i,item in C.items(): W.setdefault(i,&#123;&#125;) for j,item2 in item.items(): W[i].setdefault(j,0) W[i][j]=C[i][j]/sqrt(N[i]*N[j]) print ("---3.构造的相似矩阵---") print (W) return W#3.根据用户的历史记录，给用户推荐物品def recommandList(data,W,user,k=3,N=10): rank=&#123;&#125; for i,score in data[user].items():#获得用户user历史记录，如A用户的历史记录为&#123;'a': '1', 'b': '1', 'd': '1'&#125; for j,w in sorted(W[i].items(),key=operator.itemgetter(1),reverse=True)[0:k]:#获得与物品i相似的k个物品 if j not in data[user].keys():#该相似的物品不在用户user的记录里 rank.setdefault(j,0) rank[j]+=float(score) * w print ("---4.推荐----") print (sorted(rank.items(),key=operator.itemgetter(1),reverse=True)[0:N]) return sorted(rank.items(),key=operator.itemgetter(1),reverse=True)[0:N]if __name__=='__main__': #用户，兴趣度，物品 uid_score_bid = ['A,1,a', 'A,1,b', 'A,1,d', 'B,1,b', 'B,1,c', 'B,1,e', 'C,1,c', 'C,1,d', 'D,1,b', 'D,1,c', 'D,1,d', 'E,1,a', 'E,1,d'] data=loadData(uid_score_bid)#获得数据 W=similarity(data)#计算物品相似矩阵 recommandList(data,W,'A',3,10)#推荐 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了方便理解python的实现过程，这里只用了少数数据集并是手输上去的，具体情况运用可再加段代码用于读取并处理数据文件。 五、算法的优缺点&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这个算法实现起来也比较简单，但是在实际应用中有时候也会有问题的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;比如一些非常流行的商品可能很多人都喜欢，这种商品推荐给你就没什么意义了，所以计算的时候需要对这种商品加一个权重或者把这种商品完全去掉也行。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;再有，对于一些通用的东西，比如买书的时候的工具书，如现代汉语词典，新华字典神马的，通用性太强了，推荐也没什么必要了。 UserCF和ItemCF优缺点的对比]]></content>
      <categories>
        <category>机器学习</category>
        <category>协同过滤（CF）</category>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>推荐系统</tag>
        <tag>协同过滤（CF）</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人工神经网络（ANN）]]></title>
    <url>%2F2019%2F07%2F26%2F%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88ANN%EF%BC%89%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;初学人工智能不久，今天碰上了人工神经网（ANN），开始学的时候很懵，一大堆理论、公式、推导…..作为一名小白，还是很痛苦的，不过经过摸索，大概了 解了什么是ANN，公式的推导以及一些其他问题，下面我就总结下自己的理解，一方面作为自己的笔记，日后方便巩固；另一方面，也可以分享给其他有意者。 一、什么是神经网络1.单层神经网络首先以单层神经元为例解释人工神经元是如何工作的&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x1,x2,…, xN：神经元的输入。这些可以从输入层实际观测或者是一个隐藏层的中间值（隐藏层即介于输入与输出之间的所有节点组成的一层。后面讲到多层神经网络是会再跟大家解释的）。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;X0：偏置单元。这是常值添加到激活函数的输入（类似数学里y＝ax+b中使直线不过原点的常数b）。即截距项，通常有＋1值。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;w0,w1,w2,…,wN：对应每个输入的权重。甚至偏置单元也是有权重的。 a:神经元的输出。计算如下：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;式子里的f是已知的激活函数，f使神经网络（单层乃至多层）非常灵活并且具有能估计复杂的非线性关系的能力。在简单情况下可以是一个高斯函数、逻辑函数、双曲线函数或者甚至上是一个线性函数。利用神经网络可让其实现三个基本功能：与、或、非（AND, OR, NOT）。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里引入一个例子：and功能实现如下神经元输出：a = f( -1.5 + x1 + x2 ) 这样大家就很容易理解其工作原理，其实就是对输入值赋予不同权重，经过激活函数输出的过程。 2.多层神经网络2.1 网络结构清楚了单层神经网络，多层神经网络也好理解了，就相当于多个单层的叠加成多层的过程。神经网络分为三种类型的层： 输入层：神经网络最左边的一层，通过这些神经元输入需要训练观察的样本，即初始输入数据的一层。 隐藏层：介于输入与输出之间的所有节点组成的一层。帮助神经网络学习数据间的复杂关系，即对数据进行处理的层。 输出层：由前两层得到神经网络最后一层，即最后结果输出的一层。 2.2 传递函数/激活函数&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;前面每一层输入经过线性变换wx+b后还用到了sigmoid函数，在神经网络的结构中被称为传递函数或者激活函数。除了sigmoid，还有tanh、relu等别的激活函数。激活函数使线性的结果非线性化。 2.3 为什么需要传递函数&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;简单理解上，如果不加激活函数，无论多少层隐层，最终的结果还是原始输入的线性变化，这样一层隐层就可以达到结果，就没有多层感知器的意义了。所以每个隐层都会配一个激活函数，提供非线性变化。 二、BP算法1.BP算法基本思想&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BP算法全称叫作误差反向传播(error Back Propagation，或者也叫作误差逆传播)算法。其算法基本思想为：在上述的前馈网络中，输入信号经输入层输入，通过隐层计算由输出层输出，输出值与标记值比较，若有误差，将误差反向由输出层向输入层传播，在这个过程中，利用梯度下降算法对神经元权值进行调整。 2.BP算法的推导2.1 数学基础理论&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BP算法中核心的数学工具就是微积分的链式求导法则。 2.2推导过程 正向传播求损失，反向传播回传误差 根据误差信号修正每层的权重 f是激活函数；f(netj)是隐层的输出； f(netk）是输出层的输出O; d是target &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;结合BP网络结构，误差由输出展开至输入的过程如下：有了误差E，通过求偏导就可以求得最优的权重。（不要忘记学习率） 3. 举例说明图中元素：两个输入；隐层: b1, w1, w2, w3, w4 (都有初始值）输出层：b2, w5, w6, w7, w8（赋了初始值） 3.1 前向传播则误差： 3.2 反向传播参数更新：求误差对w1的偏导 ：注意，w1对两个输出的误差都有影响通过以上过程可以更新所有权重，就可以再次迭代更新了，直到满足条件。 三、python代码实现&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以上述例子，用python可写出如下代码，并附有详解： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091import numpy as npimport matplotlib.pyplot as pltimport matha=np.array([0.05,0.1]) #a1,a2的输入值weight1=np.array([[0.15,0.25],[0.2,0.3]]) #a1对b1,b2的权重，a2对b1，b2的权重weight2=np.array([[0.4,0.5],[0.45,0.55]]) #b1对c1,c2的权重，b2对c1，c2的权重target=np.array([0.01,0.99])d1=0.35 #输入层的偏置（1）的权重d2=0.6 #隐藏层的偏置（1）的权重β=0.5 #学习效率#一：前向传播#计算输入层到隐藏层的输入值，得矩阵netb1,netb2netb=np.dot(a,weight1)+d1#计算隐藏层的输出值,得到矩阵outb1,outb2m=[]for i in range(len(netb)): outb=1.0 / (1.0 + math.exp(-netb[i])) m.append(outb)m=np.array(m)#计算隐藏层到输出层的输入值，得矩阵netc1,netc2netc=np.dot(m,weight2)+d2#计算隐藏层的输出值,得到矩阵outc1,outc2n=[]for i in range(len(netc)): outc=1.0 / (1.0 + math.exp(-netc[i])) n.append(outc)n=np.array(n)#二：反向传播count=0 #计数e=0 #误差 E=[] #统计误差#梯度下降while True: count+=1 #总误差对w1-w4的偏导 pd1=(-(target[0]-n[0])*n[0]*(1-n[0])*weight2[0][0]-(target[1]-n[1])*n[1]*(1-n[1])*weight2[0][1])*m[0]*(1-m[0])*a[0] pd2=(-(target[0]-n[0])*n[0]*(1-n[0])*weight2[0][0]-(target[1]-n[1])*n[1]*(1-n[1])*weight2[0][1])*m[0]*(1-m[0])*a[1] pd3=(-(target[0]-n[0])*n[0]*(1-n[0])*weight2[1][0]-(target[1]-n[1])*n[1]*(1-n[1])*weight2[0][1])*m[0]*(1-m[0])*a[0] pd4=(-(target[0]-n[0])*n[0]*(1-n[0])*weight2[1][1]-(target[1]-n[1])*n[1]*(1-n[1])*weight2[0][1])*m[0]*(1-m[0])*a[1] weight1[0][0]=weight1[0][0]-β*pd1 weight1[1][0]=weight1[1][0]-β*pd2 weight1[0][1]=weight1[0][1]-β*pd3 weight1[1][1]=weight1[1][1]-β*pd4 #总误差对w5-w8的偏导 pd5=-(target[0]-n[0])*n[0]*(1-n[0])*m[0] pd6=-(target[0]-n[0])*n[0]*(1-n[0])*m[1] pd7=-(target[1]-n[1])*n[1]*(1-n[1])*m[0] pd8=-(target[1]-n[1])*n[1]*(1-n[1])*m[1] weight2[0][0]=weight2[0][0]-β*pd5 weight2[1][0]=weight2[1][0]-β*pd6 weight2[0][1]=weight2[0][1]-β*pd7 weight2[1][1]=weight2[1][1]-β*pd8 netb=np.dot(a,weight1)+d1 m=[] for i in range(len(netb)): outb=1.0 / (1.0 + math.exp(-netb[i])) m.append(outb) m=np.array(m) netc=np.dot(m,weight2)+d2 n=[] for i in range(len(netc)): outc=1.0 / (1.0 + math.exp(-netc[i])) n.append(outc) n=np.array(n) #计算总误差 for j in range(len(n)): e += (target[j]-n[j])**2/2 E.append(e) #判断 if e&lt;0.0000001: break else: e=0print(count)print(e)print(n)plt.plot(range(len(E)),E,label='error')plt.legend() plt.xlabel('time')plt.ylabel('error')plt.show() 四、BP神经网络的优缺点BP神经网络的优点： 非线性映射能力 泛化能力 容错能力，允许输入样本中带有较大误差甚至个别错误。反应正确规律的知识来自全体样本，个别样本中的误差不能左右对权矩阵的调整 BP神经网络的缺陷： 需要的参数过多，而且参数的选择没有有效的方法。确定一个BP神经网络需要知道：网络的层数、每一层神经元的个数和权值。权值可以通过学习得到，如果，隐层神经元数量太多会引起过学习，如果隐层神经元个数太少会引起欠学习。此外学习率的选择也是需要考虑。目前来说，对于参数的确定缺少一个简单有效的方法，所以导致算法很不稳定； 属于监督学习，对于样本有较大依赖性，网络学习的逼近和推广能力与样本有很大关系，如果样本集合代表性差，样本矛盾多，存在冗余样本，网络就很难达到预期的性能； 由于权值是随机给定的，所以BP神经网络具有不可重现性；]]></content>
      <categories>
        <category>机器学习</category>
        <category>BP算法</category>
        <category>人工神经网络（ANN）</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>BP算法</tag>
        <tag>人工神经网络（ANN）</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多种相似度计算的python实现]]></title>
    <url>%2F2019%2F07%2F24%2F%E5%A4%9A%E7%A7%8D%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97%E7%9A%84python%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在机器学习中有很多地方要计算相似度，比如聚类分析和协同过滤。计算相似度的有许多方法，其中有欧几里德距离(欧式距离)、曼哈顿距离、Jaccard系数和皮尔逊相关度等等。我们这里把一些常用的相似度计算方法，用python进行实现以下。大家都是初学者，我认为把公式先写下来，然后再写代码去实现比较好。 欧几里德距离(欧式距离)几个数据集之间的相似度一般是基于每对对象间的距离计算。最常用的当然是欧几里德距离，其公式为：12345678910111213#-*-coding:utf-8 -*-#计算欧几里德距离：def euclidean(p,q):#如果两数据集数目不同，计算两者之间都对应有的数same = 0for i in p: if i in q: same +=1#计算欧几里德距离,并将其标准化e = sum([(p[i] - q[i])**2 for i in range(same)])return 1/(1+e**.5) 我们用数据集可以去算一下： 123p = [1,3,2,3,4,3]q = [1,3,4,3,2,3,4,3]print euclidean(p,q) 得出结果是：0.261203874964 皮尔逊相关度几个数据集中出现异常值的时候，欧几里德距离就不如皮尔逊相关度‘稳定’，它会在出现偏差时倾向于给出更好的结果。其公式为： 123456789101112131415161718192021222324252627-*-coding:utf-8 -*-#计算皮尔逊相关度：def pearson(p,q):#只计算两者共同有的 same = 0 for i in p: if i in q: same +=1 n = same #分别求p，q的和 sumx = sum([p[i] for i in range(n)]) sumy = sum([q[i] for i in range(n)]) #分别求出p，q的平方和 sumxsq = sum([p[i]**2 for i in range(n)]) sumysq = sum([q[i]**2 for i in range(n)]) #求出p，q的乘积和 sumxy = sum([p[i]*q[i] for i in range(n)]) # print sumxy #求出pearson相关系数 up = sumxy - sumx*sumy/n down = ((sumxsq - pow(sumxsq,2)/n)*(sumysq - pow(sumysq,2)/n))**.5 #若down为零则不能计算，return 0 if down == 0 :return 0 r = up/down return r 用同样的数据集去计算： 123p = [1,3,2,3,4,3]q = [1,3,4,3,2,3,4,3]print pearson(p,q) 得出结果是：0.00595238095238 曼哈顿距离曼哈顿距离是另一种相似度计算方法，不是经常需要，但是我们仍然学会如何用python去实现，其公式为： 1234567891011121314#-*-coding:utf-8 -*-#计算曼哈顿距离：def manhattan(p,q):#只计算两者共同有的 same = 0 for i in p: if i in q: same += 1#计算曼哈顿距离 n = same vals = range(n) distance = sum(abs(p[i] - q[i]) for i in vals) return distance 用以上的数据集去计算： 123p = [1,3,2,3,4,3]q = [1,3,4,3,2,3,4,3]print manhattan(p,q) 得出结果为4 小结&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里只讲述了三种相似度的计算方法，事实上还有很多种，由于我也是刚学，其他的方法还不是很了解，以后碰到了再补上。一般情况下，这三种方法是最常用的，足够我们使用了。]]></content>
      <categories>
        <category>机器学习</category>
        <category>相似度</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
        <tag>相似度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[推荐系统之矩阵分解(MF)及其python实现]]></title>
    <url>%2F2019%2F07%2F23%2F%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B9%8B%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3(MF)%E5%8F%8A%E5%85%B6python%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;目前推荐系统中用的最多的就是矩阵分解方法，在Netflix Prize推荐系统大赛中取得突出效果。以用户-项目评分矩阵为例，矩阵分解就是预测出评分矩阵中的缺失值，然后根据预测值以某种方式向用户推荐。今天以“用户-项目评分矩阵R（M×N）”说明矩阵分解方式的原理以及python实现。 一、矩阵分解1.案例引入有如下R(5,4)的打分矩阵：（“-”表示用户没有打分） 其中打分矩阵R(n,m)是n行和m列，n表示user个数，m行表示item个数那么，如何根据目前的矩阵R（5,4）如何对未打分的商品进行评分的预测（如何得到分值为0的用户的打分值）？ ——矩阵分解的思想可以解决这个问题，其实这种思想可以看作是有监督的机器学习问题（回归问题）。 矩阵分解的过程中，,矩阵R可以近似表示为矩阵P与矩阵Q的乘积：矩阵P(n,k)表示n个user和k个特征之间的关系矩阵，这k个特征是一个中间变量，矩阵Q(k,m)的转置是矩阵Q(m,k)，矩阵Q(m,k)表示m个item和K个特征之间的关系矩阵，这里的k值是自己控制的，可以使用交叉验证的方法获得最佳的k值。为了得到近似的R(n,m)，必须求出矩阵P和Q，如何求它们呢？ 2.推导步骤 首先令： 对于式子1的左边项，表示的是r^ 第i行，第j列的元素值，对于如何衡量，我们分解的好坏呢，式子2，给出了衡量标准，也就是损失函数，平方项损失，最后的目标，就是每一个元素(非缺失值)的e(i,j)的总和最小值 使用梯度下降法获得修正的p和q分量： 求解损失函数的负梯度： 根据负梯度的方向更新变量： 不停迭代直到算法最终收敛（直到sum(e^2) &lt;=阈值，即梯度下降结束条件：f(x)的真实值和预测值小于自己设定的阈值） 为了防止过拟合，增加正则化项 3.加入正则项的损失函数求解 通常在求解的过程中，为了能够有较好的泛化能力，会在损失函数中加入正则项，以对参数进行约束，加入正则L2范数的损失函数为：对正则化不清楚的，公式可化为： 使用梯度下降法获得修正的p和q分量： 求解损失函数的负梯度： 根据负梯度的方向更新变量： 4.预测预测利用上述的过程，我们可以得到矩阵和，这样便可以为用户 i 对商品 j 进行打分： 二、python代码实现以下是根据上文的评分例子做的一个矩阵分解算法，并且附有代码详解。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from math import *import numpyimport matplotlib.pyplot as pltdef matrix_factorization(R,P,Q,K,steps=5000,alpha=0.0002,beta=0.02): #矩阵因子分解函数，steps：梯度下降次数；alpha：步长；beta：β。 Q=Q.T # .T操作表示矩阵的转置 result=[] for step in range(steps): #梯度下降 for i in range(len(R)): for j in range(len(R[i])): eij=R[i][j]-numpy.dot(P[i,:],Q[:,j]) # .DOT表示矩阵相乘 for k in range(K): if R[i][j]&gt;0: #限制评分大于零 P[i][k]=P[i][k]+alpha*(2*eij*Q[k][j]-beta*P[i][k]) #增加正则化，并对损失函数求导，然后更新变量P Q[k][j]=Q[k][j]+alpha*(2*eij*P[i][k]-beta*Q[k][j]) #增加正则化，并对损失函数求导，然后更新变量Q eR=numpy.dot(P,Q) e=0 for i in range(len(R)): for j in range(len(R[i])): if R[i][j]&gt;0: e=e+pow(R[i][j]-numpy.dot(P[i,:],Q[:,j]),2) #损失函数求和 for k in range(K): e=e+(beta/2)*(pow(P[i][k],2)+pow(Q[k][j],2)) #加入正则化后的损失函数求和 result.append(e) if e&lt;0.001: #判断是否收敛，0.001为阈值 break return P,Q.T,resultif __name__ == '__main__': #主函数 R=[ #原始矩阵 [5,3,0,1], [4,0,0,1], [1,1,0,5], [1,0,0,4], [0,1,5,4] ] R=numpy.array(R) N=len(R) #原矩阵R的行数 M=len(R[0]) #原矩阵R的列数 K=3 #K值可根据需求改变 P=numpy.random.rand(N,K) #随机生成一个 N行 K列的矩阵 Q=numpy.random.rand(M,K) #随机生成一个 M行 K列的矩阵 nP,nQ,result=matrix_factorization(R,P,Q,K) print(R) #输出原矩阵 R_MF=numpy.dot(nP,nQ.T) print(R_MF) #输出新矩阵 #画图 plt.plot(range(len(result)),result) plt.xlabel("time") plt.ylabel("loss") plt.show()]]></content>
      <categories>
        <category>机器学习</category>
        <category>推荐系统</category>
        <category>矩阵分解</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
        <tag>推荐系统</tag>
        <tag>矩阵分解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[暑期培训第一次测试题总结]]></title>
    <url>%2F2019%2F07%2F22%2F%E6%9A%91%E6%9C%9F%E5%9F%B9%E8%AE%AD%E7%AC%AC%E4%B8%80%E6%AC%A1%E6%B5%8B%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[前言这里是一些暑期培训第一次测试题的部分解释，经过这次测试的摧残，总结备录一下，方便日后回顾复习。 Feeling&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;经过几天的学习，7月22日下午，进行了第一次检测。开始以为会让我们推导一些公式什么的，结果当拿到测试题的时候，一首凉凉送给自己……开始的选择题和填空题还能接受，看到简答题，这都是什么，有种似曾相识的感觉，但就是写不出来。为时四十分钟的考试结束后，不到半个小时成绩就出来了，虽然成绩不那么好，但排名还行，然后学长给我们进行了讲解答疑，发现自己学的有点粗糙，没有注意那些细节性问题和概念，学以致用这方面也是有点差的。 Test 1为什么一般需要划分出额外的校验集(validation set)用于超参数调整，而不选择直接使用测试集(test set)?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;校验集是用于调整超参数的，从而更好的优化训练模型。测试集是用于在完成神经网络训练过程后,为了客观评价模型在其未见过(未曾影响普通参数和超参数选择)的数据上的性能,而且测试集不能提出对参数或者超参数的修改意见,只能作为评价网络性能的一个指标。 （备注：不清楚这三个数据集概念及其作用的，可看下我以前写的关于这些的一篇文章。传送门：） Test 2批量梯度下降(Batch Gradient Descent)和随机梯度下降(Stochastic Gradient Descent)在应对鞍点时有何不同表现？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们要先知道什么是BGD和SGD，从两者的运算方法上，我们就可以得知不同之处。 （1）批量梯度下降法（Batch Gradient Descent） ：在更新参数时都使用所有的样本来进行更新。 优点：全局最优解，能保证每一次更新权值，都能降低损失函数；易于并行实现。 缺点：当样本数目很多时，训练过程会很慢。 （2）随机梯度下降法（Stochastic Gradient Descent）：在更新参数时都使用一个样本来进行更新。每一次跟新参数都用一个样本，更新很多次。如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将参数迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次，这种方式计算复杂度太高。 优点：训练速度快； 缺点：准确度下降，并不是全局最优；不易于并行实现。从迭代的次数上来看，随机梯度下降法迭代的次数较多，在解空间的搜索过程看起来很盲目。噪音很多，使得它并不是每次迭代都向着整体最优化方向。 Test 3当一个模型训练完后若在训练集上的loss非常高，请问如何在不对代码进行全面排查的前提下，以最快速度定位是模型本身的拟合能力不足还是代码的实现存在某种错误？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;废话不多说，直接上图： Test 4假设我们在训练一个使用Sigmoid激活函数的全连接神经网络。在对其权重进行初始化时，为什么一般会倾向于让初始值的绝对值偏小？如果需要这样，为何不直接使用0进行初始化？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于逻辑回归，把权重初始化为0当然也是可以的，但是对于一个神经网络，如果你把权重或者参数都初始化为0，那么梯度下降将不会起作用。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果权值全初始化为0，则无法更新权值。这是由于前向传播中，所有节点输出值均相同，由于此处使用了sigmod激活函数，所以此处所有神经节点输出都为1/2，而在反向传播每个节点输出值对损失函数的偏导时，涉及到对权值相乘后的求和，该项永远为0，故所乘的结果也必然为0，这样在计算权值对算是函数的偏导时，其偏导必然为0，所有权值偏导都为0，那么就不要指望使用梯度下降法能更新权值了，自然神经网络的训练也就无法进行下去了。 Test 5在CNN中梯度不稳定指的是什么？在神经网络训练过程中，为什么会出现梯度消失的问题？如何解决？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;神经网络中的梯度不稳定指的是梯度消失和梯度爆炸问题。（备注：对于这两种问题的具体解释和为什么会出现这种问题，以及解决方法，这里不具体讨论了，我会在以后的文章中具体解释到的。） Test 6为什么在神经网络中使用交叉熵而不是均方差作为误差函数？1. 神经网络中如果预测值与实际值的误差越大，那么在反向传播训练的过程中，各种参数调整的幅度就要更大，从而使训练更快收敛，如果预测值与实际值的误差小，各种参数调整的幅度就要小，从而减少震荡。 2. 使用平方误差损失函数，误差增大参数的梯度会增大，但是当误差很大时，参数的梯度就会又减小了。 3. 使用交叉熵损失是函数，误差越大参数的梯度也越大，能够快速收敛。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（备注：对于结论的推导过程会总结在另一篇文章里。）]]></content>
      <categories>
        <category>测试题</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>测试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[KNN算法及python实现]]></title>
    <url>%2F2019%2F07%2F20%2FK-%E6%9C%80%E8%BF%91%E9%82%BB%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95(KNN)%E5%8F%8Apython%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;KNN算法即K-Nearest Neighbor，也是机器学习十大经典算法之一。前文讲解了K-means算法，今天我们就继续讲KNN算法，两者看起来挺相似的，但区别还是很大的，看完本片文章你就会明白了。 一、引入问题：确定绿色圆是属于红色三角形、还是蓝色正方形？KNN的思想：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从上图中我们可以看到，图中的数据集是良好的数据，即都打好了label，一类是蓝色的正方形，一类是红色的三角形，那个绿色的圆形是我们待分类的数据。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果K=3，那么离绿色点最近的有2个红色三角形和1个蓝色的正方形，这3个点投票，于是绿色的这个待分类点属于红色的三角形&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果K=5，那么离绿色点最近的有2个红色三角形和3个蓝色的正方形，这5个点投票，于是绿色的这个待分类点属于蓝色的正方形&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;即如果一个样本在特征空间中的k个最相邻的样本中，大多数属于某一个类别，则该样本也属于这个类别。我们可以看到，KNN本质是基于一种数据统计的方法！其实很多机器学习算法也是基于数据统计的。 二、KNN算法1.介绍&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;KNN即K-最近邻分类算法（K-Nearest Neighbor），是一种memory-based learning，也叫instance-based learning，属于lazy learning。即它没有明显的前期训练过程，而是程序开始运行时，把数据集加载到内存后，不需要进行训练，就可以开始分类了。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;KNN也是一种监督学习算法，通过计算新数据与训练数据特征值之间的距离，然后选取K(K&gt;=1)个距离最近的邻居进行分类判(投票法)或者回归。若K=1，新数据被简单分配给其近邻的类。 2.步骤1）计算测试数据与各个训练数据之间的距离； (计算距离的方式前文讲k-means时说过，不清楚的可以去查看以下➡传送门) 2）按照距离的递增关系进行排序； 3）选取距离最小的K个点； K值是由自己来确定的 4）确定前K个点所在类别的出现频率； 5）返回前K个点中出现频率最高的类别作为测试数据的预测分类。 说明：对于步骤5的预测分类有以下两种方法 多数表决法：多数表决法类似于投票的过程，也就是在 K 个邻居中选择类别最多的种类作为测试样本的类别。 加权表决法：根据距离的远近，对近邻的投票进行加权，距离越近则权重越大，通过权重计算结果最大值的类为测试样本的类别。 特点1) 非参数统计方法：不需要引入参数2) K的选择：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;K = 1时，将待分类样本划入与其最接近的样本的类。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;K = |X|时，仅根据训练样本进行频率统计，将待分类样本划入最多的类。K需要合理选择，太小容易受干扰，太大增加计算复杂性。3) 算法的复杂度：维度灾难，当维数增加时，所需的训练样本数急剧增加，一般采用降维处理。 三、算法优缺点优点 简单、有效。 重新训练的代价较低(类别体系的变化和训练集的变化，在Web环境和电子商务应用中是很常见的)。 计算时间和空间线性于训练集的规模(在一些场合不算太大)。 由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合。 该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。 缺点 KNN算法是懒散学习方法(lazy learning)，而一些积极学习的算法要快很多。 需要存储全部的训练样本 输出的可解释性不强，例如决策树的可解释性较强。 该算法在分类时有个主要的不足是，当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。该算法只计算最近的邻居样本，某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并不能影响运行结果。可以采用权值的方法(和该样本距离小的邻居权值大)来改进。 计算量较大。目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本。 四、KNN与K-means的区别&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;废话不多说，咱直接上图：相似点：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;虽然两者有很大且别，但两者也有共同之处。都包含了一个过程：给定一个点，在数据集找离它最近的点，即都用到了NN(Nearest Neighbor)算法。 五、python实例实现&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下面引入一个实例，通过python代码具体看下KNN算法的流程。 12345678910111213141516171819202122232425262728293031323334353637from numpy import *import operatordataSet = array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]])labels = ['A','A','B','B']def classify0(inX,dataSet,labels,k): #求出样本集的行数，也就是labels标签的数目 dataSetSize = dataSet.shape[0] #构造输入值和样本集的差值矩阵 diffMat = tile(inX,(dataSetSize,1)) - dataSet #计算欧式距离 sqDiffMat = diffMat**2 sqDistances = sqDiffMat.sum(axis=1) distances = sqDistances**0.5 #求距离从小到大排序的序号 sortedDistIndicies = distances.argsort() #对距离最小的k个点统计对应的样本标签 classCount = &#123;&#125; for i in range(k): #取第i+1邻近的样本对应的类别标签 voteIlabel = labels[sortedDistIndicies[i]] #以标签为key，标签出现的次数为value将统计到的标签及出现次数写进字典 classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1 #对字典按value从大到小排序 sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True) #返回排序后字典中最大value对应的key return sortedClassCount[0][0]if __name__ == '__main__': print(classify0([1.1,0],dataSet,labels,3))]]></content>
      <categories>
        <category>机器学习</category>
        <category>KNN</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>KNN</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K-means算法及python实现]]></title>
    <url>%2F2019%2F07%2F19%2FK-means%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8Apython%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;K-means(Thek-meansalgorithm)是机器学习十大经典算法之一，同时也是最为经典的无监督聚类（Unsupervised Clustering）算法。接触聚类算法，首先需要了解k-means算法的实现原理和步骤。本文将对k-means算法的基本原理和实现实例进行分析。 一.聚类算法的简介&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于”监督学习“(supervised learning)，其训练样本是带有标记信息的，并且监督学习的目的是：对带有标记的数据集进行模型学习，从而便于对新的样本进行分类。而在“无监督学习”(unsupervised learning)中，训练样本的标记信息是未知的，目标是通过对无标记训练样本的学习来揭示数据的内在性质及规律，为进一步的数据分析提供基础。对于无监督学习，应用最广的便是”聚类“(clustering)。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;”聚类算法“试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个“簇”(cluster)，通过这样的划分，每个簇可能对应于一些潜在的概念或类别。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们可以通过下面这个图来理解： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上图是未做标记的样本集，通过他们的分布，我们很容易对上图中的样本做出以下几种划分。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当需要将其划分为两个簇时，即 k=2时：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当需要将其划分为四个簇时，即 k=4 时：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 二.K-means聚类算法&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kmeans算法又名k均值算法,K-means算法中的k表示的是聚类为k个簇，means代表取每一个聚类中数据值的均值作为该簇的中心，或者称为质心，即用每一个的类的质心对该簇进行描述。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其算法思想大致为：先从样本集中随机选取 k个样本作为簇中心，并计算所有样本与这 k个“簇中心”的距离，对于每一个样本，将其划分到与其距离最近的“簇中心”所在的簇中，对于新的簇计算各个簇的新的“簇中心”。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;根据以上描述，我们大致可以猜测到实现kmeans算法的主要四点： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（1）簇个数 k 的选择 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（2）各个样本点到“簇中心”的距离 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（3）根据新划分的簇，更新“簇中心” &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（4）重复上述2、3过程，直至”簇中心”没有移动 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;优缺点： 优点：容易实现 缺点：可能收敛到局部最小值，在大规模数据上收敛较慢 三.K-means算法步骤详解Step1.K值的选择 k 的选择一般是按照实际需求进行决定，或在实现算法时直接给定 k 值。 说明： A.质心数量由用户给出，记为k，k-means最终得到的簇数量也是k B.后来每次更新的质心的个数都和初始k值相等 C.k-means最后聚类的簇个数和用户指定的质心个数相等，一个质心对应一个簇，每个样本只聚类到一个簇里面 D.初始簇为空 Step2.距离度量&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;将对象点分到距离聚类中心最近的那个簇中需要最近邻的度量策略，在欧式空间中采用的是欧式距离，在处理文档中采用的是余弦相似度函数，有时候也采用曼哈顿距离作为度量，不同的情况实用的度量公式是不同的。 2.1.欧式距离 2.2.曼哈顿距离 2.3.余弦相似度&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A与B表示向量(x1,y1)，(x2,y2)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分子为A与B的点乘，分母为二者各自的L2相乘，即将所有维度值的平方相加后开方。 说明：A.经过step2，得到k个新的簇，每个样本都被分到k个簇中的某一个簇B.得到k个新的簇后，当前的质心就会失效，需要计算每个新簇的自己的新质心 Step3.新质心的计算&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于分类后的产生的k个簇，分别计算到簇内其他点距离均值最小的点作为质心（对于拥有坐标的簇可以计算每个簇坐标的均值作为质心） 说明：A.比如一个新簇有3个样本：[[1,4], [2,5], [3,6]]，得到此簇的新质心=[(1+2+3)/3, (4+5+6)/3]B.经过step3，会得到k个新的质心，作为step2中使用的质心 Step4.是否停止K-means&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;质心不再改变，或给定loop最大次数loopLimit 说明：A当每个簇的质心，不再改变时就可以停止k-menasB.当loop次数超过looLimit时，停止k-meansC.只需要满足两者的其中一个条件，就可以停止k-meansC.如果Step4没有结束k-means，就再执行step2-step3-step4D.如果Step4结束了k-means，则就打印(或绘制)簇以及质心 四.python实现+代码详解&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以下是python得实例代码以及代码的详解，应该可以理解的。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import randomimport pandas as pdimport numpy as npimport matplotlib.pyplot as plt# 计算欧拉距离def calcDis(dataSet, centroids, k): clalist=[] for data in dataSet: diff = np.tile(data, (k, 1)) - centroids #相减 (np.tile(a,(2,1))就是把a先沿x轴复制1倍，即没有复制，仍然是 [0,1,2]。 再把结果沿y方向复制2倍得到array([[0,1,2],[0,1,2]])) squaredDiff = diff ** 2 #平方 squaredDist = np.sum(squaredDiff, axis=1) #和 (axis=1表示行) distance = squaredDist ** 0.5 #开根号 clalist.append(distance) clalist = np.array(clalist) #返回一个每个点到质点的距离len(dateSet)*k的数组 return clalist# 计算质心def classify(dataSet, centroids, k): # 计算样本到质心的距离 clalist = calcDis(dataSet, centroids, k) # 分组并计算新的质心 minDistIndices = np.argmin(clalist, axis=1) #axis=1 表示求出每行的最小值的下标 newCentroids = pd.DataFrame(dataSet).groupby(minDistIndices).mean() #DataFramte(dataSet)对DataSet分组，groupby(min)按照min进行统计分类，mean()对分类结果求均值 newCentroids = newCentroids.values # 计算变化量 changed = newCentroids - centroids return changed, newCentroids# 使用k-means分类def kmeans(dataSet, k): # 随机取质心 centroids = random.sample(dataSet, k) # 更新质心 直到变化量全为0 changed, newCentroids = classify(dataSet, centroids, k) while np.any(changed != 0): changed, newCentroids = classify(dataSet, newCentroids, k) centroids = sorted(newCentroids.tolist()) #tolist()将矩阵转换成列表 sorted()排序 # 根据质心计算每个集群 cluster = [] clalist = calcDis(dataSet, centroids, k) #调用欧拉距离 minDistIndices = np.argmin(clalist, axis=1) for i in range(k): cluster.append([]) for i, j in enumerate(minDistIndices): #enymerate()可同时遍历索引和遍历元素 cluster[j].append(dataSet[i]) return centroids, cluster # 创建数据集def createDataSet(): return [[1, 1], [1, 2], [2, 1], [6, 4], [6, 3], [5, 4]]if __name__=='__main__': dataset = createDataSet() centroids, cluster = kmeans(dataset, 2) print('质心为：%s' % centroids) print('集群为：%s' % cluster) for i in range(len(dataset)): plt.scatter(dataset[i][0],dataset[i][1], marker = 'o',color = 'green', s = 40 ,label = '原始点') # 记号形状 颜色 点的大小 设置标签 for j in range(len(centroids)): plt.scatter(centroids[j][0],centroids[j][1],marker='x',color='red',s=50,label='质心') plt.show 五.K-means算法补充1.对初始化敏感，初始质点k给定的不同，可能会产生不同的聚类结果。如下图所示，右边是k=2的结果，这个就正好，而左图是k=3的结果，可以看到右上角得这两个簇应该是可以合并成一个簇的。 改进：对k的选择可以先用一些算法分析数据的分布，如重心和密度等，然后选择合适的k2.使用存在局限性，如下面这种非球状的数据分布就搞不定了：3.数据集比较大的时候，收敛会比较慢。 4.最终会收敛。不管初始点如何选择，最终都会收敛。可是是全局收敛，也可能是局部收敛。 六.小结&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1. 聚类是一种无监督的学习方法。聚类区别于分类，即事先不知道要寻找的内容，没有预先设定好的目标变量。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2. 聚类将数据点归到多个簇中，其中相似的数据点归为同一簇，而不相似的点归为不同的簇。相似度的计算方法有很多，具体的应用选择合适的相似度计算方法 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3. K-means聚类算法，是一种广泛使用的聚类算法，其中k是需要指定的参数，即需要创建的簇的数目，K-means算法中的k个簇的质心可以通过随机的方式获得，但是这些点需要位于数据范围内。在算法中，计算每个点到质心得距离，选择距离最小的质心对应的簇作为该数据点的划分，然后再基于该分配过程后更新簇的质心。重复上述过程，直至各个簇的质心不再变化为止。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4. K-means算法虽然有效，但是容易受到初始簇质心的情况而影响，有可能陷入局部最优解。为了解决这个问题，可以使用另外一种称为二分K-means的聚类算法。二分K-means算法首先将所有数据点分为一个簇；然后使用K-means（k=2）对其进行划分；下一次迭代时，选择使得SSE下降程度最大的簇进行划分；重复该过程，直至簇的个数达到指定的数目为止。实验表明，二分K-means算法的聚类效果要好于普通的K-means聚类算法。]]></content>
      <categories>
        <category>机器学习</category>
        <category>K-means</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
        <tag>K-means</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据集的划分--训练集、验证集和测试集]]></title>
    <url>%2F2019%2F07%2F18%2F%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%88%92%E5%88%86--%E8%AE%AD%E7%BB%83%E9%9B%86%E3%80%81%E9%AA%8C%E8%AF%81%E9%9B%86%E5%92%8C%E6%B5%8B%E8%AF%95%E9%9B%86%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在机器学习中，经常提到训练集和测试集，验证集似有似无。感觉挺好奇的，就仔细查找了文献。以下谈谈训练集、验证集和测试集。 为什么要划分数据集为训练集、验证集和测试集？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;做科研，就要提出问题，找到解决方法，并证明其有效性。这里的工作有3个部分，一个是提出问题，一个是找到解决方法，另一个是证明有效性。每一个部分都可以作为科研的对象，研究的越靠前，则越偏向科学，越靠后，则越偏向技术，因此叫做科学与技术。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在人工智能领域，证明一个模型的有效性，就是对于某一问题，有一些数据，而我们提出的模型可以（部分）解决这个问题，那如何来证明呢？这和我们平时的考试也是一样的，证明我们掌握了某类知识，就是去参加考试。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;好，那么如何设计考试，让这个考试可以较为客观的考察出每个人的能力呢（注意，不是让每个人都得最高分）？回想我们的高中阶段，有一些教材，让我们平时学习其基本知识（训练集），有一些模拟考试，让我们知道我们到底掌握的怎么样，然后再改进我们的学习（验证集），最后的高考决定我们的去向（测试集）。这样的类比，是不是就很清楚了。 训练集、验证集和测试集1. **训练集**：顾名思义指的是用于训练的样本集合,主要用来训练神经网络中的参数。 2. **验证集**：从字面意思理解即为用于验证模型性能的样本集合.不同神经网络在训练集上训练结束后,通过验证集来比较判断各个模型的性能.这里的不同模型主要是指对应不同超参数的神经网络,也可以指完全不同结构的神经网络。 3. **测试集**：对于训练完成的神经网络,测试集用于客观的评价神经网络的性能。 如何划分训练集、验证集和测试集&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这个问题其实非常基础，也非常明确，在Scikit-learn里提供了各种各样的划分方法。无论是单一的训练集、验证集和测试集，还是进行交叉验证，你都会找到调用的方法，因此我们这里主要介绍两种常见的方法。 1. 前人给出训练集、验证集和测试集 对于这种情况，那么只能跟随前人的数据划分进行，一般的比赛也是如此。一定不要使用测试集来调整性能（测试集已知的情况下），尽管存在使用这种方法来提升模型的性能的行为，但是我们并不推荐这么做。最正常的做法应当是使用训练集来学习，并使用验证集来调整超参数。当在验证集上取得最优的模型时，此时就可以使用此模型的超参数来重新训练（训练集+验证集），并用测试集评估最终的性能。 我们首先说明加入验证集重新训练和不加有啥区别，从理论上讲，一方面学习的样本增多，应当是会提升模型性能的，第二，其在验证集上取得最优的模型与验证集的分布的契合度是最高的，因此最终的模型会更接近验证集的分布。 其次再说明验证集和测试集上的性能差异。事实上，在验证集上取得最优的模型，未必在测试集上取得最优。其原因就是训练的模型是否对于该问题有着较好的泛化能力，即没有对验证集产生过拟合现象。正因为有这种情况的发生，才会有人使用测试集的最优值作为最终的结果（而不管验证集的好坏）。 2. 前人没有明确给出数据集的划分 这时候可以采取第一种划分方法，对于样本数较小的数据集，同样可以采取交叉验证的方法。 交叉验证的方法的使用场景有很多，我们这里是针对不同的模型的性能好坏进行评估。 使用交叉验证，可以获得更为客观的性能差异。当使用第一种方法时，我们更建议使用P值来做显著性检验，从而保证性能差异的客观性。而使用第二种方法，即交叉验证时，我们选取其性能表现的均值作为最终的结果，更能体现该模型的泛化能力。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.存在验证集&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里五倍交叉验证是用于进行调参，此时不接触测试集。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据集首先划分出训练集与测试集（可以是4:1或者9:1）。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其次，在训练集中，再划分出验证集（通常也是4:1或者9：1）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;然后对于训练集和验证集进行5折交叉验证，选取出最优的超参数，然后把训练集和验证集一起训练出最终的模型。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.不存在验证集&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该情况通常是对比不同的模型，如自己的模型和别人的模型的性能好坏。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;只需要把数据集划分为训练集和测试集即可，然后选取5次试验的平均值作为最终的性能评价。 验证集和测试集的区别&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那么,训练集、校验集和测试集之间又有什么区别呢?一般而言,训练集与后两者之间较易分辨,校验集和测试集之间的概念较易混淆.个人是从下面的角度来理解的: 神经网络在网络结构确定的情况下,有两部分影响模型最终的性能,一是普通参数(比如权重w和偏置b),另一个是超参数(例如学习率,网络层数).普通参数我们在训练集上进行训练,超参数我们一般人工指定(比较不同超参数的模型在校验集上的性能).那为什么我们不像普通参数一样在训练集上训练超参数呢?(花书给出了解答)一是：超参数一般难以优化(无法像普通参数一样通过梯度下降的方式进行优化).二是：超参数很多时候不适合在训练集上进行训练,例如,如果在训练集上训练能控制模型容量的超参数,这些超参数总会被训练成使得模型容量最大的参数(因为模型容量越大,训练误差越小),所以训练集上训练超参数的结果就是模型绝对过拟合. 正因为超参数无法在训练集上进行训练,因此我们单独设立了一个验证集,用于选择(人工训练)最优的超参数.因为验证集是用于选择超参数的,因此校验集和训练集是独立不重叠的. 测试集是用于在完成神经网络训练过程后,为了客观评价模型在其未见过(未曾影响普通参数和超参数选择)的数据上的性能,因此测试与验证集和训练集之间也是独立不重叠的,而且测试集不能提出对参数或者超参数的修改意见,只能作为评价网络性能的一个指标. 为了方便清楚直观的了解，上一个表格： 综述&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;至此,我们可以将神经网络完整的训练过程归结为一下两个步骤: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.训练普通参数.在训练集(给定超参数)上利用学习算法,训练普通参数,使得模型在训练集上的误差降低到可接受的程度(一般接近人类的水平). &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.’训练’超参数.在验证集上验证网络的generalization error(泛化能力),并根据模型性能对超参数进行调整. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;重复1和2两个步骤,直至网络在验证集上取得较低的generalization error.此时完整的训练过程结束.在完成参数和超参数的训练后,在测试集上测试网络的性能. 附言说到底： 验证集是一定需要的； 如果验证集具有足够泛化代表性，是不需要再整出什么测试集的； 整个测试集往往就是为了在验证集只是非训练集一个小子集的情况下，好奇一下那个靠训练集（训练）和验证集（调参）多次接力训练出来的模型是不是具有了泛化性能，因而加试一下图个确定。]]></content>
      <categories>
        <category>机器学习</category>
        <category>数据集</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>数据集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[梯度下降法的三种形式BGD、SGD、MBGD及python实现]]></title>
    <url>%2F2019%2F07%2F18%2F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E7%9A%84%E4%B8%89%E7%A7%8D%E5%BD%A2%E5%BC%8FBGD%E3%80%81SGD%E3%80%81MBGD%E5%8F%8Apython%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;梯度下降法作为机器学习中较常使用的优化算法，其有着三种不同的形式：批量梯度下降（Batch Gradient Descent）、随机梯度下降（Stochastic Gradient Descent）以及小批量梯度下降（Mini-Batch Gradient Descent）。其中小批量梯度下降法也常用在深度学习中进行模型的训练。接下来，我们将对这三种不同的梯度下降法进行理解。为了便于理解，这里我们将使用只含有一个特征的线性回归来展开。 此时线性回归的假设函数为：对应的目标函数（代价函数）即为： 下图为 J(θ0,θ1)与参数 θ0,θ1 的关系的图： 1、批量梯度下降（Batch Gradient Descent，BGD）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;批量梯度下降法是最原始的形式，它是指在每一次迭代时使用所有样本来进行梯度的更新。从数学上理解如下： （1）对目标函数求偏导： 其中 i=1,2,…,m 表示样本数， j=0,1 表示特征数，这里我们使用了偏置项 x(i)0=1。 （2）每次迭代对参数进行更新： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;注意这里更新时存在一个求和函数，即为对所有样本进行计算处理，可与下文SGD法进行比较。 伪代码形式为： 优点： （1）一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。 （2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。 缺点： （1）当样本数目 mm 很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。 从迭代的次数上来看，BGD迭代的次数相对较少。其迭代的收敛曲线示意图可以表示如下： 批量梯度下降法的python实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import matplotlib.pyplot as pltimport random##样本数据x_train = [150,200,250,300,350,400,600]y_train = [6450,7450,8450,9450,11450,15450,18450]#样本个数m = len(x_train)#步长alpha = 0.00001#循环次数cnt = 0#假设函数为 y=theta0+theta1*xdef h(x): return theta0 + theta1*xtheta0 = 0theta1 = 0#导数diff0=0diff1=0#误差error0=0 error1=0 #每次迭代theta的值retn0 = [] retn1 = [] #退出迭代的条件epsilon=0.00001#批量梯度下降while 1: cnt=cnt+1 diff0=0 diff1=0 #梯度下降 for i in range(m): diff0+=h(x_train[i])-y_train[i] diff1+=(h(x_train[i])-y_train[i])*x_train[i] theta0=theta0-alpha/m*diff0 theta1=theta1-alpha/m*diff1 retn0.append(theta0) retn1.append(theta1) error1=0 #计算迭代误差 for i in range(len(x_train)): error1 += ((theta0 + theta1 * x_train[i])-y_train[i]) ** 2 / 2 #判断是否已收敛 if abs(error1 - error0) &lt; epsilon: break else: error0 = error1# 画图表现plt.title('BGD')plt.plot(range(len(retn0)),retn0,label='theta0')plt.plot(range(len(retn1)),retn1,label='theta1')plt.legend() #显示上面的labelplt.xlabel('time')plt.ylabel('theta')plt.show()plt.plot(x_train,y_train,'bo')plt.plot(x_train,[h(x) for x in x_train],color='k',label='BGD')plt.legend()plt.xlabel('area')plt.ylabel('price')print("批量梯度下降法：theta0=&#123;&#125;,theta1=&#123;&#125;".format(theta0,theta1))print("批量梯度下降法循环次数：&#123;&#125;".format(cnt))plt.show() 2、随机梯度下降（Stochastic Gradient Descent，SGD）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&amp;nbsp;随机梯度下降法不同于批量梯度下降，随机梯度下降是每次迭代使用一个样本来对参数进行更新。使得训练速度加快。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于一个样本的目标函数为： （1）对目标函数求偏导： （2）参数更新： 注意，这里不再有求和符号 伪代码形式为： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;优点： （1）由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;缺点： （1）准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛。 （2）可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势。 （3）不易于并行实现。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;解释一下为什么SGD收敛速度比BGD要快：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;答：这里我们假设有30W个样本，对于BGD而言，每次迭代需要计算30W个样本才能对参数进行一次更新，需要求得最小值可能需要多次迭代（假设这里是10）；而对于SGD，每次更新参数只需要一个样本，因此若使用这30W个样本进行参数更新，则参数会被更新（迭代）30W次，而这期间，SGD就能保证能够收敛到一个合适的最小值上了。也就是说，在收敛时，BGD计算了 10×30W 次，而SGD只计算了 1×30W 次。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从迭代的次数上来看，SGD迭代的次数较少，在解空间的搜索过程看起来很盲目。其迭代的收敛曲线示意图可以表示如下： 随机梯度下降法的python实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import matplotlib.pyplot as pltimport random##样本数据x_train = [150,200,250,300,350,400,600]y_train = [6450,7450,8450,9450,11450,15450,18450]#样本个数m = len(x_train)#步长alpha = 0.00001#循环次数cnt = 0#假设函数为 y=theta0+theta1*xdef h(x): return theta0 + theta1*xtheta0 = 0theta1 = 0#导数diff0=0diff1=0#误差error0=0 error1=0 #每次迭代theta的值retn0 = [] retn1 = [] #退出迭代的条件epsilon=0.00001#随机梯度下降for i in range(1000): cnt=cnt+1 diff0=0 diff1=0 j = random.randint(0, m - 1) diff0=h(x_train[j])-y_train[j] diff1=(h(x_train[j])-y_train[j])*x_train[j] theta0=theta0-alpha/m*diff0 theta1=theta1-alpha/m*diff1 retn0.append(theta0) retn1.append(theta1) error1=0 #计算迭代的误差 for i in range(len(x_train)): error1 += ((theta0 + theta1 * x_train[i])-y_train[i]) ** 2 / 2 #判断是否已收敛 if abs(error1 - error0) &lt; epsilon: break else: error0 = error1# 画图表现 plt.title('SGD')plt.plot(range(len(retn0)),retn0,label='theta0')plt.plot(range(len(retn1)),retn1,label='theta1')plt.legend() #显示上面的labelplt.xlabel('time')plt.ylabel('theta')plt.show()plt.plot(x_train,y_train,'bo')plt.plot(x_train,[h(x) for x in x_train],color='k',label='SGD')plt.legend()plt.xlabel('area')plt.ylabel('price')print("随机梯度下降法：theta0=&#123;&#125;,theta1=&#123;&#125;".format(theta0,theta1))print("随机梯度下降法循环次数：&#123;&#125;".format(cnt))plt.show() 3、小批量梯度下降（Mini-Batch Gradient Descent, MBGD）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;小批量梯度下降，是对批量梯度下降以及随机梯度下降的一个折中办法。其思想是：每次迭代 使用 batch_size 个样本来对参数进行更新。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里我们假设 batchsize=10，样本数 m=1000 。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;伪代码形式为： 优点： （1）通过矩阵运算，每次在一个batch上优化神经网络参数并不会比单个数据慢太多。 （2）每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果。(比如上例中的30W，设置batch_size=100时，需要迭代3000次，远小于SGD的30W次) （3）可实现并行化。 缺点： （1）batch_size的不当选择可能会带来一些问题。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;batcha_size的选择带来的影响： （1）在合理地范围内，增大batch_size的好处： a. 内存利用率提高了，大矩阵乘法的并行化效率提高。 b. 跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。 c. 在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。 （2）盲目增大batch_size的坏处： a. 内存利用率提高了，但是内存容量可能撑不住了。 b. 跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。 c. Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下图显示了三种梯度下降算法的收敛过程： 4、总结Batch gradient descent: Use all examples in each iteration； Stochastic gradient descent: Use 1 example in each iteration； Mini-batch gradient descent: Use b examples in each iteration.]]></content>
      <categories>
        <category>梯度下降</category>
        <category>机器学习</category>
        <category>python</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>python</tag>
        <tag>梯度下降</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[送给她]]></title>
    <url>%2F2019%2F04%2F13%2F%E9%80%81%E7%BB%99%E5%A5%B9%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;此篇献给傻乎乎的小强，收录了小强最喜爱的华晨宇的歌曲，希望小强傻强有傻福。😄&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 凭数据说话&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年，武音有史以来在校学生个人节目上春晚第一人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年，出道一年十大代言。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年，继在《花儿与少年》的出色表现火爆银屏后，相继受邀登上《我们都爱笑》《天天向上》《快乐大本营》等众多湖南卫视王牌节目，因他的亮相节目收视率飙升同时段第一名。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年，首张个人专辑《卡西莫多的礼物》出专辑速度打破了往届纪录。 2014年，刚出道以最快速度开万人演唱会的第一人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年，华语乐坛在千人演出场馆召开发布会的第一人，全部采用演唱会配置。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年，芒果TV和QQ音乐两平台共计线上演唱会直播门票购买次数超过12万，刷新此前演唱会的在线付费观看人数。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年，Why nobody fights 开启了新网络媒体音乐版权付费时代，成为当代音乐第一人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年，首创音乐故事录音带，时长达到40分钟。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年，新浪微博客户端首创下拉出现Q版明星形象第一人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年，第一位广告打在了“北京世贸天阶的大LED、亚洲超大LED”世贸天阶的艺人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年，华晨宇火星北京演唱会门票1分35秒全部抢光！票务网站个人演唱会预定同时在线人数达到75312人的最高记录。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年成为天娱传媒旗下第一位进行O2O线上付费演唱会直播的艺人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年，首位荣登L’Uomo Vogue的内地男歌手。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2014年，韩国知名ALO眼镜首位中国代言人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2015年，2015最具潜力青春男偶TOP10内唯一的纯歌手。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2015年，受邀观礼格莱美。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2015年，个人演唱会连开三场，成为内地第一人！5月23日第一场开票，16万人在线，35秒万张票抢空，创线上售票新记录；5月30日第二场开票，在线人数16万，1分12秒再度秒杀！今日宣布7.31加开第三场。（获大麦网麦盘点2015年度售罄时间最快演唱会） &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2015年，成为华语乐坛在大型游轮上举办发布会第一人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2015年，上海“火星”演唱会8.1日打破LiveMusic直播264万在线人数记录，成为腾讯视频历史在线人数第一，相当于坐满220个上海大舞台。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2015年，《蜉蝣》蝉联了亚洲榜13周周榜冠军，成为亚洲新歌榜创榜以来第一位连续十周歌曲位居首位的歌手。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2015年，首张数字专辑《终极双主打》上线仅10分钟就以迅猛之势突破10万销量，刷新内地数字专辑上线10分钟的最高销售纪录。一小时后，售卖成绩正式突破30万，以绝对优势打破平台预售的历史纪录。当日24小时购买数量突破35万，再次缔造内地数字专辑数据传奇。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2015年，在数字专发行状态下又发行了个人实体大碟《异类》，有胆量同步发行实体专辑和数字专辑的艺人当属华晨宇第一人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2015年，国内首位入驻二次元空间A站的艺人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2015年，福布斯中国名人榜排91名！ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2016年，90后男艺人新媒体人气指数前十内唯一纯歌手（艺人新媒体指数＝参演电视剧每日播放量×A+微博数据×B+贴吧数据×C+豆瓣数据×D+搜索量×E+其他×F）华晨宇A零分的情况下进了前十。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2016年，凭借《异类》《蜉蝣》《国王与乞丐》等五首新歌在亚洲新歌榜共夺得12周周榜冠军，成为获得周榜冠军次数最多的歌手，并荣获““亚洲新歌榜2015冠军王”。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2016年，成为内地乐坛第一位启用四面台开唱的歌手。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2016年，上海站演唱会缔造乐视全屏终端最高9万人付费在线观看纪录。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2016年，出道3年八场万人演唱会！无赞助不赠票，场场爆满。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2016年，三首歌曲荣获亚洲新歌榜年度十大金曲，风头无两。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2016年，《我的滑板鞋2016》占据香港和台湾的热搜NO.1，在Youtube热门排行榜中，《我的滑板鞋2016》排名台湾热门第一，香港热门前三。谷歌、脸书、Instagram、推特、YouTube等境外网络媒体上全面刮起“滑板鞋”飓风，数据惊人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2016年，成为了MAMA亚洲最佳艺人大奖迄今为止最年轻的获奖者。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2017年1月12日，百龄坛特醇X华晨宇首次合作，成为其代言人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2017年3月23日，成为六神公布品牌形象代言人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2017年7月13日，雅诗兰黛确认华晨宇为其品牌大使。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2017年7月，成为珠江纯生最新代言人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2017年9月27日，为最火手游王者荣耀创作鲁班七号角色主题曲《智商二五零》。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2017年10月13日-14日，2017火星演唱会在北京五棵松体育馆连开两场。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2017年是入选福布斯30岁以下精英榜唯一纯歌手。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2018年1月23日，确认作为首位补位歌手加盟《歌手》。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2018年4月13日，歌手总决赛歌王之夜以优异表现拿下第二名，整合六季投票率排名华人第一！ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2018年6月，受邀成为三星Galaxy A9 Star系列代言人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2018年9月8日～9月9日，华晨宇火星演唱会在北京鸟巢举办，成为首位在鸟巢连开两场演唱会的内地歌手。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2018年9月成为美拍全新代言人。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;截止2018年11月，华晨宇在15年即三年前发行的个人第二张专辑《异类》在台湾五大金榜华语榜连续进榜50周，其中有30+周排名前三，销售额占20%+。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2019年凭借2015年专辑异类荣获台湾五大金榜18年度销量冠军，也是首位内地歌手获此殊荣。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2019年1月参加2018微博之夜，获得年度最佳歌手奖项！ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2019年1月公布#2018腾讯娱乐白皮书#中 ：2018年现场演唱会热度#2018华晨宇火星演唱会# Top1，2018年华语歌手专业技术口碑榜，@华晨宇yu Top7成为唯一上榜的90后歌手。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2019年开年重磅电子刊#华晨宇华氏度#屡创佳绩，最终销量高达505797本，创单人电子刊销量最高纪录。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 他对音乐的态度 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;08042号选手华晨宇，黑框眼镜满脸痘坑人字拖，是我对他的第一印象了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;记得拿一首没有歌词的歌，点燃了评委尚雯婕的神经，坚持让华晨宇晋级。最后，华晨宇获得了那一届的冠军。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;到后来一段时间里，在综艺节目中《花儿与少年》，不善于沟通的华晨宇被称为“巨婴”，一时间网上批判声音不断。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;二十岁出头的年轻人，在娱乐圈中或者张狂或者乖巧，都正常的很。却总是会被贴上一个标签，或坏或好。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于不了解的人，第一印象格外重要，作为明星，一点点的细节都会被镜头凸显放大。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;华晨宇被贴上的标签：“巨婴”、“怪异”、“孤独”。怪和不讨喜一度成为了华晨宇的代名词，我也一度这样认为。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;直到某一天，认真的听完了华晨宇的一首歌才觉得，一个年轻人怪一点，又有什么错。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;不讨好这个世界，才是他音乐最大的特点。华晨宇性格、音乐、人生态度——就是一个艺术品。华晨宇总能演绎出不一样的歌曲，每一首歌都能给人带来最大的惊喜。而《天籁之战》更是让这种惊喜翻倍。;每一首华晨宇改编过的歌曲，都赋予了一种新的灵魂，如果没有听过原唱，不知道名字，当把两首歌放在一起，你绝对不会想到这是同一首歌。认认真真听完华晨宇改编的所有歌曲，想要用千言万语来形容，最终却只汇聚为两个字——鬼才。不管是古典、饶舌、深情款款的情歌、暗黑系的摇滚，只要经过了华晨宇的改编，都只有一种名字——华晨宇的音乐。;而听过华晨宇演绎出来的版本，我瞬间爱上了这首歌词简单到不需要背词的歌。华晨宇打碎了原曲的音乐结构构造了一首新生的歌曲，加入的艾米纳rap元素，更是让听者炸裂，疯狂。面对困难度极高的挑战，不带半点敷衍，释放出的巨大能量，这是华晨宇对音乐的态度。每一首改编歌曲中都深深烙印上了华晨宇的影子。​ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;华晨宇的独特之处就在于，你认识他的时候就认为他的作品已经100分了，可后来每一次呈现出来的作品都要在加1分。每一次的作品，每一次的演唱，都能富裕一首歌新的生命。也许，只有华晨宇自己才能超越华晨宇。2018年《歌手》中，我仿佛看见了五年前那个懵懵懂懂的大男孩的影子，他变了，他没变。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;他的脑袋里藏着一个令人琢磨不透却又异常纯粹的宇宙， 那些奇异的旋律诞生在此，始终和着内心的声音。 不需要太多话语，音乐就是华晨宇最精确的语言， 是他与世界交流的最佳方式。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;或许很多人说他不是走大众音乐路线的。哥哥（张国荣）当初在的时候，多少人也是看不起他的才华，我认识哥哥的作品的时候，太难过了，一边看电影听歌一边心痛得要死。所以当听到华晨宇的歌，以及看到他对音乐的态度以后，我只是不想错过这样一个认认真真对待音乐又有自己独到见解的歌手而已。华晨宇是很尊敬哥哥的，我觉得很好啊，你看他做音乐的态度和那个时候哥哥唱歌拍电影的态度多像啊。我作为一个普通人只是希望自己能看到更多好听好看的作品，所以才不想错过认识每一个我觉得在这些方面有天赋的人的机会。如果错过了，就真的太让人难过了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;等大家都看透了流量明星，这个娱乐圈浮躁的时代都过去了，华晨宇肯定会火的，而且是爆火的那种火。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;他的音乐绝对不小众相信我。包括他的颜值绝对是在线的，只是说能不能欣赏他的长相。其实现在华语乐坛挺缺他这个风格和性格的歌手的。而且他的现场绝对没得讲，多少人都是看live或者live的视频入坑的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;华晨宇，他教会了我对音乐的理解，和对人生的态度。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;音乐方面：以前我听歌是走歌词路线的，只要曲调不难听，歌词写的感同身受的歌曲是我的偏爱类型，也听摇滚，但是只是为了追求一种撕裂与绝望的宣泄。后来“不幸”遇上华晨宇，整个颠覆了我的听歌逻辑，我第一次只通过旋律和音色就感受到情绪，歌词仿佛只是成为了一首歌曲的注释。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我从花花翻唱的《我》里竟然能感受到我之前从不了解的哥哥的那份孑然和孤傲，我也会在《齐天》中为终成大圣虽身有傲骨却心生迷茫的悟空而落泪。在歌词之外，我更懂了怎样去感受音乐的其他魅力，而这正是因华晨宇而开始的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 一个坚持自我不忘初心，对音乐十分专注较真的人是不会被音乐抛弃的。他天赋使然，加上后天努力，这种魅力和态度真的能把人圈的死死的！就是喜欢这样的他，清楚自己想要的，然后坚决地去执行。坚持初心这种东西说起来很容易，能够做到的人真没几个，然而华晨宇做到了，这才是一个优质偶像应该具备的品质。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;【如今这个时代，互联网很发达，时不时唱一首歌，会很容易火遍全网络，这种传播速度太快了，会让人觉得成功太容易了。】清醒且不浮躁，是艺人最难得的品质之一。我庆幸你能在复杂纷乱的圈子始终保持着通透清澈的心，也感动你能在这个快餐时代依然坚持自己的步调。其多不易，唯有你知。​​ 歌单 嘿嘿&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;小强最喜欢中的最爱]]></content>
      <categories>
        <category>音乐</category>
      </categories>
      <tags>
        <tag>音乐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python+离散数学→逻辑演算]]></title>
    <url>%2F2019%2F04%2F02%2F%E9%80%BB%E8%BE%91%E6%BC%94%E7%AE%97%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本篇探讨的是一道逻辑演算推理题。有两种方法，一种是常规的离散数学逻辑演算，另一种则是用python程序来解决。本篇将探究两种方法： python+离散数学→逻辑演算。 问题&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在某次研讨会的中间休息时间，3名与会者根据王教授的口音对他是哪个省市的人判断如下：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;甲:王教授不是苏州人，是上海人&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;乙:王教授不是上海人，是苏州人&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;丙:王教授既不是上海人，也不是杭州人&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;听完这3人的判断后，王教授笑着说，你们3人中有一人说得全对，有一人说对了一半，另一人全不对。试用逻辑演算分析王教授到底是哪里人. 常规推理 设命题&nbsp;&nbsp;&nbsp;&nbsp;p:王教授是苏州人；&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;q:王教授是上海人；&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;r:王教授是杭州人； 用p,q,r表示甲乙丙的观点如下： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;甲：￢p∧q &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;乙：p∧￢q &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;丙：￢q∧￢r 其中一人全对，一人对一半，另一人全错。 即其中一个真命题，两个假命题。先找真命题 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;甲全对：B1=￢p∧q &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;甲对一半：B2=(￢p∧￢q) ∨(p∧q) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;甲全错：B3=p∧￢q &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;乙全对：C1= p∧￢q &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;乙对一半：C2=(￢p∧￢q) ∨(p∧q) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;乙全错：C3=￢p∧q &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;丙全对：D1=￢q∧￢r &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;丙对一半：D2=(q∧￢r) ∨( ￢q∧r) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;丙全错：D3=q∧r 有王教授那句话可以写： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;E=（B1∧C2∧D3）∨(B1∧C3∧D2) ∨(B2∧C1∧D3) ∨(B2∧C3∧D1) ∨(B3∧C1∧D2) ∨(B3∧C2∧D1) 是真命题 而B1∧C2∧D3⇔(￢p∧q) ∧((￢p∧￢q) ∨(p∧q)) ∧(q∧r) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;⇔(￢p∧q) ∧((￢p∧￢q) ∧(q∧r) ∨ (p∧q) ∧(q∧r)) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;⇔(￢p∧q) ∧(0∨(p∧q∧r)) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;⇔(￢p∧q)∧(p∧q∧r) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;⇔0 其他同理类似可得： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B1C3D2⇔￢p∧q∧￢r &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B2C1D3⇔0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B2C3D1⇔0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B3C1D2⇔p∧￢q∧r &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B3C2D1⇔0 所以E⇔(￢p∧q∧￢r) ∨(p∧￢q∧r) 而pqr中只能有一个是真的，所以p∧q⇔0，p∧r⇔0，q∧r⇔0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;E⇔(￢p∧q∧￢r) ∨0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;⇔￢p∧q∧￢r &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;⇔1 所以p为假，q为真，r为假，王教授是上海人。 python程序实现&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以下是上述过程的代码实现，并附有代码详解，有兴趣的小伙伴可以看一看。 1234567891011121314151617181920212223242526272829303132333435for p in range(2): #p:王教授是苏州人 for q in range(2): #q:王教授是上海人 for r in range(2): #r:王教授是杭州人 #以下三行表示p,q,r不可能同时为真的情况 if (p == 1 and q == 1): continue if (p == 1 and r == 1): continue if (q == 1 and r == 1): continue Jia = (not p) and q #甲的判断 Yi = p and (not q) #乙的判断 Bing = (not q) and (not r) #丙的判断 B1 = (not p) and q #甲的判断全对 B2 = ((not p) and (not q))or (p and q) #甲的判断一半对 B3 = p and (not q) #甲的判断全错 C1 = p and (not q) #乙的判断全对 C2 = (p and q) or ((not p) and (not q)) #乙的判断一半对 C3 = (not p) and q #乙的判断全错 D1 = (not q) and (not r) #丙的判断全对 D2 = ((not q) and r) or (q and(not r)) #丙的判断一半对 D3 = q and r #丙的判断全错 #王教授所说的话 E = (B1 and C2 and D3) \ or(B1 and C3 and D2) \ or(B2 and C1 and D3) \ or(B2 and C3 and D1) \ or(B3 and C1 and D2) \ or(B3 and C2 and D1) #符合王教授所的E值 if E==1 : print("%d,%d,%d E=%d,Jia=%d,Yi=%d,Bing=%d"%(p,q,r,E,Jia,Yi,Bing)) 总结&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其实用python解决离散数学的逻辑推理题比直接推理效率更高而且准确性也高，并且在遇到复杂的逻辑推理时，很容易把自己绕晕，半天也解不出来，倒不如我们可以让计算机帮我们解决，只需写几十行代码，就可以罗列出复杂的关系，何乐而不为呢，所以我们可以多学学这种解题方法！😁]]></content>
      <categories>
        <category>python</category>
        <category>离散数学</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>离散数学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OJ刷题-while(scanf("%d",&n)!=EOF)]]></title>
    <url>%2F2019%2F03%2F21%2Fwhile-1%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;“测试输入包含若干测试实例。当N为0时，输入结束，该实例不被处理。”这句话 是最早我对OJ的印象 以前也没见过这种输入要求， 做第一道题的时候就卡住了 上网看别人的代码 都有一句 while(scanf(“%d”,&amp;n)!=EOF)”scanf 函数还能放while里啊… EOF是什么玩意儿呢…” 什么是OJ？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先先介绍下什么是OJ吧(知道的同学可以跳过此部分)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Online Judge系统（简称OJ）是一个在线的判题系统。用户可以在线提交程序多种程序（如C、C++）源代码，系统对源代码进行编译和执行，并通过预先设计的测试数据来检验程序源代码的正确性。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一个用户提交的程序在Online Judge系统下执行时将受到比较严格的限制，包括运行时间限制，内存使用限制和安全限制等。用户程序执行的结果将被Online Judge系统捕捉并保存，然后再转交给一个裁判程序。该裁判程序或者比较用户程序的输出数据和标准输出样例的差别，或者检验用户程序的输出数据是否满足一定的逻辑条件。最后系统返回给用户一个状态：通过（Accepted,AC）、答案错误(Wrong Answer,WA)、超时(Time Limit Exceed,TLE)、超过输出限制（Output Limit Exceed,OLE)、超内存（Memory Limit Exceed,MLE）、运行时错误（Runtime Error,RE）、格式错误（Presentation Error,PE)、或是无法编译（Compile Error,CE），并返回程序使用的内存、运行时间等信息。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Online Judge系统最初使用于ACM-ICPC国际大学生程序设计竞赛和OI信息学奥林匹克竞赛中的自动判题和排名。现广泛应用于世界各地高校学生程序设计的训练、参赛队员的训练和选拔、各种程序设计竞赛以及数据结构和算法的学习和作业的自动提交判断中。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;知名的OJ有：RQNOJ,URAL,SPOJ,vijos,tyvj,USACO,sgu,pku(poj),zju(toj),tju,uva，HDU(HDOJ)等。 EOF是什么东东？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;EOF 是一个宏 ，一般定义为-1。1.EOF用来判断文件结束的标记(end of file) 用在文件操作中,可以查下msdn 看它的定义:EOF is returned by an I/O routine when the end-of-file (or in some cases, an error) is encountered2.EOF表示输入流的结束。3.在发送端套接字关闭后，接收端读套接字的read函数也会返回EOF. while(scanf()!=EOF)流程图 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们学过的有三种循环：for，while，do-while。但while (scanf(“%d”, &amp;n) != EOF)不像以上的任何一种，接下来看一下它的流程图： OnlineJuge的评判时，该语句的作用 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;OJ评判的原理应该是这样的：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;输入：通过管道命令，将一个包含若干测试用例的文件作&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为【标准输入流】，所以需要while(scanf() != EOF)来判断测试文件是否读完。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;输出：通过管道命令，将【标准输出流】，输出到一个文件中。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;评判：将程序的【输出文件】与【正确答案文件】进行比对：如果一样，则返回程序正确提示；否则返回程序错误的提示。 使用原理 有人说 EOF等于-1 其实就是scanf函数的返回值 不等于-1时 继续进行循环 有人说 EOF是处理到文件结束 不用EOF也可以 ………. 很多天后终于把这个弄明白了 scanf的返回值由后面的参数决定 scanf(“%d%d”, &amp;a, &amp;b); 如果a和b都被成功读入，那么scanf的返回值就是2 如果只有a被成功读入，返回值为1 如果a和b都未被成功读入，返回值为0 如果遇到错误或遇到end of file，返回值为EOF，且返回值为int型. 总结 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以上是我个人所述，若有什么不对的地方或者有补充的地方，还希望各位指点指点，大家可以一起学习，一起进步！]]></content>
      <categories>
        <category>OJ</category>
        <category>C语言</category>
      </categories>
      <tags>
        <tag>OJ</tag>
        <tag>C语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[盐于律己，甜以待人]]></title>
    <url>%2F2019%2F03%2F21%2F%E8%87%AA%E6%88%91%E4%BB%8B%E7%BB%8D%20(1)%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这是一篇关于我的自我介绍，感谢各位的支持，小弟初来乍到，请各位江湖大佬多多关照。😊 自我介绍 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2000年中国智造，长176cm，净重60kg。采用人工智能，各部分零件齐全，运转稳定，经十八年的运行，属质量信得过产品。 学习方向&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在是计算机专业的一名学生，所属计算机科学与技术人工智能方向，已学习python和C语言，以及一些其他的计算机学科，学习道路还甚远，让我们一起学习进步！ 兴趣爱好&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;普通男生一名（已有主人），平常喜欢看一些关于计算机方面的东西，也喜欢搞一些稀奇古怪的软件和程序，逛逛别人的博客（互相学习）。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;平常也喜欢到处溜达溜达，跑跑步，喜欢接触学习那些我好奇的东西（偶尔也会思考思考人生😄） 建站目的 记录美好时光 写写博客文章 分享一些使用资源 结识更多的朋友 联系我 QQ：2426545812 奋斗&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;《热爱生命》&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;–汪国真&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我不去想是否能够成功&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;既然选择了远方 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;便只顾风雨兼程 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我不去想能否赢得爱情 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;既然钟情于玫瑰 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;就勇敢地吐露真诚 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我不去想身后会不会袭来寒风冷雨 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;既然目标是地平线 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;留给世界的只能是背影 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我不去想未来是平坦还是泥泞 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;只要热爱生命 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一切，都在意料之中]]></content>
      <categories>
        <category>关于我</category>
      </categories>
      <tags>
        <tag>关于我</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python判断离散数学的合式公式]]></title>
    <url>%2F2019%2F03%2F17%2F%E5%88%A4%E6%96%AD%E5%90%88%E5%BC%8F%E5%85%AC%E5%BC%8F%20(1)%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;你没有听错，用python程序来解决离散数学的逻辑推理问题，我当我第一次听老师说的时候也很吃惊（再说上学期的Python学的也不咋地…..😩)，但经老师讲解后才知道，使用python解题不仅效率高，而且准确性也很强的，所以还是有必要学习以下的。 判断是否为合式公式&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第一个题目就是判断一个公式是否为合式公式，这也是最基础的，因为只有当在输入的合式公式正确的情况下，才能进一步的运算解题，所以先讲解一下怎么判断合式公式。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先要知道什么是合式公式？(1)原子命题常项或变项是合式公式；(2)如果A是合式公式，则（-A）也是合式公式（- 表示非）；(3)如果A，B是合式公式，则（AB）、（A+B）、（A &lt; B）、（ A ~ B）也是合式公式；(此处 合取 + 析取 &lt; 代表条件 ~ 代表双条件)(4)只有有限次地应用(1)～(3)所包含的命题变元，联结词和括号的符号串才是合式公式。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;个人思路：输入字符串，扫描字符串，把所含的各关联词分区出来，在判断每个关联词使用是否正确比如不合规则的情况：(1) 关联词所处位置不对(2) 关联词的连续使用(3) 括号不匹配(4) ……..程序中可能会有bug，希望大佬们多多指教 这道题应该有很多好的方法，但我不太会用，我这里只能暴力判断了。废话不多说，直接上代码😎12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788import reprint("定义‘!’为否定联结词")print("定义‘&amp;’为合取联结词")print("定义‘-’为析取联结词")print("定义‘&gt;’为蕴含联结词")print("定义‘~’为等价联结词")print("如果公式错误，会指出哪里错误；如果公式正确，则什么也不输出")S =input("请输入需要判断的公式：")T = list(S)if S[0] in ['&amp;','-',')','~','&gt;'] : print("不是合式公式")a ,b,c,d,f ="!" , "&amp;" ,"-" , "&gt;" , "~"if a in T: Q = [m.start() for m in re.finditer(a, S)] for i in Q : if S[-1] == '!' : print ("‘!’联结词使用错误，不是合式公式") break if S[i+1] in ['&amp;','!','-',')','~','&gt;']: print("‘!’联结词使用错误，不是合式公式") break if S[i-1] in ['!']: print("‘!’联结词使用错误，不是合式公式！") breakif b in T: W = [m.start() for m in re.finditer(b, S)] for y in W : if S[-1] == '&amp;' : print ("‘&amp;’联结词使用错误，不是合式公式") break if S[y+1] in ['-',')','~','&gt;','&amp;'] : print("‘&amp;’联结词使用错误，不是合式公式") break if S[y-1] in ['-','(','~','&gt;','&amp;'] : print("‘&amp;’联结词使用错误，不是合式公式") breakif c in T: E = [m.start() for m in re.finditer(c, S)] for h in E : if S[-1] == '-' : print("‘-’联结词使用错误，不是合式公式") break if S[h+1] in (')','&gt;','~','-') : print("‘-’联结词使用错误，不是合式公式") break if S[h-1] in ('&amp;','-','&gt;','~') : print("‘-’联结词使用错误，不是合式公式") breakif d in T: R = [m.start() for m in re.finditer(d, S)] for k in R : if S[-1] == '&gt;' : print ("‘&gt;’联结词使用错误，不是合式公式") break if S[k+1] in [')','&gt;'] : print("‘&gt;’联结词使用错误，不是合式公式") break if S[k-1] in ['(','&gt;'] : print("‘&gt;’联结词使用错误，不是合式公式") breakif f in T: O = [m.start() for m in re.finditer(f, S)] for v in O : if S[-1] == '~' : print("‘~’联结词使用错误，不是合式公式") break if S[v+1] in [')','~'] : print("‘~’联结词使用错误，不是合式公式") break if S[v-1] in ['(','~'] : print("‘~’联结词使用错误，不是合式公式") breakif '(' or ')' in S : e = 0 klb =[] for i in S : if i == '(': klb.append(i) if i == ')': if len(klb)==0: e=1 break else : klb.pop() if len(klb)!=0 : e = 1 if e==1: print("括号使用错误，不是合式公式") &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我也是小白，程序哪里可能有些不足之处，请多多指教，如果你有更好的解题方法或思路，可以联系我，大家可以一起学习，一起进步的！😊]]></content>
      <categories>
        <category>python</category>
        <category>离散数学</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>离散数学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客搭建]]></title>
    <url>%2F2019%2F03%2F15%2F%E6%90%AD%E5%BB%BA(2)%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本篇文章用于介绍Hexo个人博客的搭建过程，这也是我搭建本博客后的第一篇文章，分享一下搭建方法，有兴趣的小伙伴也可以自主搭建一个属于自己的博客！ 首先感谢大家的来访支持！&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;自己本来开始做的是html网页，网页做完之后需要购买服务器和域名（价格不便宜）才能运营，还要定期维护它，对于我个人只是感兴趣做个自己的网页，单单做个html网页就很费力气了，实在是没有精力和时间运营它，所以也没有必要去购买服务器和域名（性价比低），但我做的html网页在我自己的电脑上还是可以运营的（感兴趣的小伙伴可以联系我，一起探讨做html网页的方法）。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其次，现在市面上的博客有很多，如CSDN，博客园等平台，可以直接在上面发表，也有很多优点，但缺点是比较不自由，会受到各种限制和恶心的广告。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所以我选择了直接在github page平台上托管我的博客，这样可以安心写作，也不需要定期维护，而且hexo作为一个快速简洁的博客框架，用它来搭建博客也很简单。我在两天内完成了Hexo-Github个人博客的基本搭建（其实搭建不难，自己脑子笨，做的慢些了……）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在这里记录一下我的第搭建一个博客，并给大家分享一下搭建的流程，以及在搭建过程中遇到的问题和解决方案。如果你也有搭建个人博客的想法，希望能给你一点经验帮助。 大家都是新手，可以分享经验，互相交流学习的。 搭建经历 ❤2019-3-13：初次看到Hexo-Github搭建个人博客，很感兴趣，便开始了自己博客的搭建； ❤2019-3-14：完成了博客的基本搭建（可以运营），并上传了第一篇文章； ❤2019-3-17：博客搭建工作全部完结（主题优化，程序添加，分类管理……）; ❤未完待续…… 搭建步骤1. 安装Git 2. 安装Node.js 3. 安装Hexo 4. 注册Github账号并创建新仓库 5. 生产SSH添加到Github 6. 将Hexo部署到Github 7. 修改主题 8. 优化主题 1.安装Git&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;直接到Git官网下载就行了，右键点击软件使用Git Bash的命令行工具，以后就用这个工具来使用Git 2.安装Node.js&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;安装同Git一样，也是到Node.js官网下载就可以了。 3.安装Hexo&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;使用下载好的Git Bash来下载Hexo，代码如下： 1npm install hexo-cli -g 4.注册Github账号并创建一个新的仓库&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Github是一个很好的开源项目托管平台，大部分人应该都注册的有账号（有账号的小伙伴可以跳过这步），即使没有注册过也没有关系，只需现在注册下就行了，请参考Github基础设置及使用详解，里面的有详细的注册过程,过程也很简单。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;注册过账号后，先创建一个新的仓库（创建的具体方法上面的那个网站也有详细过程，但这里需要注意的是，创建的仓库名一定要是 name.github.io,其中name为你注册的Github的用户名，修改成你自己的。 5.生成SSH添加到Github&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先配置SSH，同样使用Git Bash,输入一下代码： 1ssh-keygen -t rsa -C "邮件地址" &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中”邮件地址”是你注册Github时填写的邮箱（输入时双引号要带上），输入后要连续按回车键，再等待回应。它会回复： 1Enter file in which to save the key (/c/Users/lenovo/.ssh/id_rsa): &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;然后进入(/c/Users/lenovo/.ssh/id_rsa.pub),将里面的内容复制下来，再进入Github官网，到GitHub设置-&gt;SSH and GPG keys-&gt;New SSH key，粘贴此处并确定。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;再回到Git Bash,输入一下代码： 1ssh -T git@github.com &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用于测试SSH是否配置好，看到You&#39;ve successfully authenticated, but GitHub does not provide shell access.则说明配置好了，否则无法使用hexo d。 6. 将Hexo部署到Github&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;初始化Hexo12hexo init Blog cd Blog &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Blog可修该城其他名字；cd Blog 指打开Blog文件夹。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;安装Hexo扩展12npm install hexo-deployer-git --savenpm install &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本地调试123hexo cleanhexo g hexo s &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hexo s是开启本地预览服务，打开浏览器访问 http://localhost:4000 即可看到内容。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上传到Github Pages&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上传之前，打开站点配置文件_config.yml，位于站点根目录下，修改最后一部分为如下部分。 1234deploy: type: git repository: git@github.com:name/name.github.io.git #name修改为你的Github用户名 branch: master &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上传到Github 123hexo cleanhexo g hexo d 7.修改主题&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主题位置在themes目录下，默认为landscape主题。可从官网https://hexo.io/themes/选择各种下载（得看个人喜好了），推荐NexT主题，以下为安装方法。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;安装过程就一行代码，站点根目录下运行。 1git clone https://github.com/theme-next/hexo-theme-next themes/next &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;然后打开站点配置文件_config.yml，找到theme： landscape，把landscape修改为next就可以了。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;到现在你就已经搭建好了一个属于你自己的博客平台了，赶紧去熟悉熟悉它吧。👍 8.主题优化&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其实刚才我们已经搭建好了，并且也可以使用了，但我们仍可以使我们的博客变得更加好看吸引人，以及添加更多的小功能，使我们有更好的体验，也就是进行主题优化。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;但由于主题优化内容比较多，所以在这就先不讲解了，不用担心，我会在后续的博客文章中持续更新关于如何进行主题优化，使我们的博客变得“高大上”。🤞]]></content>
      <categories>
        <category>Hexo</category>
        <category>博客搭建</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>博客搭建</tag>
      </tags>
  </entry>
</search>
