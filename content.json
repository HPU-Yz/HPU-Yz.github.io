{"meta":{"title":"Best-yz","subtitle":"　","description":"走最远的路，爬最高的山，看最美的风景。","author":"Flaneur","url":"https://hpu-yz.github.io","root":"/"},"pages":[{"title":"留言板","date":"2020-04-06T10:31:59.000Z","updated":"2020-05-26T05:25:26.496Z","comments":true,"path":"guestbook/index.html","permalink":"https://hpu-yz.github.io/guestbook/index.html","excerpt":"","text":"有什么想留的，有什么想问的，都可以在评论区留言~ 有时候会觉得，世界真的很小， 也许我们就在某时某地，曾经擦肩而过，而你我，却浑然不觉… If you leave something, I will reply everything. 友链申请请到友链页面评论区回复申请"},{"title":"分类","date":"2020-04-06T10:29:35.000Z","updated":"2020-04-09T03:08:11.661Z","comments":true,"path":"categories/index.html","permalink":"https://hpu-yz.github.io/categories/index.html","excerpt":"","text":""},{"title":"关于我","date":"2020-04-06T10:31:59.000Z","updated":"2020-05-26T05:28:43.353Z","comments":true,"path":"about/index.html","permalink":"https://hpu-yz.github.io/about/index.html","excerpt":"","text":"Yz HPU（大二） 联系方式见左侧 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一个什么都感兴趣的三分钟热度渣，喜欢简单，向往美好，在这该死的生活中隐忍不发游刃有余的一个人。没有特别喜欢的事情也没有什么特别讨厌的事情，不喜欢孤独但是也不喜欢热闹，时而恍惚，时而亢奋！ Try：勇于尝试 Enjoy：停下来享受生活 Documentary：记得整理和记录"},{"title":"友链","date":"2020-04-06T10:31:59.000Z","updated":"2020-05-26T01:35:35.456Z","comments":true,"path":"links/index.html","permalink":"https://hpu-yz.github.io/links/index.html","excerpt":"","text":"欢迎交流~（排名不分先后） 同学们~ Uncle_drew »Hand down,man down. 王荣胜 »可以允许不完美，但不能不做！ zha_zha_wei »又骚又强 AngelNI »May we meet the best self. o_oyao »keep learning. 小伙伴~ Poll的笔记 »机器学习，数据挖掘以及深度学习算法 菠菜眾長 »从 ACM 到 Web，分享，记录！ 醉秋风 »要相信，一切都是最好得安排！ OkYes! 技术博客 »念念不忘，必有回响 Soraの小站 »永远相信美好的事情即将发生 Leoの小站 »一个初一渣渣得技术博客 Barkure »Now that you are here, just take a look~ Sanarous »码农人生 阳光派Plus »聪明的妖怪已经录下了唐僧的紧箍 cungudafa »一个学习记录者！ zkpeace »宇宙,地球和我 皮皮凛の小窝 »永远相信美好的事情即将发生~ 添加友链说明：申请友链前请先将本站链接添加到您的网站友链~拒绝不正规网站不定期检查友链是否失效，如果失效请及时联系我进行更改若单方面长时间不添加或者删除本站友链，本站将不再保留贵站友链 友链格式如下：博客名称：Best-yz博客地址：https://best-yz.cn博客头像：https://cdn.jsdelivr.net/gh/HPU-Yz/cdn-speed@1.0/medias/avatar.jpg博客简介：生活还是很美好滴…"},{"title":"标签云","date":"2020-04-06T10:31:09.000Z","updated":"2020-04-09T03:07:55.815Z","comments":true,"path":"tags/index.html","permalink":"https://hpu-yz.github.io/tags/index.html","excerpt":"","text":""},{"title":"说说","date":"2020-05-25T10:31:59.000Z","updated":"2020-05-26T05:31:23.513Z","comments":true,"path":"shuoshuo/index.html","permalink":"https://hpu-yz.github.io/shuoshuo/index.html","excerpt":"","text":"记录动态，不定时更新~ &nbsp;&nbsp;Best-yz&nbsp;&nbsp;2020-05-26 11:15:27 没错，又又又…换主题了，真香~🐱‍🐉一直对主题不满意，总感觉不是自己想要的，希望这是最后一次了（🤫经过这两天的折腾，对原主题进行了一顿魔改，终于整好了（也就是现在这个样子…后续有需要的话会继续完善），道路坎坷啊…不过也很满足 &nbsp;&nbsp;Best-yz&nbsp;&nbsp;2020-05-25 23:56:11 新搞个说说动态页面（小声嘀咕：其实它是个假说说，就一表格…🙈回首记录下开始吧~记得，那是2019年的一天（3.15）一个懵懂少年，踏入hexo，至今还在坑中…"}],"posts":[{"title":"A-Star算法求解N数码问题","slug":"A_star算法求解N数码问题","date":"2020-04-23T16:00:00.000Z","updated":"2020-05-25T08:07:09.754Z","comments":true,"path":"2020/04/24/a-star-suan-fa-qiu-jie-n-shu-ma-wen-ti/","link":"","permalink":"https://hpu-yz.github.io/2020/04/24/a-star-suan-fa-qiu-jie-n-shu-ma-wen-ti/","excerpt":"","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;N数码问题和A-Star 算法的介绍，网上讲解的有很多，基本意思都是一样的，所以在这里就不再做详细的介绍了，本文主要是讲解python实现A-Star算法对N数码问题的求解过程。 python代码实现&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以下是代码，并附有详细注释。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131import numpy as npimport timeimport copyfrom operator import itemgetterdef get_loc(mat,num): #获取某个元素num的位置 for i in range(n): for j in range(n): if num == mat[i][j]: return i,j def get_dis(mat1,mat2): #计算当前矩阵和目标矩阵的曼哈顿距离 dis = 0 for i in range(n): for j in range(n): if mat1[i][j] != mat2[i][j] and mat2[i][j] != 0: x , y = get_loc(mat1,mat2[i][j]) d = abs(i-x) + abs(j-y) #曼哈顿距离的计算 dis += d return disdef get_act(mat): #获取当前位置的下步可移动位置，可减少不必要的移动 x , y = get_loc(mat,0) act = [(-1,0),(1,0),(0,-1),(0,1)] #对应的是上下左右 if x == 0: act.remove((-1,0)) if y == 0: act.remove((0,-1)) if x == n-1: act.remove((1,0)) if y == n-1: act.remove((0,1)) return list(act) def move(mat,act): #移动交换元素 x , y = get_loc(mat,0) (a , b) = act h = mat[x+a][y+b] s = copy.deepcopy(mat) #注意不能在原有矩阵移动，否则下次移动就不是在原本矩阵的移动 s[x+a][y+b] = 0 s[x][y] = h return s def expand(node,act,step): #节点扩展 children = [] for a in act: child = &#123;&#125; child['parent'] = node child['mat'] = move(node['mat'],a) child['dis'] = get_dis(goal['mat'],child['mat']) #h(n) child['step'] = step + 1 #g(n) child['dis'] = child['step'] + child['dis'] #f(n) = g(n) + h(n) child['act'] = get_act(child['mat']) children.append(child) return childrendef main(): A = [[2,8,3],[1,6,4],[7,0,5]] #初始矩阵，0表示空白 B = [[1,2,3],[8,0,4],[7,6,5]] #目标矩阵 openlist = [] closelist = [] global goal,n goal = &#123;&#125; goal['mat'] = np.array(B) n = len(goal['mat']) cs = &#123;&#125; #初始矩阵信息的配置 cs['mat'] = np.array(A) #存放矩阵 cs['dis'] = get_dis(cs['mat'],goal['mat']) #f(n)=g(n)+h(n)，初始g(n)=0 cs['step'] = 0 #步数 cs['act'] = get_act(cs['mat']) #可移动位置列表 cs['parent'] = &#123;&#125; #存放父节点,初始无空 if (goal['mat'] == cs['mat']).all(): print(\"初始矩阵与目标矩阵相同，无需操作\") return openlist.append(cs) #加入初始状态 start_time = time.perf_counter() #计时 while openlist: children = [] node = openlist.pop() closelist.append(node) if (node['mat'] == goal['mat']).all(): #如果当前矩阵与目标矩阵相同，输出相关信息，退出 end_time = time.perf_counter() print('步数：%d' % node['dis']) print('运行时间：%f s' % (end_time-start_time)) print('解的路径：') way = [] while closelist: way.append(node['mat']) node = node['parent'] #寻找当前节点的父节点，直至初始矩阵 if (node['mat'] == cs['mat']).all(): way.append(node['mat']) break print('初状态') #输出打印出移动过程 print(cs['mat']) for i in range(1,len(way)): print('\\n第 %d 步' % i) print(way[::-1][i]) return children = expand(node,node['act'],node['step']) #如果当前矩阵与目标矩阵不相同，则节点扩展 for child in children: f = flag = False j = 0 for i in range(len(openlist)): #判断扩展后的节点是否在openlist中 if (child['mat'] == openlist[i]['mat']).all(): j = i flag = True break for i in range(len(closelist)): #判断扩展后的节点是否在closelist中 if (child['mat'] == closelist[i]).all(): f = True break if f == False and flag == False: #如果扩展后的节点不在open和close中，则加入openlist中 openlist.append(child) elif flag == True: #如果扩展后的节点已经在openlist中，则就行比较，保留路径较短的 if child['dis'] &lt; openlist[j]['dis']: del openlist[j] openlist.append(child) openlist = sorted(openlist,key = itemgetter('dis'),reverse = True) #对扩展后的新openlis按路径长度排序if __name__ == '__main__': main()","categories":[],"tags":[{"name":"机器学习 - A-Star","slug":"机器学习-A-Star","permalink":"https://hpu-yz.github.io/tags/机器学习-A-Star/"}]},{"title":"Python爬虫~豆瓣电影排行榜","slug":"Python爬虫--爬取豆瓣 TOP250 电影排行榜","date":"2020-03-19T16:00:00.000Z","updated":"2020-05-25T08:07:31.514Z","comments":true,"path":"2020/03/20/python-pa-chong-pa-qu-dou-ban-top250-dian-ying-pai-xing-bang/","link":"","permalink":"https://hpu-yz.github.io/2020/03/20/python-pa-chong-pa-qu-dou-ban-top250-dian-ying-pai-xing-bang/","excerpt":"","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本篇讲介绍一个简单的Python爬虫案例–爬取豆瓣 TOP250 电影排行榜。 很多朋友在看一部电影前都喜欢先找一下网友们对该片的评价。 说到电影评分的网站，除了国外的 IMDB 和烂番茄，国内要数豆瓣最为出名。 主要原因是豆瓣有一套完整的评分和防水军机制 。 在这套机制下，豆瓣评分高的电影不一定是所有人都喜欢的，但是豆瓣评分低的电影，一定是实打实的烂片！ 虽然每个人的喜好偏爱不同，但通常豆瓣评分 8 分以上的电影，都是值得一看的。 豆瓣还专门提供了一个 TOP250 的电影链接 -&gt; https://movie.douban.com/top250 爬取思路爬取的过程很好理解，这里只需要两个过程：① 从服务器上下载所需页面② 解析这个页面，得到自己需要有用的内容 ①抓取页面有的人可能会利用 urllib 模块实现网络抓取功能。但在 Python 中，有一个更好地替代者——Requests。Requests 简化了 urllib 的诸多冗杂且无意义的操作，并提供了更强大的功能。所以在这里我们使用 Requests 模块的 get() 方法从服务器上来下载这个页面。 12345678import requestsurl = \"https://movie.douban.com/top250\"headers = &#123; \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3314.0 Safari/537.36 SE 2.X MetaSr 1.0\" &#125;res = requests.get(url,headers=headers) res就是我们需要的这个页面的资源，我们不妨打开来看看是不是 12with open(\"豆瓣电影.txt\",'w',encoding='utf-8') as f: f.write(res.text) 打开文本如下图我们可以看出这确实是当前网页的资源，所以我们就抓取成功了。 ②解析页面解析网页内容推荐使用 BeautifulSoup 模块，它可以化腐朽为神奇，将一个复杂的网页结构转化为书籍目录的形式供你浏览。例如，我们现在需要解析提取出当前页面的电影名字 12345import bs4soup = bs4.BeautifulSoup(res.text,\"html.parser\")targets = soup.find_all(\"div\",class_=\"hd\")for each in targets: print(each.a.span.text) 可以得到如下结果 肖申克的救赎霸王别姬阿甘正传这个杀手不太冷美丽人生泰坦尼克号千与千寻辛德勒的名单 盗梦空间 忠犬八公的故事 海上钢琴师机器人总动员三傻大闹宝莱坞楚门的世界放牛班的春天 星际穿越 大话西游之大圣娶亲 熔炉 疯狂动物城 无间道 龙猫 教父 当幸福来敲门怦然心动触不可及 这里你可能就会有疑问，这些数据是怎么得来的呢？我们先来看下 HTML 源代码：发现每个电影的标题都是位于 &lt;div class=&quot;hd&quot;&gt;...&lt;/div&gt; 标签中的，它的从属关系是：div -&gt; a -&gt; span。 所以我们先调用 find_all() 方法，找到所有 class=”hd” 的 div 标签，然后按照从属关系即可直接取出电影名。同理，我们借用此发方法来解析提取出电影的评分、介绍等需要的信息。 附加问题我们刚才解析提取的仅仅是第一页的页面，那么还有第二、第三、第四页……呢？ 其实，解决起来也很简单，我们可以使用for循环来对每一页进行上述的两个过程。 但，我们此时又有新的问题，我们不可能每抓取一次，就重新输入下一网页的链接地址，这样很麻烦，效率也不高。 我们可以分析每一页的链接： 第一页：https://movie.douban.com/top250第二页：https://movie.douban.com/top250?start=25第三页：https://movie.douban.com/top250?start=50第四页：https://movie.douban.com/top250?start=75第五页：https://movie.douban.com/top250?start=100 … … …. … 我们可以发现这样的规律：每一次的更新的 url = https://movie.douban.com/top250 + &#39;/?start=&#39; + str(25*i)其中i可以表示为页数-1 咦，这个时候，你可能会有疑问，我们怎么知道一共有多少页呢，不能一直for循环无穷吧。 那当然不可能的了，我们可以按第二步解析网页方式来获取页数 1depth = soup.find('span',class_='next').previous_sibling.previous_sibling.text 注意，这个返回的depth是给字符串形式，需要int() 这样结合刚才的过程，就可以迭代每一页了 代码清单感兴趣的话，可以试一试哦 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import requestsimport bs4#抓取网页def open_url(url): headers = &#123;\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3314.0 Safari/537.36 SE 2.X MetaSr 1.0\"&#125; res = requests.get(url,headers=headers) return res#得到总页数def find_depth(res): soup = bs4.BeautifulSoup(res.text,'html.parser') depth = soup.find('span',class_='next').previous_sibling.previous_sibling.text return int(depth)#解析网页，提取内容def find_movies(res): soup = bs4.BeautifulSoup(res.text,'html.parser') names = [] target = soup.find_all('div',class_='hd') for i in target: names.append(i.a.span.text) ranks = [] target = soup.find_all('span',class_='rating_num') for i in target: ranks.append(i.text) messages = [] target = soup.find_all('div',class_='bd') for i in target: try: messages.append(i.p.text.split('\\n')[1].strip() + i.p.text.split('\\n')[2].strip()) except: continue result = [] length = len(names) for i in range(length): result.append(names[i] + ',' +ranks[i] + ',' + messages[i] + '\\n') return resultdef main(): host = \"https://movie.douban.com/top250\" res = open_url(host) depth = find_depth(res) result = [] for i in range(depth): url = host + '/?start=' + str(25*i) res = open_url(url) result.extend(find_movies(res)) with open(\"豆瓣TOP250电影.txt\",'w',encoding='utf-8') as f: for each in result: f.write(each)if __name__==\"__main__\": main()","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://hpu-yz.github.io/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://hpu-yz.github.io/tags/爬虫/"}]},{"title":"【小王子】摘录","slug":"【小王子】“正因为你为你的玫瑰花费了时间，这才使你的玫瑰变得如此重要。”","date":"2020-03-17T16:00:00.000Z","updated":"2020-05-25T08:06:33.559Z","comments":true,"path":"2020/03/18/xiao-wang-zi-zheng-yin-wei-ni-wei-ni-de-mei-gui-hua-fei-liao-shi-jian-zhe-cai-shi-ni-de-mei-gui-bian-de-ru-ci-chong-yao/","link":"","permalink":"https://hpu-yz.github.io/2020/03/18/xiao-wang-zi-zheng-yin-wei-ni-wei-ni-de-mei-gui-hua-fei-liao-shi-jian-zhe-cai-shi-ni-de-mei-gui-bian-de-ru-ci-chong-yao/","excerpt":"","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这是法国名著《小王子》中一个有名的寓言故事，这个童话，实在太有智慧灵性了。人与人之间，不也是一样道理吗？我不得不承认，我被触动了。 【小王子】 有一天，小王子的星球上，不知从哪里来了一颗种子，后来忽然发了芽。小王子小心翼翼地监护着这株与众不同的嫩芽。小王子看到嫩芽长出一个巨大的花蕾，感到从中一定会产生奇迹。但是，这朵花儿却躲在她那绿茵茵的房间里，梳妆打扮没个完。一天清晨，正好当太阳升起的时候，她露面了。 她，精心做了那么许多的准备，却伸着懒腰打着呵欠说：“哎呀！我刚刚睡醒。。。请原谅。。。瞧我的头发还是乱蓬蓬的。。。” 小王子此时抑制不住自己那爱慕之情。 “你真美！” “是么？你可知道，太阳是和我同时诞生的。。。” 小王子看得出，这花儿很不谦虚，不过，她确实楚楚动人。 “我想，是该吃早点的时候了吧？”她随后又说，“请您记得我需要。。。” 小王子感到十分惭愧，于是去提了一壶清水来给她浇灌。 不久，她就以她的多疑的虚荣心来折磨小王子。例如，有一天，她谈起她身上长着的四根刺时，她对小王子说：“老虎么，让它们带着它们的利抓来吧！” “我的星球上根本没有老虎，”小王子反驳说，“而且老虎是不吃草的。” 花儿娇嗔地说：“我可不是草。” “请原谅！” “我不怕什么老虎，可是我受不了穿堂风，您没有屏风吗？” 小王子思忖着：“受不了穿堂风。。。这对一株植物来说，太不幸了。这朵花儿有点难弄…” 这时，她故意咳得更响，好使他良心受责备。 尽管小王子真心诚意爱这朵花儿，这样一来，他开始对她产生怀疑了。小王子对一些无关紧要的话，看得过分认真，结果使自己很苦恼。 小王子选择了离开，去旅行。 临行那天早上，当小王子最后一次浇花，准备为她盖上玻璃罩子时，他觉得自己想哭出来。 “再见了！”他对花儿说。 花儿咳嗽起来，但并不是由于感冒。她终于开口说：“我真傻，我请求你的原谅。祝你幸福。”她居然没有一句责备他的话，这实在使他感到意外。他站在那里手足无措，玻璃罩举在半空。他不懂得这份脉脉的柔情。 “的确，我爱你。”花儿对他说，“但是你一点儿都没体会到，这实在是我的错。再说也没有用了。不过，你过去也和我一样的傻。希望你今后幸福。。。把罩子放到一边去吧，我用不着它了。” “要是风。。。” “我的感冒并没有那么厉害。。。晚间的凉风对我有好处。我是花儿呀。” “要是虫子野兽。。。” “我要是想结识蝴蝶，我就应当忍受得了两三只小毛虫在身上爬。据说，这很美妙。如果没有蝴蝶和毛虫，还有谁来看我呢？你么，你又远在天边。至于大动物，我并不怕，我有爪子。” 她天真地伸出她那四根刺，随后又说：“别这样磨磨蹭蹭啦，这挺叫人烦心！你既然决定走，那就快走吧！” 她是怕小王子看见她哭。这是一朵有傲气的花。。。 有一天，小王子对我倾诉：“我当时什么也不懂！我应该根据她的行动，而不是根据她的话判断她。她对我散发芳香，她使我的生活充满阳光。我真不该离开她。我本应看得出她耍的那些小花招后面隐藏着的一片柔情。花儿是多么里外不一致！我当时年纪太小，不懂得爱她。” 小王子走着走着，这时候，一只狐狸出现了。 “你好！”狐狸说。 “你好！”小王子彬彬有礼地回答。但回头一看，什么也没有看见。 “我就在这儿。”那声音说，“在苹果树下。” “你是谁？”小王子说，“你真漂亮。” “我是狐狸。”狐狸说。 “来跟我玩玩吧。”小王子建议说，“我很不开心。。。” “我不能跟你玩，”狐狸说，“我还没经过驯养。” “啊，对不起！”小王子说。但是，想了一想后，他又说：“什么叫做驯养？” 狐狸说：“意思就是：建立一种关系。” “建立关系？” “一点不错，”狐狸说，“在我看来，你只不过是一个小男孩，跟成千上万的男孩毫无两样。我不需要你，你也不需要我。对你来说我只不过是一只狐狸，跟成千上万的狐狸毫无两样。但是，你如果驯养了我，那么我们俩就彼此相互需要。对我来说，你是世界上独一无二的；我在你看来，也是世界上独一无二的。。。” 狐狸叹口气说：“世上没有十全十美的事，我的生活很单调枯燥，但是，如果你驯养了我，我的生活就会充满阳光。我会听得出一种与众不同的脚步声。别的脚步声会使我往洞里钻，你的脚步声却像音乐一般，把我从洞里召唤出来。还有，你看！那边的麦田，你看见了吗？我不吃面包，麦子对我来说，一点也没用。麦田不能引起我什么联想，这真使人扫兴！但是，你有金色的头发。一旦你驯养了我，那就会十分美妙！麦子，黄澄澄，会使我联想到你，而且我甚至会喜欢风吹麦浪的沙沙声。。。” 狐狸没说下去，盯着小王子看了好久。 “请你。。。驯养我吧！”他说。 “我是很愿意的，”小王子回答道，“可是我的时间不多。我还要认识一些新朋友，了解许许多多的事。” “人们只能了解自己所驯养的东西，”狐狸说，“人们不会有时间去了解任何东西。他们想要什么东西，都往商店去买现成的。可是，世界上没有可以购买朋友的商店，所以人也就得不到朋友。你要朋友，就请驯养我吧！” “要驯养，该怎样做呢？”小王子说。 “必须非常耐心。”狐狸回答道，“首先，你离我远一点，像这样，坐在草地上。我用眼梢瞅着你，你一句话也别说。话语往往是误会的根源。不过，每天你坐得更靠近我一些。。。” 第二天，小王子又来了。 “最好还是在同一时间来，”狐狸说，“比如说，你在下午四点钟来，一到三点钟我就开始感到幸福了。时间越接近，我就越感到幸福。到了四点钟，我就会坐立不安，焦虑重重；我就会发现幸福的代价。可是，你如果想什么时间来就什么时间来，我就不知道什么时候准备好我的心。。。应当有一定的常规。” “常规是什么？”小王子问道。 “这也是被人差不多忘得一干二净的事，”狐狸说，“这就是使某一天与其他日子不同，使某一时刻与其他时刻不同。比如说，猎人也有一种常规。他们每星期四都和乡村里的姑娘跳舞。星期四就成为我开心的日子！我甚至可以一直逛到葡萄园。要是猎人什么时候都去跳舞，这一天和其他日子没什么不同，那我就终年没放假的日子了。” 于是小王子驯养了狐狸。当小王子快要离开时，狐狸说：“哎！。。。我想哭。” “这是你自己的错，我从未想过要使你难受，但是，你却要我驯养你。。。”小王子说。 “是这样！”狐狸说。 “可是你现在又要哭！”小王子说。 “当然啦。”狐狸说。 “这样对你有什么好处。” “对我有好处。”狐狸说，“有了麦子的颜色。” 小王子走了。路上他碰见满园的玫瑰花儿。 满园的玫瑰花儿向小王子用力点着头微笑，小王子说：“你们很美，但你们是空虚的。没有人会为你们去死。当然，我的那朵玫瑰，一个普通的过路人会认为她们和你们是一样的。可是，她单独一朵比你们全体更可贵，因为我给她浇过水，因为我为她盖过罩子，因为我给她用屏风挡风，因为我倾听过她的怨艾，她的夸口，有时甚至倾听她的沉默。因为她是我的玫瑰花。” 小王子又回到狐狸身旁：“再见了。。。狐狸” “再见了。”狐狸说，“本质的东西，眼睛是看不到的，只有用心灵看。” “本质的东西，眼睛是看不到的，只有用心灵看。”小王子重复这句话，以免忘记。 “正是因为我对我的玫瑰，花费了时间，才使花儿变得那么重要。”小王子重复这句话，以免忘记。 “这个真理，已经被人忘记了，”狐狸说，“但是你千万不要忘记。对你驯养的东西，你要永远负责。你必须对你的花儿负责。。。” “我要对我的花儿负责。”小王子又重复跟着说，为了牢牢记住。 摘自《小王子》","categories":[],"tags":[{"name":"生活","slug":"生活","permalink":"https://hpu-yz.github.io/tags/生活/"}]},{"title":"动态规划解决矩阵连乘","slug":"python动态规划解决矩阵连乘","date":"2020-03-04T16:00:00.000Z","updated":"2020-05-26T04:14:28.237Z","comments":true,"path":"2020/03/05/python-dong-tai-gui-hua-jie-jue-ju-zhen-lian-cheng/","link":"","permalink":"https://hpu-yz.github.io/2020/03/05/python-dong-tai-gui-hua-jie-jue-ju-zhen-lian-cheng/","excerpt":"","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;动态规划算法与分治法类似，其基本思想也就是将待求解的问题分解成若干个子问题，先求解子问题，然后从这些子问题的解得到原问题的解，简单概括为自顶向下分解，自底向上求解。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;与分治法不同的是，适合于用动态规划法求解的问题，经分解得到的子问题往往不是相互独立的，换句话说，就是前面解决过的子问题，在后面的子问题中又碰到了前面解决过的子问题，子问题之间是有联系的。如果用分治法，有些同样的子问题会被重复计算几次，这样就很浪费时间了。所以动态规划是为了解决分治法的弊端而提出的，动态规划的基本思想就是，用一个表来记录所有已经解决过的子问题的答案，不管该子问题在以后是否会被用到，只要它被计算过，就将其结果填入表中，以后碰到同样的子问题，就可以从表中直接调用该子问题的答案，而不需要再计算一次。具体的动态规划的算法多种多样，但他们都具有相同的填表式。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;动态规划的适用场合，一般适用于解最优化问题，例如矩阵连乘问题、最长公共子序列、背包问题等等。 矩阵连乘问题描述&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;给定n个矩阵：A1,A2,…,An，其中Ai与Ai+1是可乘的，i=1，2…，n-1。确定计算矩阵连乘积的计算次序，使得依此次序计算矩阵连乘积需要的数乘次数最少。输入数据为矩阵个数和每个矩阵规模，输出结果为计算矩阵连乘积的计算次序和最少数乘次数。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;若A是一个p × q的矩阵，B是一个q × r的矩阵，则其乘积C=AB是一个p × r的矩阵。数乘次数是p × q × r. 疑问A(3 × 5)A(5 × 7)A(7 × 2)的连乘次数和括号划分有关系吗？ (A(3 × 5)A(5 × 7))A(7 × 2) 相乘次数： (3 × 5 7)+(3 × 7 × 2) = 147A(3 × 5)(A(5 × 7)A(7 × 2)) 相乘次数： (5 × 7 2)+(3 × 5 × 2) = 100 答案很明显是有关系的。 分析 求 A1A2A3…An 定义 AiAi+1…Ak…Aj-1Aj 子列， 可看成是Ai…Ak，Ak…Aj确定k的位置，然后按照递归的思想来逐步解决 求得结果后，使i=1，j=n原问题即可求解。 建立递归关系（状态转移方程） 设 Ai…Aj相乘 的最小数乘次数存储于m[i][j]中。S[i][j]存储最佳断开位置。A1：P0 × P1A2：P1 × P2A3：P2 × P3…Ai：Pi-1 × PiAi+1：Pi × Pi+1 … An：Pn-1 × Pn P0 × P1 × P2 … × Pn——n+1个 当i=j时，m[i][j] = 0;当i&lt;j时，m[i][j] = m[i][k]+m[k+1][j]+Pi-1PkPjk在i，j之间取值，取值范围为i&lt;=k&lt;j有递推关系如下：动态规划的最优子结构性质是： 问题的最优解包含了其子问题的最优解。最优子结构性质是问题可用动态规划法求解的显著特征。Ai…Ak，Ak+1…Aj的最优划分也包含在Ai…Aj的最优划分中 在计算出最优值m[i][j]后，可递归地由s[i][j]构造出相应的最优解。 python代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243import randomfrom pandas import * input = int(input(\"输入矩阵数：\"))matrix = [[0] * 2 for i in range(input)]for i in range(input): #生成矩阵 if i == 0: matrix[i][0] = random.randrange(100) matrix[i][1] = random.randrange(100) else: matrix[i][0] = matrix[i-1][1] matrix[i][1] = random.randrange(100)m = [[0] * input for i in range(input)] #记录连乘次数s = [[0] * input for j in range(input)] #记录括号位置def MatrixMultiplication(inp): for i in range(inp): m[i][i] = 0 for r in range(1, inp): for i in range(inp-r): j = i + r m[i][j] = m[i+1][j] + matrix[i][0] * matrix[i][1] * matrix[j][1] s[i][j] = i+1 for k in range(i+1, j): judge = m[i][k] + m[k+1][j] + matrix[i][0] * matrix[k][1] * matrix[j][1] if judge &lt; m[i][j]: m[i][j] = judge s[i][j] = k+1def printmatrix(left, right): if left == right: print(\"A\"+str(left+1), end='') else: print(\"(\", end='') printmatrix(left, s[left][right]-1) printmatrix(s[left][right], right) print(\")\", end='')MatrixMultiplication(input)dm = DataFrame(m, index=list(range(1, input+1)), columns=list(range(1, input+1)))ds = DataFrame(s, index=list(range(1, input+1)), columns=list(range(1, input+1)))print(matrix)print(\"数乘次数：\\n\", dm)print(\"括号位置：\\n\", ds)print(\"最终结果：\")printmatrix(0, input-1)","categories":[],"tags":[{"name":"动态规划","slug":"动态规划","permalink":"https://hpu-yz.github.io/tags/动态规划/"}]},{"title":"朴素贝叶斯算法","slug":"朴素贝叶斯算法","date":"2019-08-04T16:00:00.000Z","updated":"2020-05-25T08:06:47.247Z","comments":true,"path":"2019/08/05/po-su-bei-xie-si-suan-fa/","link":"","permalink":"https://hpu-yz.github.io/2019/08/05/po-su-bei-xie-si-suan-fa/","excerpt":"","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;朴素贝叶斯算法是流行的十大算法之一，该算法是有监督的学习算法，解决的是分类问题，如客户是否流失、是否值得投资、信用等级评定等多分类问题。该算法的优点在于简单易懂、学习效率高、在某些领域的分类问题中能够与决策树、神经网络相媲美。但由于该算法以自变量之间的独立（条件特征独立）性和连续变量的正态性假设为前提，就会导致算法精度在某种程度上受影响。 一、问题的提出先举一个具体的例子： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;“一所学校里面有 60% 的男生，40% 的女生。男生总是穿长裤，女生则一半穿长裤一半穿裙子。有了这些信息之后我们可以容易地计算“随机选取一个学生，他（她）穿长裤的概率和穿裙子的概率是多大”，这个就是前面说的“正向概率”的计算。然而，假设你走在校园中，迎面走来一个穿长裤的学生（很不幸的是你高度近似，你只看得见他（她）穿的是否长裤，而无法确定他（她）的性别），你能够推断出他（她）是男生的概率是多大吗？” &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们来算一算：假设学校里面人的总数是 U 个。60% 的男生都穿长裤，于是我们得到了 U P(Boy) P(Pants|Boy) 个穿长裤的（男生）（其中 P(Boy) 是男生的概率 = 60%，这里可以简单的理解为男生的比例；P(Pants|Boy) 是条件概率，即在 Boy 这个条件下穿长裤的概率是多大，这里是 100% ，因为所有男生都穿长裤）。40% 的女生里面又有一半（50%）是穿长裤的，于是我们又得到了 U P(Girl) P(Pants|Girl) 个穿长裤的（女生）。加起来一共是 U P(Boy) P(Pants|Boy) + U P(Girl) P(Pants|Girl) 个穿长裤的，其中有 U P(Girl) P(Pants|Girl) 个女生。两者一比就是你要求的答案。下面我们把这个答案形式化一下：我们要求的是 P(Girl|Pants) （穿长裤的人里面有多少女生），我们计算的结果是 U P(Girl) P(Pants|Girl) / [U P(Boy) P(Pants|Boy) + U P(Girl) P(Pants|Girl)] 。容易发现这里校园内人的总数是无关的，可以消去。于是得到 P(Girl|Pants) = P(Girl) P(Pants|Girl) / [P(Boy) P(Pants|Boy) + P(Girl) * P(Pants|Girl)] &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;注意，如果把上式收缩起来，分母其实就是 P(Pants) ，分子其实就是 P(Pants, Girl) 。而这个比例很自然地就读作：在穿长裤的人（ P(Pants) ）里面有多少（穿长裤）的女孩（ P(Pants, Girl) ）。进一步得到公式的一般形式： P(B|A) = P(A|B) P(B) / [P(A|B) P(B) + P(A|~B) * P(~B) ] 收缩起来就是：P(B|A) = P(AB) / P(A)其实这个就等于：P(B|A) * P(A) = P(AB) 二、正式的定义&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;朴素贝叶斯算法是基于贝叶斯定理与特征条件独立假设的分类方法，然后依据被分类项属于各个类的概率，概率最大者即为所划分的类别。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;设输入空间X=（x1,x2,x3…xn），输出类标记为Y=（y1,y2,y3…yn）。x的集合记为X，称为属性集。一般X和Y的关系不确定的，你只能在某种程度上说x有多大可能性属于类y1，比如说x有80%的可能性属于类y1，这时可以把X和Y看做是随机变量，P(Y|X)称为Y的后验概率（posterior probability），与之相对的P(Y)称为Y的先验概率（prior probability），P(X=x|Y=y1)称之为条件概率。 先验概率&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过经验来判断事情发生的概率，比如说“贝叶死”的发病率是万分之一，就是先验概率。再比如南方的梅雨季是 6-7 月，就是通过往年的气候总结出来的经验，这个时候下雨的概率就比其他时间高出很多。 后验概率&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;后验概率就是发生结果之后，推测原因的概率。比如说某人查出来了患有“贝叶死”，那么患病的原因可能是 A、B 或 C。患有“贝叶死”是因为原因 A 的概率就是后验概率。它是属于条件概率的一种。 条件概率&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;事件 A 在另外一个事件 B 已经发生条件下的发生概率，表示为 P(A|B)，读作“在 B 发生的条件下 A 发生的概率”。比如原因 A 的条件下，患有“贝叶死”的概率，就是条件概率。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;简单说来就是：贝叶斯分类算法的理论基于贝叶斯公式： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中P(A|B)称为条件概率，P(B)先验概率，对应P(B|A)为后验概率。朴素贝叶斯分类器基于一个简单的假定，即给定的目标值属性之间是相互独立。贝叶斯公式之所以有用是因为在日常生活中，我们可以很容易得到P(A|B)，而很难得出P(B|A)，但我们更关心P(B|A)，所以就可以根据贝叶斯公式来计算。 三、应用举例&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如下表所示，训练数据学习一个朴素贝叶斯分类器并确定x=(2,S)T的类标记y。表中x1、x2为特征，取值的集合分别为X1={1,2,3}，X2={S,M,L}，类标记Y={1,-1} 根据贝叶斯算法得到如下概率： P(Y=1)=9/15，P(Y=-1)=6/15 P(X1=1|Y=1)=2/9，P(X1=2|Y=1)=3/9，P(X1=3|Y=1)=4/9 P(X2=S|Y=1)=1/9，P(X2=M|Y=1)=4/9，P(X2=L|Y=1)=4/9 P(X1=1|Y=-1)=3/6，P(X1=2|Y=-1)=2/6，P(X1=3|Y=-1)=1/6 P(X2=S|Y=-1)=3/6，P(X2=M|Y=-1)=2/6，P((X2=L|Y=-1)=1/6 所以对于给定的x=(2,S)T计算： P(Y=1)P(X1=2|Y=1)P(X2=S|Y=1)=(9/15)(3/9)(1/9)=1/45 P(Y=-1)P(X1=2|Y=-1)P(X2=S|Y=-1)=(6/15)(2/6)(1/6)=1/15 所以分类结果为y=-1 四、朴素贝叶斯算法的优缺点优点： 朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率； 对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已； 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练（即可以实时的对新增的样本进行训练）； 对缺失数据不太敏感，算法也比较简单，常用于文本分类； 朴素贝叶斯对结果解释容易理解。 缺点 需要计算先验概率； 分类决策存在错误率； 对输入数据的表达形式很敏感； 由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。 五、应用领域 欺诈检测中使用较多； 一封电子邮件是否是垃圾邮件； 一篇文章应该分到科技、政治，还是体育类； 一段文字表达的是积极的情绪还是消极的情绪； 人脸识别","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://hpu-yz.github.io/tags/机器学习/"},{"name":"朴素贝叶斯算法","slug":"朴素贝叶斯算法","permalink":"https://hpu-yz.github.io/tags/朴素贝叶斯算法/"}]},{"title":"浅谈机器学习-分类和聚类的区别","slug":"浅谈机器学习-分类和聚类的区别","date":"2019-07-31T16:00:00.000Z","updated":"2020-05-25T08:06:48.817Z","comments":true,"path":"2019/08/01/qian-tan-ji-qi-xue-xi-fen-lei-he-ju-lei-de-qu-bie/","link":"","permalink":"https://hpu-yz.github.io/2019/08/01/qian-tan-ji-qi-xue-xi-fen-lei-he-ju-lei-de-qu-bie/","excerpt":"","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;机器学习中有两类的大问题，一个是分类，一个是聚类。在我们的生活中，我们常常没有过多的去区分这两个概念，觉得聚类就是分类，分类也差不多就是聚类，下面，我们就具体来研究下分类与聚类之间在数据挖掘中本质的区别。 分类分类有如下几种说法，但表达的意思是相同的。 分类（classification）:分类任务就是通过学习得到一个目标函数f，把每个属性集x映射到一个预先定义的类标号y中。 分类是根据一些给定的已知类别标号的样本，训练某种学习机器（即得到某种目标函数），使它能够对未知类别的样本进行分类。这属于supervised learning（监督学习）。 分类：通过学习来得到样本属性与类标号之间的关系。用自己的话来说，就是我们根据已知的一些样本（包括属性与类标号）来得到分类模型（即得到样本属性与类标号之间的函数），然后通过此目标函数来对只包含属性的样本数据进行分类。 分类算法的局限 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分类作为一种监督学习方法，要求必须事先明确知道各个类别的信息，并且断言所有待分类项都有一个类别与之对应。但是很多时候上述条件得不到满足，尤其是在处理海量数据的时候，如果通过预处理使得数据满足分类算法的要求，则代价非常大，这时候可以考虑使用聚类算法。 聚类聚类的相关的一些概念如下 聚类指事先并不知道任何样本的类别标号，希望通过某种算法来把一组未知类别的样本划分成若干类别，聚类的时候，我们并不关心某一类是什么，我们需要实现的目标只是把相似的东西聚到一起，这在机器学习中被称作 unsupervised learning （无监督学习） 通常，人们根据样本间的某种距离或者相似性来定义聚类，即把相似的（或距离近的）样本聚为同一类，而把不相似的（或距离远的）样本归在其他类。 聚类的目标：组内的对象相互之间时相似的（相关的），而不同组中的对象是不同的（不相关的）。组内的相似性越大，组间差别越大，聚类就越好。 分类与聚类的比较 分类：有训练数据，且训练数据包含输入和输出（有监督学习），已知分类的类别（即训练数据的输出）。学习出一个模型，用该模型对未分好类（预测数据）的数据进行预测分类（已知的类别中）。 聚类：训练数据只有输入（无监督学习）。训练过程即预测过程（聚类过程），且不知道类别，甚至不知道有多少个类别，类别的数量需要指定（K-means）,也可以直接通过算法学习出来（DBSCAN）。只能通过特征的相似性对样本分类。该过程即聚类。 聚类分析是研究如何在没有训练的条件下把样本划分为若干类。 在分类中，对于目标数据库中存在哪些类是知道的，要做的就是将每一条记录分别属于哪一类标记出来。 聚类需要解决的问题是将已给定的若干无标记的模式聚集起来使之成为有意义的聚类，聚类是在预先不知道目标数据库到底有多少类的情况下，希望将所有的记录组成不同的类或者说聚类，并且使得在这种分类情况下，以某种度量（例如：距离）为标准的相似性，在同一聚类之间最小化，而在不同聚类之间最大化。 与分类不同，无监督学习不依赖预先定义的类或带类标记的训练实例，需要由聚类学习算法自动确定标记，而分类学习的实例或数据样本有类别标记。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://hpu-yz.github.io/tags/机器学习/"},{"name":"分类","slug":"分类","permalink":"https://hpu-yz.github.io/tags/分类/"},{"name":"聚类","slug":"聚类","permalink":"https://hpu-yz.github.io/tags/聚类/"}]},{"title":"浅谈机器学习-回归与分类的区别","slug":"浅谈机器学习-回归与分类的区别","date":"2019-07-31T16:00:00.000Z","updated":"2020-05-25T08:03:09.655Z","comments":true,"path":"2019/08/01/qian-tan-ji-qi-xue-xi-hui-gui-yu-fen-lei-de-qu-bie/","link":"","permalink":"https://hpu-yz.github.io/2019/08/01/qian-tan-ji-qi-xue-xi-hui-gui-yu-fen-lei-de-qu-bie/","excerpt":"","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;机器学习的主要任务便是聚焦于两个问题：分类和回归。本文将浅谈下两者的区别。 区别&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;回归会给出一个具体的结果，例如房价的数据，根据位置、周边、配套等等这些维度，给出一个房价的预测。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分类相信大家都不会陌生，生活中会见到很多的应用，比如垃圾邮件识别、信用卡发放等等，就是基于数据集，作出二分类或者多分类的选择。 浅层： 两者的的预测目标变量类型不同，回归问题是连续变量，分类问题离散变量。中层： 回归问题是定量问题，分类问题是定性问题。高层： 回归与分类的根本区别在于输出空间是否为一个度量空间。 解释分类和回归的区别在于输出变量的类型。 定量输出称为回归，或者说是连续变量预测；定性输出称为分类，或者说是离散变量预测。 举个例子：预测明天的气温是多少度，这是一个回归任务；预测明天是阴、晴还是雨，就是一个分类任务。 应用场景不同1.回归问题的应用场景 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;回归问题通常是用来预测一个值，如预测房价、未来的天气情况等等，例如一个产品的实际价格为500元，通过回归分析预测值为499元，我们认为这是一个比较好的回归分析。一个比较常见的回归算法是线性回归算法（LR）。另外，回归分析用在神经网络上，其最上层是不需要加上softmax函数的，而是直接对前一层累加即可。回归是对真实值的一种逼近预测。 2.分类问题的应用场景 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分类问题是用于将事物打上一个标签，通常结果为离散值。例如判断一幅图片上的动物是一只猫还是一只狗，分类通常是建立在回归之上，分类的最后一层通常要使用softmax函数进行判断其所属类别。分类并没有逼近的概念，最终正确结果只有一个，错误的就是错误的，不会有相近的概念。最常见的分类方法是逻辑回归，或者叫逻辑分类。 本质&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分类模型和回归模型本质一样，都是要建立映射关系。在实际操作中，我们确实常常将回归问题和分类问题互相转化，即分类模型可将回归模型的输出离散化，回归模型也可将分类模型的输出连续化。（分类问题回归化：逻辑回归；回归问题分类化：年龄预测问题——&gt;年龄段分类问题）","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://hpu-yz.github.io/tags/机器学习/"},{"name":"分类","slug":"分类","permalink":"https://hpu-yz.github.io/tags/分类/"},{"name":"回归","slug":"回归","permalink":"https://hpu-yz.github.io/tags/回归/"}]},{"title":"卷积神经网络（CNN）","slug":"卷积神经网络（CNN）","date":"2019-07-29T16:00:00.000Z","updated":"2020-05-25T08:06:42.382Z","comments":true,"path":"2019/07/30/juan-ji-shen-jing-wang-luo-cnn/","link":"","permalink":"https://hpu-yz.github.io/2019/07/30/juan-ji-shen-jing-wang-luo-cnn/","excerpt":"","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CNN，即卷积神经网络，主要用于图像识别，分类。这篇卷积神经网络是前面介绍的多层神经网络的进一步深入，它将深度学习的思想引入到了神经网络当中，通过卷积运算来由浅入深的提取图像的不同层次的特征，而利用神经网络的训练过程让整个网络自动调节卷积核的参数，从而无监督的产生了最适合的分类特征。 一、卷积神经网络的网络结构&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;卷积神经网络主要由这几类层构成：输入层、卷积层，ReLU层、池化（Pooling）层和全连接层（全连接层和常规神经网络中的一样）。通过将这些层叠加起来，就可以构建一个完整的卷积神经网络。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在实际应用中往往将卷积层与ReLU层共同称之为卷积层，所以卷积层经过卷积操作也是要经过激活函数的。具体说来，卷积层和全连接层（CONV/FC）对输入执行变换操作的时候，不仅会用到激活函数，还会用到很多参数，即神经元的权值w和偏差b；而ReLU层和池化层则是进行一个固定不变的函数操作。卷积层和全连接层中的参数会随着梯度下降被训练，这样卷积神经网络计算出的分类评分就能和训练集中的每个图像的标签吻合了。 二、卷积层&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;卷积层是构建卷积神经网络的核心层，它产生了网络中大部分的计算量。注意是计算量而不是参数量。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;先简单给大家介绍一下什么叫做卷积，那么白话解释一下啥叫卷积，很形象，就是“卷”，你假象一下，你把毛巾卷起来，成为一个圆柱体的卷，就是这个意思，抽象的说，就是将原来的对象变小一些，但又能保证原来图像中的信息尽量多的保留下来，可以看看下面这个卷积过程图。看看这个图，大概理解卷积是一个什么样子的过程就可以了，用映射两个字我认为是比较恰当的。 1.卷积过程二维卷积：即滤波器的每个格子与滤波器选中数据的格子相乘。 三维卷积：三维数据的话，滤波器的也是三维。同时对每个维度进行卷积，最后将每个维度的卷积结果相加，输出二维。 2.过滤器过滤器的作用是：寻找图片的特征。这就涉及到CNN要做的工作了。每一个过滤器中的数值，都是算法自己学习来的，不需要我们费心去设置。 需要我们做的有： ① 设置过滤器的大小（用字母“F”表示） 一般情况下，我们的过滤器大小是3×3，即F=3。 当然，你还可以设置成5×5，都是可以的。 只不过，需要注意的是：过滤器的尺寸越大，得到的图像细节就越少，最终得到的特征图的尺寸也更小。 ② 设置过滤器滑动的步幅数（用字母“S”表示） 一般情况下，过滤器滑动的步幅是1，即每次过滤器向右或向下滑动1个像素单位。 当然，你也可以将步幅设置为2或更多，但是通常情况下，我们会使用S=1或S=2。 ③ 设置过滤器的个数（用字母“K”表示） 以下，我们分别给大家展示了4种过滤器。所以你可以理解为K=4，如下图： 当然，你可以设置任意个数。 再次强调：不要在意过滤器里面的数值，那是算法自己学习来的，不需要我们操心，我们只要把过滤器的个数设置好，就可以了。 所以，一张图片，在经过4种过滤器的提取后，会得到4种不同的特征图片： 从上面的例子我们能够看到，“卷积”输出的结果，是包含“宽、高、深”3个维度的： 实际上，在CNN中，所有图片都是包含有“宽、高、深”。 像输入的图片——萌狗，它也是包含3个维度，只不过，它的深度是1，所以在我们的图片中没有明显地体现出来： 所以，我们要记住，经过“卷积”层的处理后，图片含有深度，这个“深度”，等于过滤器的个数。 例如，上面我们采用了4种过滤器，那么，输出的结果，深度就为4。 ④ 设置是否补零（用字母“P”表示） 何为“补零”？ 当我们采用了3×3大小的过滤器，直接在原始图片滑过。 从结果中可以看到，最终得到的“特征图片”比“原始图片”小了一圈： 为什么会出现这种情况？ 原因很简单：过滤器将原始图片中，每3*3=9个像素点，提取为1个像素点 所以，当过滤器遍历整个图片后，得到的特征图片会比原始图片更小。 当然，你也可以得到一个和原始图片大小一样的特征图，这就需要采用“在原始图片外围补零”的方法： 下面，我们来看看“补零”后的效果： 如何确定“补零”的圈数，才能保证图片大小一致？假设你的过滤器大小为F，滑动步幅S=1，想要实现这一目标，补零的个数应为： 举个例子： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在上图中，因为我们使用的是3*3大小的过滤器，而且每次滑动时，都是向右或向下移动1格。 所以，为了使特征图片与原始图片保持一致，需要补零P=(3-1)/2=1，即在原始图片外围，补1圈零。 如果你使用的过滤器大小为5*5，那么补零P=(5-1)/2=2，即在原始图片外围，补2圈零。 当然，是否需要“补零”，由你自己来决定，“补零”并不是硬性规定。 3.卷积层（Conv Layer）的输出张量（图像）的大小计算定义如下： O=输出图像的尺寸。 I=输入图像的尺寸。 K=卷积层的核尺寸 N=核数量 S=移动步长 P =填充数 输出图像尺寸的计算公式如下： 输出图像的通道数等于核数量N。 示例：AlexNet中输入图像的尺寸为2272273.第一个卷积层有96个尺寸为11113的核。步长为4，填充为0.输出的图像为555596（每个核对应1个通道） 4. 卷积层作用 滤波器的作用或者说是卷积的作用。卷积层的参数是有一些可学习的滤波器集合构成的。每个滤波器在空间上（宽度和高度）都比较小，但是深度和输入数据一致（这一点很重要，后面会具体介绍）。直观地来说，网络会让滤波器学习到当它看到某些类型的视觉特征时就激活，具体的视觉特征可能是某些方位上的边界，或者在第一层上某些颜色的斑点，甚至可以是网络更高层上的蜂巢状或者车轮状图案。 可以被看做是神经元的一个输出。神经元只观察输入数据中的一小部分，并且和空间上左右两边的所有神经元共享参数（因为这些数字都是使用同一个滤波器得到的结果）。 降低参数的数量。这个由于卷积具有“权值共享”这样的特性，可以降低参数数量，达到降低计算开销，防止由于参数过多而造成过拟合。 三、激活函数 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如上图所示，一个filter或者神经元的内部计算过程，我们可以看到通过卷积并加上bias后（其实在感知机那个时代到这一步就可以了，模型就具备了线性分类的能力，但是大千世界我们的分类问题很少是线性函数可以拟合的，所以我们就需要拟合非线性函数），filter还做了一次函数映射运算，这里的函数 f 就是我们的非线性激活函数，它的作用是：使模型不再是线性组合，具有可以逼近任意函数的能力。下图就是我们经常使用激活函数 ReLU，还有sigmoid函数或者tanh函数这些，读者可以自行搜索。 如果是负数，出来的结果就是0 如果是正数或零，出来的结果仍是自己本身 举个例子： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设经过“过滤器”提取后，得到的“特征图片”其像素值如左下图，那么，经过ReLU处理后，得到的“新的”特征图片会呈现右下图形态： 回到我们之前的例子中，在“卷积”的后面，再加上一步“ReLU计算”，即为： 四、池化层&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有时图像太大，即使我们参数不太多，但图像的像素实在太多，导致卷积操作后，我们得到的结果仍然过大。我们需要减少训练参数的数量，它被要求在随后的卷积层之间周期性地引进池化层。池化的一个目的是减少图像的空间大小。池化在每一个纵深维度上独自完成，因此图像的纵深保持不变。 1.池化过程所谓“池化”，就是在保留图片主要信息的前提下，将图片的尺寸缩小。 池化的类型有很多种，诸如“最大池化”、“平均池化”、“求和池化”等等，它们的运算原理基本一致。 因为“最大池化”更为常用，所以我们重点介绍“最大池化”的运算过程。 假设经过ReLU处理后，我们得到的特征图片，如下图： 在对特征图片进行“池化”处理时，与“卷积”类似，需要我们设置2个超参数： 过滤器大小（F）和 滑动的步幅数（S）这里，假设我们设置F=2，S=2，那么，“最大池化”为： 在每2*2（即4个）像素区域内，保留像素值最大的那一个，其余3个像素值抛弃。如下图： 这样，经过“池化”处理，就将一个44大小的图片，缩小为22大小的图片了。 当然，在“池化”过程中，你也可以将超参数设置为F=3、S=2，只不过“F=2、S=2”更为常用。 回到我们之前的例子中，在“ReLU计算”的后面，再加上一步“最大池化”，即为： 2.池化层（MaxPool Layer）的输出张量（图像）的大小计算定义如下： O=输出图像的尺寸。I=输入图像的尺寸。S=移动步长PS=池化层尺寸 输出图像尺寸的计算公式如下：不同于卷积层，池化层的输出通道数不改变。 示例：每1层卷积层后的池化层的池化层尺寸为33，步长为2。根据前面卷积层的输出为555596。池化层的输出图像尺寸如下：输出尺寸为2727*96。 3.池化的作用和特点： 降维，减少网络的参数，达到防止过拟合的效果 可以实现平移、旋转的不变性 只改变图像尺寸，不改变图像深度 没有需要训练的参数 五、卷积和池化的叠加有的时候，我们会进行多次卷积和池化，所以，更一般的形式： 举个例子： ★ 你可以构建2层“卷积 + ReLU”，如下： 输入图片 → 卷积 → ReLU → 卷积 → ReLU → 池化 ★ 也可以构建2层“卷积 + ReLU + 池化”： 输入图片 → 卷积 → ReLU → 池化 → 卷积 → ReLU → 池化 当然，你还可以构建更多网络层，这里就不一一列举了。 六、全连接层经过上面一系列的处理，此时得到的图片，已经可以被视为一串串简单的数字（即像素值）。 将这一个个的像素值，塞到我们早已接触过的表达式中： x是谁？ 在计算机“眼中”的世界，图像是这样的： 那一个格子、一个格子中的数字，就是x。 y是谁？ 我们给每张图片打上的标签：这张图是“猫”、那张图是“狗”…… 统统这些标签，就是y。 θ又是什么？ 当你给计算机很多组、很多组（x,y）时，它会自己去学习寻找x与y之间的关系，这个“关系”，就是θ。 当你拥有了θ，下一次，即使拿到一张没有打过标签的图片，你也可以通过已知的θ和x，知道y的取值，从而“知道图片里画的是什么？”。 具体过程： 首先，需要把卷积后得到的图片，其中的像素值“展开”。 为了方便大家理解，假设我们卷积后，得到了2张2*2的图片，如果把它们的像素值“展开”，得到的效果为： 这样，我们就拿到了x。展开后所得到的 就是x。 在训练的最初，我们拥有每张图片的标签，即“我们已经拥有了y值”。 所以，在模型的最后，我们需要让计算机努力找到x-y之间的关系。 而寻找的办法，就需要依靠“全连接神经网络”：所谓“全连接”，是指下一层的每一个神经元（即图中的“□”），与上一层的神经元全部相连，这里为了表达得更清晰，已经省去了中间的连接符。实际上，更多的时候，你会看到下面这样的图片：过，本文的重点不在于讲解“全连接神经网络”，本文的核心在于卷积神经网络（CNN）。 这里，你只需要记住2点： ① “全连接神经网络”可以帮助我们学习到参数θ。有了它，下一次再给计算机“看”图片时，计算机便会自动识别出图中的景象。 ② 模型得到的最终结果，表示“图片为某种类别的概率”。注意：这里的概率之和，永远为1。 例如：即计算机“判断”，最初输入的图片为“狗”。 我们将CNN的所有流程，整合起来，即： 六、纵观其实卷积神经网络(CNN)与人工神经网络(ANN)的大概思路是一样的，包括两个阶段： 第一阶段，向前传播阶段： a）从样本集中取一个样本(X,Yp)，将X输入网络； b）计算相应的实际输出Op。 在此阶段，信息从输入层经过逐级的变换，传送到输出层。这个过程也是网络在完成训练后正常运行时执行的过程。在此过程中，网络执行的是计算（实际上就是输入与每层的权值矩阵相点乘，得到最后的输出结果）： Op=Fn（…（F2（F1（XpW（1））W（2））…）W（n）） 第二阶段，向后传播阶段 a）算实际输出Op与相应的理想输出Yp的差； b）按极小化误差的方法反向传播调整权矩阵。","categories":[],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://hpu-yz.github.io/tags/深度学习/"},{"name":"卷积神经网络（CNN）","slug":"卷积神经网络（CNN）","permalink":"https://hpu-yz.github.io/tags/卷积神经网络（CNN）/"}]},{"title":"基于协同过滤（CF）算法的推荐系统","slug":"基于协同过滤（CF）算法的推荐系统","date":"2019-07-27T16:00:00.000Z","updated":"2020-05-25T08:06:39.820Z","comments":true,"path":"2019/07/28/ji-yu-xie-tong-guo-lu-cf-suan-fa-de-tui-jian-xi-tong/","link":"","permalink":"https://hpu-yz.github.io/2019/07/28/ji-yu-xie-tong-guo-lu-cf-suan-fa-de-tui-jian-xi-tong/","excerpt":"","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 随着计算机领域技术的高速发展，电子商务时代的普及，个性化的推荐系统深入生活应用的各个方面。个性化推荐算法是推荐系统中最核心的技术，在很大程度上决定了电子商务推荐系统性能的优劣。而协同过滤推荐是个性化推荐系统应用最为广泛的技术，协同过滤推荐主要分为基于用户的协同过滤推荐、基于项目的协同过滤推荐和基于模型的协同过滤推荐。 一、协同过滤算法描述&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;大数据时代产生了海量的数据，里面蕴含了丰富的价值。但是，大数据体量之大、种类之繁以及产生速率之快，海量的数据并不都是有价值的，用户从海量的数据中提取有用的、针对性的信息需要花费很大的时间成本。比如，当你面对如此多的电影列表，你想找到一部最符合自己兴趣的电影，因为电影数量之多，你不可能把所有的电影简介都看一遍。那么怎么解决这个问题呢？电影平台搜集你过去看过的全部电影，分析了解你对什么类型电影感兴趣，然后针对性的把你感兴趣的电影主动的罗列给你，你不用花费太多的精力便可快速找到满足自己需求的电影。推荐算法便是为解决这类实际需求而诞生了。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 推荐系统应用数据分析技术，找出用户最可能喜欢的东西推荐给用户，现在很多电子商务网站都有这个应用。目前用的比较多、比较成熟的推荐算法是协同过滤（Collaborative Filtering，简称CF）推荐算法，CF的基本思想是根据用户之前的喜好以及其他兴趣相近的用户的选择来给用户推荐物品。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如图所示，在CF中，用m×n的矩阵表示用户对物品的喜好情况，一般用打分表示用户对物品的喜好程度，分数越高表示越喜欢这个物品，0表示没有买过该物品。图中行表示一个用户，列表示一个物品，Uij表示用户i对物品j的打分情况。CF分为两个过程，一个为预测过程，另一个为推荐过程。预测过程是预测用户对没有购买过的物品的可能打分值，推荐是根据预测阶段的结果推荐用户最可能喜欢的一个或Top-N个物品。 二、协同过滤的实现要实现协同过滤的推荐算法，要进行以下三个步骤： 收集数据 找到相似用户或物品 进行推荐 1、收集数据&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里的数据指的都是用户的历史行为数据，比如用户的购买历史，关注，收藏行为，或者发表了某些评论，给某个物品打了多少分等等，这些都可以用来作为数据供推荐算法使用，服务于推荐算法。需要特别指出的在于，不同的数据准确性不同，在使用时需要考虑到噪音所带来的影响。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;要从用户的行为和偏好中发现规律，并基于此给予推荐，如何收集用户的偏好信息成为系统推荐效果最基础的决定因素。用户有很多方式向系统提供自己的偏好信息，而且不同的应用也可能大不相同，如下图： 2、找到相似用户或物品&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这一步其实就是计算用户间以及物品间的相似度。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关于相似度的计算，现有的几种基本方法都是基于向量（Vector）的，其实也就是计算两个向量的距离，距离越近相似度越大。在推荐的场景中，在用户 - 物品偏好的二维矩阵中，我们可以将一个用户对所有物品的偏好作为一个向量来计算用户之间的相似度，或者将所有用户对某个物品的偏好作为一个向量来计算物品之间的相似度。下面我们详细介绍几种常用的相似度计算方法： 下一步是作出预测。你可以运用下面的公式做一个预测： 3、进行推荐3.1、基于用户的协同过滤推荐(User-based Collaborative Filtering Recommendation) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于用户的协同过滤推荐算法先使用统计技术寻找与目标用户有相同喜好的邻居，然后根据目标用户的邻居的喜好产生向目标用户的推荐。基本原理就是利用用户访问行为的相似性来互相推荐用户可能感兴趣的资源，如图所示： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上图示意出基于用户的协同过滤推荐机制的基本原理，假设用户 A 喜欢物品 A、物品 C，用户 B 喜欢物品 B，用户 C 喜欢物品 A 、物品 C 和物品 D；从这些用户的历史喜好信息中，我们可以发现用户 A 和用户 C 的口味和偏好是比较类似的，同时用户 C 还喜欢物品 D，那么我们可以推断用户 A 可能也喜欢物品 D，因此可以将物品 D 推荐给用户 A。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于用户的协同过滤推荐机制和基于人口统计学的推荐机制都是计算用户的相似度，并基于“邻居”用户群计算推荐，但它们所不同的是如何计算用户的相似度，基于人口统计学的机制只考虑用户本身的特征，而基于用户的协同过滤机制可是在用户的历史偏好的数据上计算用户的相似度，它的基本假设是，喜欢类似物品的用户可能有相同或者相似的口味和偏好。 3.2、基于项目的协同过滤推荐(Item-based Collaborative Filtering Recommendation)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;根据所有用户对物品或者信息的评价，发现物品和物品之间的相似度，然后根据用户的历史偏好信息将类似的物品推荐给该用户，如图所示：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上图表明基于项目的协同过滤推荐的基本原理，用户A喜欢物品A和物品C，用户B喜欢物品A、物品B和物品C，用户C喜欢物品A，从这些用户的历史喜好中可以认为物品A与物品C比较类似，喜欢物品A的都喜欢物品C，基于这个判断用户C可能也喜欢物品C，所以推荐系统将物品C推荐给用户C。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 基于项目的协同过滤推荐和基于内容的协同过滤推荐都是基于物品相似度预测推荐，只是相似度度量的方法不一样，前者是从用户历史的偏好推断，而后者是基于物品本身的属性特征信息。 3.3、UserCF和ItemCF的区别 三、算法实例以ItemCF为例子具体介绍下整个算法流程。算法流程： 构建用户–&gt;物品的倒排； 构建物品与物品的同现矩阵； 计算物品之间的相似度，即计算相似矩阵； 根据用户的历史记录，给用户推荐物品； 算法流程1构建用户–&gt;物品的倒排如下表，行表示用户，列表示物品，1表示用户喜欢该物品 12345678例如python构建的数据格式如下&#123;'A': &#123;'a': '1', 'b': '1', 'd': '1'&#125;, 'B': &#123;'c': '1', 'b': '1', 'e': '1'&#125;, 'C': &#123;'c': '1', 'd': '1'&#125;, 'D': &#123;'c': '1', 'b': '1', 'd': '1'&#125;,'E': &#123;'a': '1', 'd': '1'&#125;&#125; 算法流程2构建物品与物品的同现矩阵 共现矩阵C表示同时喜欢两个物品的用户数，是根据用户物品倒排表计算出来的。如根据上面的用户物品倒排表可以计算出如下的共现矩阵C： 算法流程3计算物品之间的相似度，即计算相似矩阵 其中两个物品之间的相似度如何计算？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;设|N(i)|表示喜欢物品i的用户数，|N(i)⋂N(j)|表示同时喜欢物品i，j的用户数，则物品i与物品j的相似度为： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1)式有一个问题，当物品j是一个很热门的商品时，人人都喜欢，那么wijwij就会很接近于1，即(1)式会让很多物品都和热门商品有一个很大的相似度，所以可以改进一下公式： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;算法流程2中的共现矩阵C其实就是式(2)的分子，矩阵N（用于计算分母）表示喜欢某物品的用户数（是总的用户数），则(2)式中的分母便很容易求解出来了。 矩阵N如下所示： 利用式（2）便能计算物品之间的余弦相似矩阵如下： 算法流程4根据用户的历史记录，给用户推荐物品； 最终推荐的是什么物品，是由预测兴趣度决定的。 物品j预测兴趣度=用户喜欢的物品i的兴趣度×物品i和物品j的相似度 例如：A用户喜欢a，b，d ，兴趣度分别为1,1,1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;推荐c的预测兴趣度=1X0.67+1X0.58=1.25&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;推荐e的预测兴趣度=1X0.58=0.58 四、python实现案例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566from math import sqrtimport operator#1.构建用户--&gt;物品的倒排def loadData(files): data =&#123;&#125; for line in files: user,score,item=line.split(\",\") #按\",\"将每组数据单独分开,并将每组的数据赋予对应变量 data.setdefault(user,&#123;&#125;) data[user][item]=score print (\"----1.用户：物品的倒排----\") print (data) return data#2.计算# 2.1 构造物品--&gt;物品的共现矩阵# 2.2 计算物品与物品的相似矩阵def similarity(data): # 2.1 构造物品：物品的共现矩阵 N=&#123;&#125;;#喜欢物品i的总人数 C=&#123;&#125;;#喜欢物品i也喜欢物品j的人数 for user,item in data.items(): for i,score in item.items(): N.setdefault(i,0) N[i]+=1 C.setdefault(i,&#123;&#125;) for j,scores in item.items(): if j not in i: C[i].setdefault(j,0) C[i][j]+=1 print (\"---2.构造的共现矩阵---\") print ('N:',N) print ('C:',C) #2.2 计算物品与物品的相似矩阵 W=&#123;&#125;; for i,item in C.items(): W.setdefault(i,&#123;&#125;) for j,item2 in item.items(): W[i].setdefault(j,0) W[i][j]=C[i][j]/sqrt(N[i]*N[j]) print (\"---3.构造的相似矩阵---\") print (W) return W#3.根据用户的历史记录，给用户推荐物品def recommandList(data,W,user,k=3,N=10): rank=&#123;&#125; for i,score in data[user].items():#获得用户user历史记录，如A用户的历史记录为&#123;'a': '1', 'b': '1', 'd': '1'&#125; for j,w in sorted(W[i].items(),key=operator.itemgetter(1),reverse=True)[0:k]:#获得与物品i相似的k个物品 if j not in data[user].keys():#该相似的物品不在用户user的记录里 rank.setdefault(j,0) rank[j]+=float(score) * w print (\"---4.推荐----\") print (sorted(rank.items(),key=operator.itemgetter(1),reverse=True)[0:N]) return sorted(rank.items(),key=operator.itemgetter(1),reverse=True)[0:N]if __name__=='__main__': #用户，兴趣度，物品 uid_score_bid = ['A,1,a', 'A,1,b', 'A,1,d', 'B,1,b', 'B,1,c', 'B,1,e', 'C,1,c', 'C,1,d', 'D,1,b', 'D,1,c', 'D,1,d', 'E,1,a', 'E,1,d'] data=loadData(uid_score_bid)#获得数据 W=similarity(data)#计算物品相似矩阵 recommandList(data,W,'A',3,10)#推荐 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了方便理解python的实现过程，这里只用了少数数据集并是手输上去的，具体情况运用可再加段代码用于读取并处理数据文件。 五、算法的优缺点&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这个算法实现起来也比较简单，但是在实际应用中有时候也会有问题的。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;比如一些非常流行的商品可能很多人都喜欢，这种商品推荐给你就没什么意义了，所以计算的时候需要对这种商品加一个权重或者把这种商品完全去掉也行。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;再有，对于一些通用的东西，比如买书的时候的工具书，如现代汉语词典，新华字典神马的，通用性太强了，推荐也没什么必要了。 UserCF和ItemCF优缺点的对比","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://hpu-yz.github.io/tags/机器学习/"},{"name":"协同过滤（CF）","slug":"协同过滤（CF）","permalink":"https://hpu-yz.github.io/tags/协同过滤（CF）/"},{"name":"推荐系统","slug":"推荐系统","permalink":"https://hpu-yz.github.io/tags/推荐系统/"}]},{"title":"人工神经网络（ANN）","slug":"人工神经网络（ANN）","date":"2019-07-25T16:00:00.000Z","updated":"2020-05-25T08:06:53.960Z","comments":true,"path":"2019/07/26/ren-gong-shen-jing-wang-luo-ann/","link":"","permalink":"https://hpu-yz.github.io/2019/07/26/ren-gong-shen-jing-wang-luo-ann/","excerpt":"","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;初学人工智能不久，今天碰上了人工神经网（ANN），开始学的时候很懵，一大堆理论、公式、推导…..作为一名小白，还是很痛苦的，不过经过摸索，大概了 解了什么是ANN，公式的推导以及一些其他问题，下面我就总结下自己的理解，一方面作为自己的笔记，日后方便巩固；另一方面，也可以分享给其他有意者。 一、什么是神经网络1.单层神经网络首先以单层神经元为例解释人工神经元是如何工作的&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x1,x2,…, xN：神经元的输入。这些可以从输入层实际观测或者是一个隐藏层的中间值（隐藏层即介于输入与输出之间的所有节点组成的一层。后面讲到多层神经网络是会再跟大家解释的）。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;X0：偏置单元。这是常值添加到激活函数的输入（类似数学里y＝ax+b中使直线不过原点的常数b）。即截距项，通常有＋1值。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;w0,w1,w2,…,wN：对应每个输入的权重。甚至偏置单元也是有权重的。 a:神经元的输出。计算如下：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;式子里的f是已知的激活函数，f使神经网络（单层乃至多层）非常灵活并且具有能估计复杂的非线性关系的能力。在简单情况下可以是一个高斯函数、逻辑函数、双曲线函数或者甚至上是一个线性函数。利用神经网络可让其实现三个基本功能：与、或、非（AND, OR, NOT）。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里引入一个例子：and功能实现如下神经元输出：a = f( -1.5 + x1 + x2 ) 这样大家就很容易理解其工作原理，其实就是对输入值赋予不同权重，经过激活函数输出的过程。 2.多层神经网络2.1 网络结构清楚了单层神经网络，多层神经网络也好理解了，就相当于多个单层的叠加成多层的过程。神经网络分为三种类型的层： 输入层：神经网络最左边的一层，通过这些神经元输入需要训练观察的样本，即初始输入数据的一层。 隐藏层：介于输入与输出之间的所有节点组成的一层。帮助神经网络学习数据间的复杂关系，即对数据进行处理的层。 输出层：由前两层得到神经网络最后一层，即最后结果输出的一层。 2.2 传递函数/激活函数&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;前面每一层输入经过线性变换wx+b后还用到了sigmoid函数，在神经网络的结构中被称为传递函数或者激活函数。除了sigmoid，还有tanh、relu等别的激活函数。激活函数使线性的结果非线性化。 2.3 为什么需要传递函数&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;简单理解上，如果不加激活函数，无论多少层隐层，最终的结果还是原始输入的线性变化，这样一层隐层就可以达到结果，就没有多层感知器的意义了。所以每个隐层都会配一个激活函数，提供非线性变化。 二、BP算法1.BP算法基本思想&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BP算法全称叫作误差反向传播(error Back Propagation，或者也叫作误差逆传播)算法。其算法基本思想为：在上述的前馈网络中，输入信号经输入层输入，通过隐层计算由输出层输出，输出值与标记值比较，若有误差，将误差反向由输出层向输入层传播，在这个过程中，利用梯度下降算法对神经元权值进行调整。 2.BP算法的推导2.1 数学基础理论&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;BP算法中核心的数学工具就是微积分的链式求导法则。 2.2推导过程 正向传播求损失，反向传播回传误差 根据误差信号修正每层的权重 f是激活函数；f(netj)是隐层的输出； f(netk）是输出层的输出O; d是target &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;结合BP网络结构，误差由输出展开至输入的过程如下：有了误差E，通过求偏导就可以求得最优的权重。（不要忘记学习率） 3. 举例说明图中元素：两个输入；隐层: b1, w1, w2, w3, w4 (都有初始值）输出层：b2, w5, w6, w7, w8（赋了初始值） 3.1 前向传播则误差： 3.2 反向传播参数更新：求误差对w1的偏导 ：注意，w1对两个输出的误差都有影响通过以上过程可以更新所有权重，就可以再次迭代更新了，直到满足条件。 三、python代码实现&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以上述例子，用python可写出如下代码，并附有详解： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091import numpy as npimport matplotlib.pyplot as pltimport matha=np.array([0.05,0.1]) #a1,a2的输入值weight1=np.array([[0.15,0.25],[0.2,0.3]]) #a1对b1,b2的权重，a2对b1，b2的权重weight2=np.array([[0.4,0.5],[0.45,0.55]]) #b1对c1,c2的权重，b2对c1，c2的权重target=np.array([0.01,0.99])d1=0.35 #输入层的偏置（1）的权重d2=0.6 #隐藏层的偏置（1）的权重β=0.5 #学习效率#一：前向传播#计算输入层到隐藏层的输入值，得矩阵netb1,netb2netb=np.dot(a,weight1)+d1#计算隐藏层的输出值,得到矩阵outb1,outb2m=[]for i in range(len(netb)): outb=1.0 / (1.0 + math.exp(-netb[i])) m.append(outb)m=np.array(m)#计算隐藏层到输出层的输入值，得矩阵netc1,netc2netc=np.dot(m,weight2)+d2#计算隐藏层的输出值,得到矩阵outc1,outc2n=[]for i in range(len(netc)): outc=1.0 / (1.0 + math.exp(-netc[i])) n.append(outc)n=np.array(n)#二：反向传播count=0 #计数e=0 #误差 E=[] #统计误差#梯度下降while True: count+=1 #总误差对w1-w4的偏导 pd1=(-(target[0]-n[0])*n[0]*(1-n[0])*weight2[0][0]-(target[1]-n[1])*n[1]*(1-n[1])*weight2[0][1])*m[0]*(1-m[0])*a[0] pd2=(-(target[0]-n[0])*n[0]*(1-n[0])*weight2[0][0]-(target[1]-n[1])*n[1]*(1-n[1])*weight2[0][1])*m[0]*(1-m[0])*a[1] pd3=(-(target[0]-n[0])*n[0]*(1-n[0])*weight2[1][0]-(target[1]-n[1])*n[1]*(1-n[1])*weight2[0][1])*m[0]*(1-m[0])*a[0] pd4=(-(target[0]-n[0])*n[0]*(1-n[0])*weight2[1][1]-(target[1]-n[1])*n[1]*(1-n[1])*weight2[0][1])*m[0]*(1-m[0])*a[1] weight1[0][0]=weight1[0][0]-β*pd1 weight1[1][0]=weight1[1][0]-β*pd2 weight1[0][1]=weight1[0][1]-β*pd3 weight1[1][1]=weight1[1][1]-β*pd4 #总误差对w5-w8的偏导 pd5=-(target[0]-n[0])*n[0]*(1-n[0])*m[0] pd6=-(target[0]-n[0])*n[0]*(1-n[0])*m[1] pd7=-(target[1]-n[1])*n[1]*(1-n[1])*m[0] pd8=-(target[1]-n[1])*n[1]*(1-n[1])*m[1] weight2[0][0]=weight2[0][0]-β*pd5 weight2[1][0]=weight2[1][0]-β*pd6 weight2[0][1]=weight2[0][1]-β*pd7 weight2[1][1]=weight2[1][1]-β*pd8 netb=np.dot(a,weight1)+d1 m=[] for i in range(len(netb)): outb=1.0 / (1.0 + math.exp(-netb[i])) m.append(outb) m=np.array(m) netc=np.dot(m,weight2)+d2 n=[] for i in range(len(netc)): outc=1.0 / (1.0 + math.exp(-netc[i])) n.append(outc) n=np.array(n) #计算总误差 for j in range(len(n)): e += (target[j]-n[j])**2/2 E.append(e) #判断 if e&lt;0.0000001: break else: e=0print(count)print(e)print(n)plt.plot(range(len(E)),E,label='error')plt.legend() plt.xlabel('time')plt.ylabel('error')plt.show() 四、BP神经网络的优缺点BP神经网络的优点： 非线性映射能力 泛化能力 容错能力，允许输入样本中带有较大误差甚至个别错误。反应正确规律的知识来自全体样本，个别样本中的误差不能左右对权矩阵的调整 BP神经网络的缺陷： 需要的参数过多，而且参数的选择没有有效的方法。确定一个BP神经网络需要知道：网络的层数、每一层神经元的个数和权值。权值可以通过学习得到，如果，隐层神经元数量太多会引起过学习，如果隐层神经元个数太少会引起欠学习。此外学习率的选择也是需要考虑。目前来说，对于参数的确定缺少一个简单有效的方法，所以导致算法很不稳定； 属于监督学习，对于样本有较大依赖性，网络学习的逼近和推广能力与样本有很大关系，如果样本集合代表性差，样本矛盾多，存在冗余样本，网络就很难达到预期的性能； 由于权值是随机给定的，所以BP神经网络具有不可重现性；","categories":[],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://hpu-yz.github.io/tags/深度学习/"},{"name":"人工神经网络（ANN）","slug":"人工神经网络（ANN）","permalink":"https://hpu-yz.github.io/tags/人工神经网络（ANN）/"}]},{"title":"多种相似度计算方法","slug":"多种相似度计算的python实现","date":"2019-07-23T16:00:00.000Z","updated":"2020-05-25T08:06:34.856Z","comments":true,"path":"2019/07/24/duo-chong-xiang-si-du-ji-suan-de-python-shi-xian/","link":"","permalink":"https://hpu-yz.github.io/2019/07/24/duo-chong-xiang-si-du-ji-suan-de-python-shi-xian/","excerpt":"","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在机器学习中有很多地方要计算相似度，比如聚类分析和协同过滤。计算相似度的有许多方法，其中有欧几里德距离(欧式距离)、曼哈顿距离、Jaccard系数和皮尔逊相关度等等。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们这里把一些常用的相似度计算方法，用python进行实现以下。大家都是初学者，我认为把公式先写下来，然后再写代码去实现比较好。 欧几里德距离(欧式距离)几个数据集之间的相似度一般是基于每对对象间的距离计算。最常用的当然是欧几里德距离，其公式为：12345678910111213#-*-coding:utf-8 -*-#计算欧几里德距离：def euclidean(p,q):#如果两数据集数目不同，计算两者之间都对应有的数same = 0for i in p: if i in q: same +=1#计算欧几里德距离,并将其标准化e = sum([(p[i] - q[i])**2 for i in range(same)])return 1/(1+e**.5) 我们用数据集可以去算一下： 123p = [1,3,2,3,4,3]q = [1,3,4,3,2,3,4,3]print euclidean(p,q) 得出结果是：0.261203874964 皮尔逊相关度几个数据集中出现异常值的时候，欧几里德距离就不如皮尔逊相关度‘稳定’，它会在出现偏差时倾向于给出更好的结果。其公式为： 123456789101112131415161718192021222324252627-*-coding:utf-8 -*-#计算皮尔逊相关度：def pearson(p,q):#只计算两者共同有的 same = 0 for i in p: if i in q: same +=1 n = same #分别求p，q的和 sumx = sum([p[i] for i in range(n)]) sumy = sum([q[i] for i in range(n)]) #分别求出p，q的平方和 sumxsq = sum([p[i]**2 for i in range(n)]) sumysq = sum([q[i]**2 for i in range(n)]) #求出p，q的乘积和 sumxy = sum([p[i]*q[i] for i in range(n)]) # print sumxy #求出pearson相关系数 up = sumxy - sumx*sumy/n down = ((sumxsq - pow(sumxsq,2)/n)*(sumysq - pow(sumysq,2)/n))**.5 #若down为零则不能计算，return 0 if down == 0 :return 0 r = up/down return r 用同样的数据集去计算： 123p = [1,3,2,3,4,3]q = [1,3,4,3,2,3,4,3]print pearson(p,q) 得出结果是：0.00595238095238 曼哈顿距离曼哈顿距离是另一种相似度计算方法，不是经常需要，但是我们仍然学会如何用python去实现，其公式为： 1234567891011121314#-*-coding:utf-8 -*-#计算曼哈顿距离：def manhattan(p,q):#只计算两者共同有的 same = 0 for i in p: if i in q: same += 1#计算曼哈顿距离 n = same vals = range(n) distance = sum(abs(p[i] - q[i]) for i in vals) return distance 用以上的数据集去计算： 123p = [1,3,2,3,4,3]q = [1,3,4,3,2,3,4,3]print manhattan(p,q) 得出结果为4 小结&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里只讲述了三种相似度的计算方法，事实上还有很多种，由于我也是刚学，其他的方法还不是很了解，以后碰到了再补上。一般情况下，这三种方法是最常用的，足够我们使用了。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://hpu-yz.github.io/tags/机器学习/"},{"name":"相似度","slug":"相似度","permalink":"https://hpu-yz.github.io/tags/相似度/"}]},{"title":"推荐系统之矩阵分解(MF)及其python实现","slug":"推荐系统之矩阵分解(MF)及其python实现","date":"2019-07-22T16:00:00.000Z","updated":"2020-05-25T08:07:06.256Z","comments":true,"path":"2019/07/23/tui-jian-xi-tong-zhi-ju-zhen-fen-jie-mf-ji-qi-python-shi-xian/","link":"","permalink":"https://hpu-yz.github.io/2019/07/23/tui-jian-xi-tong-zhi-ju-zhen-fen-jie-mf-ji-qi-python-shi-xian/","excerpt":"","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;目前推荐系统中用的最多的就是矩阵分解方法，在Netflix Prize推荐系统大赛中取得突出效果。以用户-项目评分矩阵为例，矩阵分解就是预测出评分矩阵中的缺失值，然后根据预测值以某种方式向用户推荐。今天以“用户-项目评分矩阵R（M×N）”说明矩阵分解方式的原理以及python实现。 一、矩阵分解1.案例引入有如下R(5,4)的打分矩阵：（“-”表示用户没有打分） 其中打分矩阵R(n,m)是n行和m列，n表示user个数，m行表示item个数那么，如何根据目前的矩阵R（5,4）如何对未打分的商品进行评分的预测（如何得到分值为0的用户的打分值）？ ——矩阵分解的思想可以解决这个问题，其实这种思想可以看作是有监督的机器学习问题（回归问题）。 矩阵分解的过程中，,矩阵R可以近似表示为矩阵P与矩阵Q的乘积：矩阵P(n,k)表示n个user和k个特征之间的关系矩阵，这k个特征是一个中间变量，矩阵Q(k,m)的转置是矩阵Q(m,k)，矩阵Q(m,k)表示m个item和K个特征之间的关系矩阵，这里的k值是自己控制的，可以使用交叉验证的方法获得最佳的k值。为了得到近似的R(n,m)，必须求出矩阵P和Q，如何求它们呢？ 2.推导步骤 首先令： 对于式子1的左边项，表示的是r^ 第i行，第j列的元素值，对于如何衡量，我们分解的好坏呢，式子2，给出了衡量标准，也就是损失函数，平方项损失，最后的目标，就是每一个元素(非缺失值)的e(i,j)的总和最小值 使用梯度下降法获得修正的p和q分量： 求解损失函数的负梯度： 根据负梯度的方向更新变量： 不停迭代直到算法最终收敛（直到sum(e^2) &lt;=阈值，即梯度下降结束条件：f(x)的真实值和预测值小于自己设定的阈值） 为了防止过拟合，增加正则化项 3.加入正则项的损失函数求解 通常在求解的过程中，为了能够有较好的泛化能力，会在损失函数中加入正则项，以对参数进行约束，加入正则L2范数的损失函数为： 对正则化不清楚的，公式可化为： 使用梯度下降法获得修正的p和q分量： 求解损失函数的负梯度： 根据负梯度的方向更新变量： 4.预测预测利用上述的过程，我们可以得到矩阵和，这样便可以为用户 i 对商品 j 进行打分： 二、python代码实现以下是根据上文的评分例子做的一个矩阵分解算法，并且附有代码详解。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051from math import *import numpyimport matplotlib.pyplot as pltdef matrix_factorization(R,P,Q,K,steps=5000,alpha=0.0002,beta=0.02): #矩阵因子分解函数，steps：梯度下降次数；alpha：步长；beta：β。 Q=Q.T # .T操作表示矩阵的转置 result=[] for step in range(steps): #梯度下降 for i in range(len(R)): for j in range(len(R[i])): eij=R[i][j]-numpy.dot(P[i,:],Q[:,j]) # .DOT表示矩阵相乘 for k in range(K): if R[i][j]&gt;0: #限制评分大于零 P[i][k]=P[i][k]+alpha*(2*eij*Q[k][j]-beta*P[i][k]) #增加正则化，并对损失函数求导，然后更新变量P Q[k][j]=Q[k][j]+alpha*(2*eij*P[i][k]-beta*Q[k][j]) #增加正则化，并对损失函数求导，然后更新变量Q eR=numpy.dot(P,Q) e=0 for i in range(len(R)): for j in range(len(R[i])): if R[i][j]&gt;0: e=e+pow(R[i][j]-numpy.dot(P[i,:],Q[:,j]),2) #损失函数求和 for k in range(K): e=e+(beta/2)*(pow(P[i][k],2)+pow(Q[k][j],2)) #加入正则化后的损失函数求和 result.append(e) if e&lt;0.001: #判断是否收敛，0.001为阈值 break return P,Q.T,resultif __name__ == '__main__': #主函数 R=[ #原始矩阵 [5,3,0,1], [4,0,0,1], [1,1,0,5], [1,0,0,4], [0,1,5,4] ] R=numpy.array(R) N=len(R) #原矩阵R的行数 M=len(R[0]) #原矩阵R的列数 K=3 #K值可根据需求改变 P=numpy.random.rand(N,K) #随机生成一个 N行 K列的矩阵 Q=numpy.random.rand(M,K) #随机生成一个 M行 K列的矩阵 nP,nQ,result=matrix_factorization(R,P,Q,K) print(R) #输出原矩阵 R_MF=numpy.dot(nP,nQ.T) print(R_MF) #输出新矩阵 #画图 plt.plot(range(len(result)),result) plt.xlabel(\"time\") plt.ylabel(\"loss\") plt.show()","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://hpu-yz.github.io/tags/机器学习/"},{"name":"推荐系统","slug":"推荐系统","permalink":"https://hpu-yz.github.io/tags/推荐系统/"},{"name":"矩阵分解","slug":"矩阵分解","permalink":"https://hpu-yz.github.io/tags/矩阵分解/"}]},{"title":"暑期培训第一次测试题总结","slug":"暑期培训第一次测试题总结","date":"2019-07-21T16:00:00.000Z","updated":"2020-05-25T08:06:56.756Z","comments":true,"path":"2019/07/22/shu-qi-pei-xun-di-yi-ci-ce-shi-ti-zong-jie/","link":"","permalink":"https://hpu-yz.github.io/2019/07/22/shu-qi-pei-xun-di-yi-ci-ce-shi-ti-zong-jie/","excerpt":"","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里是一些暑期培训第一次测试题的部分解释，经过这次测试的摧残，总结备录一下，方便日后回顾复习。 Feeling&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;经过几天的学习，7月22日下午，进行了第一次检测。开始以为会让我们推导一些公式什么的，结果当拿到测试题的时候，一首凉凉送给自己……开始的选择题和填空题还能接受，看到简答题，这都是什么，有种似曾相识的感觉，但就是写不出来。为时四十分钟的考试结束后，不到半个小时成绩就出来了，虽然成绩不那么好，但排名还行，然后学长给我们进行了讲解答疑，发现自己学的有点粗糙，没有注意那些细节性问题和概念，学以致用这方面也是有点差的。 Test 1为什么一般需要划分出额外的校验集(validation set)用于超参数调整，而不选择直接使用测试集(test set)?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;校验集是用于调整超参数的，从而更好的优化训练模型。测试集是用于在完成神经网络训练过程后,为了客观评价模型在其未见过(未曾影响普通参数和超参数选择)的数据上的性能,而且测试集不能提出对参数或者超参数的修改意见,只能作为评价网络性能的一个指标。 （备注：不清楚这三个数据集概念及其作用的，可看下我以前写的关于这些的一篇文章。传送门：） Test 2批量梯度下降(Batch Gradient Descent)和随机梯度下降(Stochastic Gradient Descent)在应对鞍点时有何不同表现？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们要先知道什么是BGD和SGD，从两者的运算方法上，我们就可以得知不同之处。 （1）批量梯度下降法（Batch Gradient Descent） ：在更新参数时都使用所有的样本来进行更新。 优点：全局最优解，能保证每一次更新权值，都能降低损失函数；易于并行实现。 缺点：当样本数目很多时，训练过程会很慢。 （2）随机梯度下降法（Stochastic Gradient Descent）：在更新参数时都使用一个样本来进行更新。每一次跟新参数都用一个样本，更新很多次。如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将参数迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次，这种方式计算复杂度太高。 优点：训练速度快； 缺点：准确度下降，并不是全局最优；不易于并行实现。从迭代的次数上来看，随机梯度下降法迭代的次数较多，在解空间的搜索过程看起来很盲目。噪音很多，使得它并不是每次迭代都向着整体最优化方向。 Test 3当一个模型训练完后若在训练集上的loss非常高，请问如何在不对代码进行全面排查的前提下，以最快速度定位是模型本身的拟合能力不足还是代码的实现存在某种错误？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;废话不多说，直接上图： Test 4假设我们在训练一个使用Sigmoid激活函数的全连接神经网络。在对其权重进行初始化时，为什么一般会倾向于让初始值的绝对值偏小？如果需要这样，为何不直接使用0进行初始化？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于逻辑回归，把权重初始化为0当然也是可以的，但是对于一个神经网络，如果你把权重或者参数都初始化为0，那么梯度下降将不会起作用。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果权值全初始化为0，则无法更新权值。这是由于前向传播中，所有节点输出值均相同，由于此处使用了sigmod激活函数，所以此处所有神经节点输出都为1/2，而在反向传播每个节点输出值对损失函数的偏导时，涉及到对权值相乘后的求和，该项永远为0，故所乘的结果也必然为0，这样在计算权值对算是函数的偏导时，其偏导必然为0，所有权值偏导都为0，那么就不要指望使用梯度下降法能更新权值了，自然神经网络的训练也就无法进行下去了。 Test 5在CNN中梯度不稳定指的是什么？在神经网络训练过程中，为什么会出现梯度消失的问题？如何解决？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;神经网络中的梯度不稳定指的是梯度消失和梯度爆炸问题。（备注：对于这两种问题的具体解释和为什么会出现这种问题，以及解决方法，这里不具体讨论了，我会在以后的文章中具体解释到的。） Test 6为什么在神经网络中使用交叉熵而不是均方差作为误差函数？1. 神经网络中如果预测值与实际值的误差越大，那么在反向传播训练的过程中，各种参数调整的幅度就要更大，从而使训练更快收敛，如果预测值与实际值的误差小，各种参数调整的幅度就要小，从而减少震荡。 2. 使用平方误差损失函数，误差增大参数的梯度会增大，但是当误差很大时，参数的梯度就会又减小了。 3. 使用交叉熵损失是函数，误差越大参数的梯度也越大，能够快速收敛。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（备注：对于结论的推导过程会总结在另一篇文章里。）","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://hpu-yz.github.io/tags/机器学习/"}]},{"title":"KNN算法及python实现","slug":"K-最近邻分类算法(KNN)及python实现","date":"2019-07-19T16:00:00.000Z","updated":"2020-05-25T08:07:16.008Z","comments":true,"path":"2019/07/20/k-zui-jin-lin-fen-lei-suan-fa-knn-ji-python-shi-xian/","link":"","permalink":"https://hpu-yz.github.io/2019/07/20/k-zui-jin-lin-fen-lei-suan-fa-knn-ji-python-shi-xian/","excerpt":"","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;KNN算法即K-Nearest Neighbor，也是机器学习十大经典算法之一。前文讲解了K-means算法，今天我们就继续讲KNN算法，两者看起来挺相似的，但区别还是很大的，看完本片文章你就会明白了。 一、引入问题：确定绿色圆是属于红色三角形、还是蓝色正方形？KNN的思想：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从上图中我们可以看到，图中的数据集是良好的数据，即都打好了label，一类是蓝色的正方形，一类是红色的三角形，那个绿色的圆形是我们待分类的数据。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果K=3，那么离绿色点最近的有2个红色三角形和1个蓝色的正方形，这3个点投票，于是绿色的这个待分类点属于红色的三角形&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果K=5，那么离绿色点最近的有2个红色三角形和3个蓝色的正方形，这5个点投票，于是绿色的这个待分类点属于蓝色的正方形&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;即如果一个样本在特征空间中的k个最相邻的样本中，大多数属于某一个类别，则该样本也属于这个类别。我们可以看到，KNN本质是基于一种数据统计的方法！其实很多机器学习算法也是基于数据统计的。 二、KNN算法1.介绍&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;KNN即K-最近邻分类算法（K-Nearest Neighbor），是一种memory-based learning，也叫instance-based learning，属于lazy learning。即它没有明显的前期训练过程，而是程序开始运行时，把数据集加载到内存后，不需要进行训练，就可以开始分类了。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;KNN也是一种监督学习算法，通过计算新数据与训练数据特征值之间的距离，然后选取K(K&gt;=1)个距离最近的邻居进行分类判(投票法)或者回归。若K=1，新数据被简单分配给其近邻的类。 2.步骤1）计算测试数据与各个训练数据之间的距离； (计算距离的方式前文讲k-means时说过，不清楚的可以去查看以下➡传送门) 2）按照距离的递增关系进行排序； 3）选取距离最小的K个点； K值是由自己来确定的 4）确定前K个点所在类别的出现频率； 5）返回前K个点中出现频率最高的类别作为测试数据的预测分类。 说明：对于步骤5的预测分类有以下两种方法 多数表决法：多数表决法类似于投票的过程，也就是在 K 个邻居中选择类别最多的种类作为测试样本的类别。 加权表决法：根据距离的远近，对近邻的投票进行加权，距离越近则权重越大，通过权重计算结果最大值的类为测试样本的类别。 特点1) 非参数统计方法：不需要引入参数2) K的选择：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;K = 1时，将待分类样本划入与其最接近的样本的类。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;K = |X|时，仅根据训练样本进行频率统计，将待分类样本划入最多的类。K需要合理选择，太小容易受干扰，太大增加计算复杂性。3) 算法的复杂度：维度灾难，当维数增加时，所需的训练样本数急剧增加，一般采用降维处理。 三、算法优缺点优点 简单、有效。 重新训练的代价较低(类别体系的变化和训练集的变化，在Web环境和电子商务应用中是很常见的)。 计算时间和空间线性于训练集的规模(在一些场合不算太大)。 由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合。 该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。 缺点 KNN算法是懒散学习方法(lazy learning)，而一些积极学习的算法要快很多。 需要存储全部的训练样本 输出的可解释性不强，例如决策树的可解释性较强。 该算法在分类时有个主要的不足是，当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。该算法只计算最近的邻居样本，某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并不能影响运行结果。可以采用权值的方法(和该样本距离小的邻居权值大)来改进。 计算量较大。目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本。 四、KNN与K-means的区别&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;废话不多说，咱直接上图：相似点：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;虽然两者有很大且别，但两者也有共同之处。都包含了一个过程：给定一个点，在数据集找离它最近的点，即都用到了NN(Nearest Neighbor)算法。 五、python实例实现&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下面引入一个实例，通过python代码具体看下KNN算法的流程。 12345678910111213141516171819202122232425262728293031323334353637from numpy import *import operatordataSet = array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]])labels = ['A','A','B','B']def classify0(inX,dataSet,labels,k): #求出样本集的行数，也就是labels标签的数目 dataSetSize = dataSet.shape[0] #构造输入值和样本集的差值矩阵 diffMat = tile(inX,(dataSetSize,1)) - dataSet #计算欧式距离 sqDiffMat = diffMat**2 sqDistances = sqDiffMat.sum(axis=1) distances = sqDistances**0.5 #求距离从小到大排序的序号 sortedDistIndicies = distances.argsort() #对距离最小的k个点统计对应的样本标签 classCount = &#123;&#125; for i in range(k): #取第i+1邻近的样本对应的类别标签 voteIlabel = labels[sortedDistIndicies[i]] #以标签为key，标签出现的次数为value将统计到的标签及出现次数写进字典 classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1 #对字典按value从大到小排序 sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True) #返回排序后字典中最大value对应的key return sortedClassCount[0][0]if __name__ == '__main__': print(classify0([1.1,0],dataSet,labels,3))","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://hpu-yz.github.io/tags/机器学习/"},{"name":"KNN","slug":"KNN","permalink":"https://hpu-yz.github.io/tags/KNN/"}]},{"title":"K-means算法及python实现","slug":"K-means聚类算法原理及python实现","date":"2019-07-18T16:00:00.000Z","updated":"2020-05-25T08:07:18.900Z","comments":true,"path":"2019/07/19/k-means-ju-lei-suan-fa-yuan-li-ji-python-shi-xian/","link":"","permalink":"https://hpu-yz.github.io/2019/07/19/k-means-ju-lei-suan-fa-yuan-li-ji-python-shi-xian/","excerpt":"","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;K-means(Thek-meansalgorithm)是机器学习十大经典算法之一，同时也是最为经典的无监督聚类（Unsupervised Clustering）算法。接触聚类算法，首先需要了解k-means算法的实现原理和步骤。本文将对k-means算法的基本原理和实现实例进行分析。 一.聚类算法的简介&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于”监督学习“(supervised learning)，其训练样本是带有标记信息的，并且监督学习的目的是：对带有标记的数据集进行模型学习，从而便于对新的样本进行分类。而在“无监督学习”(unsupervised learning)中，训练样本的标记信息是未知的，目标是通过对无标记训练样本的学习来揭示数据的内在性质及规律，为进一步的数据分析提供基础。对于无监督学习，应用最广的便是”聚类“(clustering)。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;”聚类算法“试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个“簇”(cluster)，通过这样的划分，每个簇可能对应于一些潜在的概念或类别。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们可以通过下面这个图来理解： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上图是未做标记的样本集，通过他们的分布，我们很容易对上图中的样本做出以下几种划分。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当需要将其划分为两个簇时，即 k=2时：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当需要将其划分为四个簇时，即 k=4 时：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 二.K-means聚类算法&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kmeans算法又名k均值算法,K-means算法中的k表示的是聚类为k个簇，means代表取每一个聚类中数据值的均值作为该簇的中心，或者称为质心，即用每一个的类的质心对该簇进行描述。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其算法思想大致为：先从样本集中随机选取 k个样本作为簇中心，并计算所有样本与这 k个“簇中心”的距离，对于每一个样本，将其划分到与其距离最近的“簇中心”所在的簇中，对于新的簇计算各个簇的新的“簇中心”。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;根据以上描述，我们大致可以猜测到实现kmeans算法的主要四点： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（1）簇个数 k 的选择 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（2）各个样本点到“簇中心”的距离 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（3）根据新划分的簇，更新“簇中心” &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;（4）重复上述2、3过程，直至”簇中心”没有移动 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;优缺点： 优点：容易实现 缺点：可能收敛到局部最小值，在大规模数据上收敛较慢 三.K-means算法步骤详解Step1.K值的选择 k 的选择一般是按照实际需求进行决定，或在实现算法时直接给定 k 值。 说明： A.质心数量由用户给出，记为k，k-means最终得到的簇数量也是k B.后来每次更新的质心的个数都和初始k值相等 C.k-means最后聚类的簇个数和用户指定的质心个数相等，一个质心对应一个簇，每个样本只聚类到一个簇里面 D.初始簇为空 Step2.距离度量&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;将对象点分到距离聚类中心最近的那个簇中需要最近邻的度量策略，在欧式空间中采用的是欧式距离，在处理文档中采用的是余弦相似度函数，有时候也采用曼哈顿距离作为度量，不同的情况实用的度量公式是不同的。 2.1.欧式距离 2.2.曼哈顿距离 2.3.余弦相似度&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A与B表示向量(x1,y1)，(x2,y2)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分子为A与B的点乘，分母为二者各自的L2相乘，即将所有维度值的平方相加后开方。 说明：A.经过step2，得到k个新的簇，每个样本都被分到k个簇中的某一个簇B.得到k个新的簇后，当前的质心就会失效，需要计算每个新簇的自己的新质心 Step3.新质心的计算&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于分类后的产生的k个簇，分别计算到簇内其他点距离均值最小的点作为质心（对于拥有坐标的簇可以计算每个簇坐标的均值作为质心） 说明：A.比如一个新簇有3个样本：[[1,4], [2,5], [3,6]]，得到此簇的新质心=[(1+2+3)/3, (4+5+6)/3]B.经过step3，会得到k个新的质心，作为step2中使用的质心 Step4.是否停止K-means&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;质心不再改变，或给定loop最大次数loopLimit 说明：A当每个簇的质心，不再改变时就可以停止k-menasB.当loop次数超过looLimit时，停止k-meansC.只需要满足两者的其中一个条件，就可以停止k-meansC.如果Step4没有结束k-means，就再执行step2-step3-step4D.如果Step4结束了k-means，则就打印(或绘制)簇以及质心 四.python实现+代码详解&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以下是python得实例代码以及代码的详解，应该可以理解的。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import randomimport pandas as pdimport numpy as npimport matplotlib.pyplot as plt# 计算欧拉距离def calcDis(dataSet, centroids, k): clalist=[] for data in dataSet: diff = np.tile(data, (k, 1)) - centroids #相减 (np.tile(a,(2,1))就是把a先沿x轴复制1倍，即没有复制，仍然是 [0,1,2]。 再把结果沿y方向复制2倍得到array([[0,1,2],[0,1,2]])) squaredDiff = diff ** 2 #平方 squaredDist = np.sum(squaredDiff, axis=1) #和 (axis=1表示行) distance = squaredDist ** 0.5 #开根号 clalist.append(distance) clalist = np.array(clalist) #返回一个每个点到质点的距离len(dateSet)*k的数组 return clalist# 计算质心def classify(dataSet, centroids, k): # 计算样本到质心的距离 clalist = calcDis(dataSet, centroids, k) # 分组并计算新的质心 minDistIndices = np.argmin(clalist, axis=1) #axis=1 表示求出每行的最小值的下标 newCentroids = pd.DataFrame(dataSet).groupby(minDistIndices).mean() #DataFramte(dataSet)对DataSet分组，groupby(min)按照min进行统计分类，mean()对分类结果求均值 newCentroids = newCentroids.values # 计算变化量 changed = newCentroids - centroids return changed, newCentroids# 使用k-means分类def kmeans(dataSet, k): # 随机取质心 centroids = random.sample(dataSet, k) # 更新质心 直到变化量全为0 changed, newCentroids = classify(dataSet, centroids, k) while np.any(changed != 0): changed, newCentroids = classify(dataSet, newCentroids, k) centroids = sorted(newCentroids.tolist()) #tolist()将矩阵转换成列表 sorted()排序 # 根据质心计算每个集群 cluster = [] clalist = calcDis(dataSet, centroids, k) #调用欧拉距离 minDistIndices = np.argmin(clalist, axis=1) for i in range(k): cluster.append([]) for i, j in enumerate(minDistIndices): #enymerate()可同时遍历索引和遍历元素 cluster[j].append(dataSet[i]) return centroids, cluster # 创建数据集def createDataSet(): return [[1, 1], [1, 2], [2, 1], [6, 4], [6, 3], [5, 4]]if __name__=='__main__': dataset = createDataSet() centroids, cluster = kmeans(dataset, 2) print('质心为：%s' % centroids) print('集群为：%s' % cluster) for i in range(len(dataset)): plt.scatter(dataset[i][0],dataset[i][1], marker = 'o',color = 'green', s = 40 ,label = '原始点') # 记号形状 颜色 点的大小 设置标签 for j in range(len(centroids)): plt.scatter(centroids[j][0],centroids[j][1],marker='x',color='red',s=50,label='质心') plt.show 五.K-means算法补充1.对初始化敏感，初始质点k给定的不同，可能会产生不同的聚类结果。如下图所示，右边是k=2的结果，这个就正好，而左图是k=3的结果，可以看到右上角得这两个簇应该是可以合并成一个簇的。 改进：对k的选择可以先用一些算法分析数据的分布，如重心和密度等，然后选择合适的k2.使用存在局限性，如下面这种非球状的数据分布就搞不定了：3.数据集比较大的时候，收敛会比较慢。 4.最终会收敛。不管初始点如何选择，最终都会收敛。可是是全局收敛，也可能是局部收敛。 六.小结&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1. 聚类是一种无监督的学习方法。聚类区别于分类，即事先不知道要寻找的内容，没有预先设定好的目标变量。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2. 聚类将数据点归到多个簇中，其中相似的数据点归为同一簇，而不相似的点归为不同的簇。相似度的计算方法有很多，具体的应用选择合适的相似度计算方法 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3. K-means聚类算法，是一种广泛使用的聚类算法，其中k是需要指定的参数，即需要创建的簇的数目，K-means算法中的k个簇的质心可以通过随机的方式获得，但是这些点需要位于数据范围内。在算法中，计算每个点到质心得距离，选择距离最小的质心对应的簇作为该数据点的划分，然后再基于该分配过程后更新簇的质心。重复上述过程，直至各个簇的质心不再变化为止。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4. K-means算法虽然有效，但是容易受到初始簇质心的情况而影响，有可能陷入局部最优解。为了解决这个问题，可以使用另外一种称为二分K-means的聚类算法。二分K-means算法首先将所有数据点分为一个簇；然后使用K-means（k=2）对其进行划分；下一次迭代时，选择使得SSE下降程度最大的簇进行划分；重复该过程，直至簇的个数达到指定的数目为止。实验表明，二分K-means算法的聚类效果要好于普通的K-means聚类算法。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://hpu-yz.github.io/tags/机器学习/"},{"name":"K-means","slug":"K-means","permalink":"https://hpu-yz.github.io/tags/K-means/"}]},{"title":"数据集的划分","slug":"数据集的划分--训练集、验证集和测试集","date":"2019-07-17T16:00:00.000Z","updated":"2020-05-25T08:03:17.750Z","comments":true,"path":"2019/07/18/shu-ju-ji-de-hua-fen-xun-lian-ji-yan-zheng-ji-he-ce-shi-ji/","link":"","permalink":"https://hpu-yz.github.io/2019/07/18/shu-ju-ji-de-hua-fen-xun-lian-ji-yan-zheng-ji-he-ce-shi-ji/","excerpt":"","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在机器学习中，经常提到训练集和测试集，验证集似有似无。感觉挺好奇的，就仔细查找了文献。以下谈谈训练集、验证集和测试集。 为什么要划分数据集为训练集、验证集和测试集？&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;做科研，就要提出问题，找到解决方法，并证明其有效性。这里的工作有3个部分，一个是提出问题，一个是找到解决方法，另一个是证明有效性。每一个部分都可以作为科研的对象，研究的越靠前，则越偏向科学，越靠后，则越偏向技术，因此叫做科学与技术。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在人工智能领域，证明一个模型的有效性，就是对于某一问题，有一些数据，而我们提出的模型可以（部分）解决这个问题，那如何来证明呢？这和我们平时的考试也是一样的，证明我们掌握了某类知识，就是去参加考试。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;好，那么如何设计考试，让这个考试可以较为客观的考察出每个人的能力呢（注意，不是让每个人都得最高分）？回想我们的高中阶段，有一些教材，让我们平时学习其基本知识（训练集），有一些模拟考试，让我们知道我们到底掌握的怎么样，然后再改进我们的学习（验证集），最后的高考决定我们的去向（测试集）。这样的类比，是不是就很清楚了。 训练集、验证集和测试集1. **训练集**：顾名思义指的是用于训练的样本集合,主要用来训练神经网络中的参数。 2. **验证集**：从字面意思理解即为用于验证模型性能的样本集合.不同神经网络在训练集上训练结束后,通过验证集来比较判断各个模型的性能.这里的不同模型主要是指对应不同超参数的神经网络,也可以指完全不同结构的神经网络。 3. **测试集**：对于训练完成的神经网络,测试集用于客观的评价神经网络的性能。 如何划分训练集、验证集和测试集&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这个问题其实非常基础，也非常明确，在Scikit-learn里提供了各种各样的划分方法。无论是单一的训练集、验证集和测试集，还是进行交叉验证，你都会找到调用的方法，因此我们这里主要介绍两种常见的方法。 1. 前人给出训练集、验证集和测试集 对于这种情况，那么只能跟随前人的数据划分进行，一般的比赛也是如此。一定不要使用测试集来调整性能（测试集已知的情况下），尽管存在使用这种方法来提升模型的性能的行为，但是我们并不推荐这么做。最正常的做法应当是使用训练集来学习，并使用验证集来调整超参数。当在验证集上取得最优的模型时，此时就可以使用此模型的超参数来重新训练（训练集+验证集），并用测试集评估最终的性能。 我们首先说明加入验证集重新训练和不加有啥区别，从理论上讲，一方面学习的样本增多，应当是会提升模型性能的，第二，其在验证集上取得最优的模型与验证集的分布的契合度是最高的，因此最终的模型会更接近验证集的分布。 其次再说明验证集和测试集上的性能差异。事实上，在验证集上取得最优的模型，未必在测试集上取得最优。其原因就是训练的模型是否对于该问题有着较好的泛化能力，即没有对验证集产生过拟合现象。正因为有这种情况的发生，才会有人使用测试集的最优值作为最终的结果（而不管验证集的好坏）。 2. 前人没有明确给出数据集的划分 这时候可以采取第一种划分方法，对于样本数较小的数据集，同样可以采取交叉验证的方法。 交叉验证的方法的使用场景有很多，我们这里是针对不同的模型的性能好坏进行评估。 使用交叉验证，可以获得更为客观的性能差异。当使用第一种方法时，我们更建议使用P值来做显著性检验，从而保证性能差异的客观性。而使用第二种方法，即交叉验证时，我们选取其性能表现的均值作为最终的结果，更能体现该模型的泛化能力。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.存在验证集&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里五倍交叉验证是用于进行调参，此时不接触测试集。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据集首先划分出训练集与测试集（可以是4:1或者9:1）。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其次，在训练集中，再划分出验证集（通常也是4:1或者9：1）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;然后对于训练集和验证集进行5折交叉验证，选取出最优的超参数，然后把训练集和验证集一起训练出最终的模型。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.不存在验证集&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该情况通常是对比不同的模型，如自己的模型和别人的模型的性能好坏。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;只需要把数据集划分为训练集和测试集即可，然后选取5次试验的平均值作为最终的性能评价。 验证集和测试集的区别&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那么,训练集、校验集和测试集之间又有什么区别呢?一般而言,训练集与后两者之间较易分辨,校验集和测试集之间的概念较易混淆.个人是从下面的角度来理解的: 神经网络在网络结构确定的情况下,有两部分影响模型最终的性能,一是普通参数(比如权重w和偏置b),另一个是超参数(例如学习率,网络层数).普通参数我们在训练集上进行训练,超参数我们一般人工指定(比较不同超参数的模型在校验集上的性能).那为什么我们不像普通参数一样在训练集上训练超参数呢?(花书给出了解答)一是：超参数一般难以优化(无法像普通参数一样通过梯度下降的方式进行优化).二是：超参数很多时候不适合在训练集上进行训练,例如,如果在训练集上训练能控制模型容量的超参数,这些超参数总会被训练成使得模型容量最大的参数(因为模型容量越大,训练误差越小),所以训练集上训练超参数的结果就是模型绝对过拟合. 正因为超参数无法在训练集上进行训练,因此我们单独设立了一个验证集,用于选择(人工训练)最优的超参数.因为验证集是用于选择超参数的,因此校验集和训练集是独立不重叠的. 测试集是用于在完成神经网络训练过程后,为了客观评价模型在其未见过(未曾影响普通参数和超参数选择)的数据上的性能,因此测试与验证集和训练集之间也是独立不重叠的,而且测试集不能提出对参数或者超参数的修改意见,只能作为评价网络性能的一个指标. 为了方便清楚直观的了解，上一个表格： 综述&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;至此,我们可以将神经网络完整的训练过程归结为一下两个步骤: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.训练普通参数.在训练集(给定超参数)上利用学习算法,训练普通参数,使得模型在训练集上的误差降低到可接受的程度(一般接近人类的水平). &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.’训练’超参数.在验证集上验证网络的generalization error(泛化能力),并根据模型性能对超参数进行调整. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;重复1和2两个步骤,直至网络在验证集上取得较低的generalization error.此时完整的训练过程结束.在完成参数和超参数的训练后,在测试集上测试网络的性能. 附言说到底： 验证集是一定需要的； 如果验证集具有足够泛化代表性，是不需要再整出什么测试集的； 整个测试集往往就是为了在验证集只是非训练集一个小子集的情况下，好奇一下那个靠训练集（训练）和验证集（调参）多次接力训练出来的模型是不是具有了泛化性能，因而加试一下图个确定。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://hpu-yz.github.io/tags/机器学习/"},{"name":"数据集","slug":"数据集","permalink":"https://hpu-yz.github.io/tags/数据集/"}]},{"title":"梯度下降法","slug":"梯度下降法的三种形式BGD、SGD、MBGD及python实现","date":"2019-07-17T16:00:00.000Z","updated":"2020-05-25T08:07:03.743Z","comments":true,"path":"2019/07/18/ti-du-xia-jiang-fa-de-san-chong-xing-shi-bgd-sgd-mbgd-ji-python-shi-xian/","link":"","permalink":"https://hpu-yz.github.io/2019/07/18/ti-du-xia-jiang-fa-de-san-chong-xing-shi-bgd-sgd-mbgd-ji-python-shi-xian/","excerpt":"","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;梯度下降法作为机器学习中较常使用的优化算法，其有着三种不同的形式：批量梯度下降（Batch Gradient Descent）、随机梯度下降（Stochastic Gradient Descent）以及小批量梯度下降（Mini-Batch Gradient Descent）。其中小批量梯度下降法也常用在深度学习中进行模型的训练。接下来，我们将对这三种不同的梯度下降法进行理解。 为了便于理解，这里我们将使用只含有一个特征的线性回归来展开。 此时线性回归的假设函数为：对应的目标函数（代价函数）即为： 下图为 J(θ0,θ1)与参数 θ0,θ1 的关系的图： 1、批量梯度下降（Batch Gradient Descent，BGD）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;批量梯度下降法是最原始的形式，它是指在每一次迭代时使用所有样本来进行梯度的更新。从数学上理解如下： （1）对目标函数求偏导： 其中 i=1,2,…,m 表示样本数， j=0,1 表示特征数，这里我们使用了偏置项 x(i)0=1。 （2）每次迭代对参数进行更新： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;注意这里更新时存在一个求和函数，即为对所有样本进行计算处理，可与下文SGD法进行比较。 伪代码形式为： 优点： （1）一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。 （2）由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，BGD一定能够得到全局最优。 缺点： （1）当样本数目 mm 很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。 从迭代的次数上来看，BGD迭代的次数相对较少。其迭代的收敛曲线示意图可以表示如下： 批量梯度下降法的python实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import matplotlib.pyplot as pltimport random##样本数据x_train = [150,200,250,300,350,400,600]y_train = [6450,7450,8450,9450,11450,15450,18450]#样本个数m = len(x_train)#步长alpha = 0.00001#循环次数cnt = 0#假设函数为 y=theta0+theta1*xdef h(x): return theta0 + theta1*xtheta0 = 0theta1 = 0#导数diff0=0diff1=0#误差error0=0 error1=0 #每次迭代theta的值retn0 = [] retn1 = [] #退出迭代的条件epsilon=0.00001#批量梯度下降while 1: cnt=cnt+1 diff0=0 diff1=0 #梯度下降 for i in range(m): diff0+=h(x_train[i])-y_train[i] diff1+=(h(x_train[i])-y_train[i])*x_train[i] theta0=theta0-alpha/m*diff0 theta1=theta1-alpha/m*diff1 retn0.append(theta0) retn1.append(theta1) error1=0 #计算迭代误差 for i in range(len(x_train)): error1 += ((theta0 + theta1 * x_train[i])-y_train[i]) ** 2 / 2 #判断是否已收敛 if abs(error1 - error0) &lt; epsilon: break else: error0 = error1# 画图表现plt.title('BGD')plt.plot(range(len(retn0)),retn0,label='theta0')plt.plot(range(len(retn1)),retn1,label='theta1')plt.legend() #显示上面的labelplt.xlabel('time')plt.ylabel('theta')plt.show()plt.plot(x_train,y_train,'bo')plt.plot(x_train,[h(x) for x in x_train],color='k',label='BGD')plt.legend()plt.xlabel('area')plt.ylabel('price')print(\"批量梯度下降法：theta0=&#123;&#125;,theta1=&#123;&#125;\".format(theta0,theta1))print(\"批量梯度下降法循环次数：&#123;&#125;\".format(cnt))plt.show() 2、随机梯度下降（Stochastic Gradient Descent，SGD）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&amp;nbsp;随机梯度下降法不同于批量梯度下降，随机梯度下降是每次迭代使用一个样本来对参数进行更新。使得训练速度加快。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于一个样本的目标函数为： （1）对目标函数求偏导： （2）参数更新： 注意，这里不再有求和符号 伪代码形式为： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;优点： （1）由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;缺点： （1）准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛。 （2）可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势。 （3）不易于并行实现。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;解释一下为什么SGD收敛速度比BGD要快：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;答：这里我们假设有30W个样本，对于BGD而言，每次迭代需要计算30W个样本才能对参数进行一次更新，需要求得最小值可能需要多次迭代（假设这里是10）；而对于SGD，每次更新参数只需要一个样本，因此若使用这30W个样本进行参数更新，则参数会被更新（迭代）30W次，而这期间，SGD就能保证能够收敛到一个合适的最小值上了。也就是说，在收敛时，BGD计算了 10×30W 次，而SGD只计算了 1×30W 次。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从迭代的次数上来看，SGD迭代的次数较少，在解空间的搜索过程看起来很盲目。其迭代的收敛曲线示意图可以表示如下： 随机梯度下降法的python实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import matplotlib.pyplot as pltimport random##样本数据x_train = [150,200,250,300,350,400,600]y_train = [6450,7450,8450,9450,11450,15450,18450]#样本个数m = len(x_train)#步长alpha = 0.00001#循环次数cnt = 0#假设函数为 y=theta0+theta1*xdef h(x): return theta0 + theta1*xtheta0 = 0theta1 = 0#导数diff0=0diff1=0#误差error0=0 error1=0 #每次迭代theta的值retn0 = [] retn1 = [] #退出迭代的条件epsilon=0.00001#随机梯度下降for i in range(1000): cnt=cnt+1 diff0=0 diff1=0 j = random.randint(0, m - 1) diff0=h(x_train[j])-y_train[j] diff1=(h(x_train[j])-y_train[j])*x_train[j] theta0=theta0-alpha/m*diff0 theta1=theta1-alpha/m*diff1 retn0.append(theta0) retn1.append(theta1) error1=0 #计算迭代的误差 for i in range(len(x_train)): error1 += ((theta0 + theta1 * x_train[i])-y_train[i]) ** 2 / 2 #判断是否已收敛 if abs(error1 - error0) &lt; epsilon: break else: error0 = error1# 画图表现 plt.title('SGD')plt.plot(range(len(retn0)),retn0,label='theta0')plt.plot(range(len(retn1)),retn1,label='theta1')plt.legend() #显示上面的labelplt.xlabel('time')plt.ylabel('theta')plt.show()plt.plot(x_train,y_train,'bo')plt.plot(x_train,[h(x) for x in x_train],color='k',label='SGD')plt.legend()plt.xlabel('area')plt.ylabel('price')print(\"随机梯度下降法：theta0=&#123;&#125;,theta1=&#123;&#125;\".format(theta0,theta1))print(\"随机梯度下降法循环次数：&#123;&#125;\".format(cnt))plt.show() 3、小批量梯度下降（Mini-Batch Gradient Descent, MBGD）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;小批量梯度下降，是对批量梯度下降以及随机梯度下降的一个折中办法。其思想是：每次迭代 使用 batch_size 个样本来对参数进行更新。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里我们假设 batchsize=10，样本数 m=1000 。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;伪代码形式为： 优点： （1）通过矩阵运算，每次在一个batch上优化神经网络参数并不会比单个数据慢太多。 （2）每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果。(比如上例中的30W，设置batch_size=100时，需要迭代3000次，远小于SGD的30W次) （3）可实现并行化。 缺点： （1）batch_size的不当选择可能会带来一些问题。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;batcha_size的选择带来的影响： （1）在合理地范围内，增大batch_size的好处： a. 内存利用率提高了，大矩阵乘法的并行化效率提高。 b. 跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。 c. 在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。 （2）盲目增大batch_size的坏处： a. 内存利用率提高了，但是内存容量可能撑不住了。 b. 跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。 c. Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下图显示了三种梯度下降算法的收敛过程： 4、总结Batch gradient descent: Use all examples in each iteration； Stochastic gradient descent: Use 1 example in each iteration； Mini-batch gradient descent: Use b examples in each iteration.","categories":[],"tags":[{"name":"机器学习 - 梯度下降","slug":"机器学习-梯度下降","permalink":"https://hpu-yz.github.io/tags/机器学习-梯度下降/"}]},{"title":"Python+离散数学→逻辑演算","slug":"python+离散数学→逻辑演算","date":"2019-04-01T16:00:00.000Z","updated":"2020-05-25T08:07:23.317Z","comments":true,"path":"2019/04/02/python-chi-san-shu-xue-luo-ji-yan-suan/","link":"","permalink":"https://hpu-yz.github.io/2019/04/02/python-chi-san-shu-xue-luo-ji-yan-suan/","excerpt":"","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本篇探讨的是一道逻辑演算推理题。有两种方法，一种是常规的离散数学逻辑演算，另一种则是用python程序来解决。本篇将探究两种方法： python+离散数学→逻辑演算。 问题&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在某次研讨会的中间休息时间，3名与会者根据王教授的口音对他是哪个省市的人判断如下：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;甲:王教授不是苏州人，是上海人&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;乙:王教授不是上海人，是苏州人&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;丙:王教授既不是上海人，也不是杭州人&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;听完这3人的判断后，王教授笑着说，你们3人中有一人说得全对，有一人说对了一半，另一人全不对。试用逻辑演算分析王教授到底是哪里人. 常规推理 设命题&nbsp;&nbsp;&nbsp;&nbsp;p:王教授是苏州人；&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;q:王教授是上海人；&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;r:王教授是杭州人； 用p,q,r表示甲乙丙的观点如下： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;甲：￢p∧q &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;乙：p∧￢q &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;丙：￢q∧￢r 其中一人全对，一人对一半，另一人全错。 即其中一个真命题，两个假命题。先找真命题 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;甲全对：B1=￢p∧q &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;甲对一半：B2=(￢p∧￢q) ∨(p∧q) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;甲全错：B3=p∧￢q &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;乙全对：C1= p∧￢q &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;乙对一半：C2=(￢p∧￢q) ∨(p∧q) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;乙全错：C3=￢p∧q &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;丙全对：D1=￢q∧￢r &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;丙对一半：D2=(q∧￢r) ∨( ￢q∧r) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;丙全错：D3=q∧r 有王教授那句话可以写： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;E=（B1∧C2∧D3）∨(B1∧C3∧D2) ∨(B2∧C1∧D3) ∨(B2∧C3∧D1) ∨(B3∧C1∧D2) ∨(B3∧C2∧D1) 是真命题 而B1∧C2∧D3⇔(￢p∧q) ∧((￢p∧￢q) ∨(p∧q)) ∧(q∧r) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;⇔(￢p∧q) ∧((￢p∧￢q) ∧(q∧r) ∨ (p∧q) ∧(q∧r)) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;⇔(￢p∧q) ∧(0∨(p∧q∧r)) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;⇔(￢p∧q)∧(p∧q∧r) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;⇔0 其他同理类似可得： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B1C3D2⇔￢p∧q∧￢r &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B2C1D3⇔0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B2C3D1⇔0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B3C1D2⇔p∧￢q∧r &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B3C2D1⇔0 所以E⇔(￢p∧q∧￢r) ∨(p∧￢q∧r) 而pqr中只能有一个是真的，所以p∧q⇔0，p∧r⇔0，q∧r⇔0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;E⇔(￢p∧q∧￢r) ∨0 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;⇔￢p∧q∧￢r &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;⇔1 所以p为假，q为真，r为假，王教授是上海人。 python程序实现&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以下是上述过程的代码实现，并附有代码详解，有兴趣的小伙伴可以看一看。 1234567891011121314151617181920212223242526272829303132333435for p in range(2): #p:王教授是苏州人 for q in range(2): #q:王教授是上海人 for r in range(2): #r:王教授是杭州人 #以下三行表示p,q,r不可能同时为真的情况 if (p == 1 and q == 1): continue if (p == 1 and r == 1): continue if (q == 1 and r == 1): continue Jia = (not p) and q #甲的判断 Yi = p and (not q) #乙的判断 Bing = (not q) and (not r) #丙的判断 B1 = (not p) and q #甲的判断全对 B2 = ((not p) and (not q))or (p and q) #甲的判断一半对 B3 = p and (not q) #甲的判断全错 C1 = p and (not q) #乙的判断全对 C2 = (p and q) or ((not p) and (not q)) #乙的判断一半对 C3 = (not p) and q #乙的判断全错 D1 = (not q) and (not r) #丙的判断全对 D2 = ((not q) and r) or (q and(not r)) #丙的判断一半对 D3 = q and r #丙的判断全错 #王教授所说的话 E = (B1 and C2 and D3) \\ or(B1 and C3 and D2) \\ or(B2 and C1 and D3) \\ or(B2 and C3 and D1) \\ or(B3 and C1 and D2) \\ or(B3 and C2 and D1) #符合王教授所的E值 if E==1 : print(\"%d,%d,%d E=%d,Jia=%d,Yi=%d,Bing=%d\"%(p,q,r,E,Jia,Yi,Bing)) 总结&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其实用python解决离散数学的逻辑推理题比直接推理效率更高而且准确性也高，并且在遇到复杂的逻辑推理时，很容易把自己绕晕，半天也解不出来，倒不如我们可以让计算机帮我们解决，只需写几十行代码，就可以罗列出复杂的关系，何乐而不为呢，所以我们可以多学学这种解题方法！😁","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://hpu-yz.github.io/tags/python/"}]},{"title":"while(scanf(\"%d\",&n)!=EOF)","slug":"OJ","date":"2019-03-20T16:00:00.000Z","updated":"2020-05-25T08:07:21.598Z","comments":true,"path":"2019/03/21/oj/","link":"","permalink":"https://hpu-yz.github.io/2019/03/21/oj/","excerpt":"","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;“测试输入包含若干测试实例。当N为0时，输入结束，该实例不被处理。”这句话 是最早我对OJ的印象 以前也没见过这种输入要求， 做第一道题的时候就卡住了 上网看别人的代码 都有一句 while(scanf(“%d”,&amp;n)!=EOF)”scanf 函数还能放while里啊… EOF是什么玩意儿呢…” 什么是OJ？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先先介绍下什么是OJ吧(知道的同学可以跳过此部分)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Online Judge系统（简称OJ）是一个在线的判题系统。用户可以在线提交程序多种程序（如C、C++）源代码，系统对源代码进行编译和执行，并通过预先设计的测试数据来检验程序源代码的正确性。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一个用户提交的程序在Online Judge系统下执行时将受到比较严格的限制，包括运行时间限制，内存使用限制和安全限制等。用户程序执行的结果将被Online Judge系统捕捉并保存，然后再转交给一个裁判程序。该裁判程序或者比较用户程序的输出数据和标准输出样例的差别，或者检验用户程序的输出数据是否满足一定的逻辑条件。最后系统返回给用户一个状态：通过（Accepted,AC）、答案错误(Wrong Answer,WA)、超时(Time Limit Exceed,TLE)、超过输出限制（Output Limit Exceed,OLE)、超内存（Memory Limit Exceed,MLE）、运行时错误（Runtime Error,RE）、格式错误（Presentation Error,PE)、或是无法编译（Compile Error,CE），并返回程序使用的内存、运行时间等信息。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Online Judge系统最初使用于ACM-ICPC国际大学生程序设计竞赛和OI信息学奥林匹克竞赛中的自动判题和排名。现广泛应用于世界各地高校学生程序设计的训练、参赛队员的训练和选拔、各种程序设计竞赛以及数据结构和算法的学习和作业的自动提交判断中。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;知名的OJ有：RQNOJ,URAL,SPOJ,vijos,tyvj,USACO,sgu,pku(poj),zju(toj),tju,uva，HDU(HDOJ)等。 EOF是什么东东？ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;EOF 是一个宏 ，一般定义为-1。1.EOF用来判断文件结束的标记(end of file) 用在文件操作中,可以查下msdn 看它的定义:EOF is returned by an I/O routine when the end-of-file (or in some cases, an error) is encountered2.EOF表示输入流的结束。3.在发送端套接字关闭后，接收端读套接字的read函数也会返回EOF. while(scanf()!=EOF)流程图 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们学过的有三种循环：for，while，do-while。但while (scanf(“%d”, &amp;n) != EOF)不像以上的任何一种，接下来看一下它的流程图： OnlineJuge的评判时，该语句的作用 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;OJ评判的原理应该是这样的：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;输入：通过管道命令，将一个包含若干测试用例的文件作&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为【标准输入流】，所以需要while(scanf() != EOF)来判断测试文件是否读完。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;输出：通过管道命令，将【标准输出流】，输出到一个文件中。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;评判：将程序的【输出文件】与【正确答案文件】进行比对：如果一样，则返回程序正确提示；否则返回程序错误的提示。 使用原理 有人说 EOF等于-1 其实就是scanf函数的返回值 不等于-1时 继续进行循环 有人说 EOF是处理到文件结束 不用EOF也可以 ………. 很多天后终于把这个弄明白了 scanf的返回值由后面的参数决定 scanf(“%d%d”, &amp;a, &amp;b); 如果a和b都被成功读入，那么scanf的返回值就是2 如果只有a被成功读入，返回值为1 如果a和b都未被成功读入，返回值为0 如果遇到错误或遇到end of file，返回值为EOF，且返回值为int型. 总结 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以上是我个人所述，若有什么不对的地方或者有补充的地方，还希望各位指点指点，大家可以一起学习，一起进步！","categories":[],"tags":[{"name":"OJ","slug":"OJ","permalink":"https://hpu-yz.github.io/tags/OJ/"}]},{"title":"Python判断合式公式","slug":"Python判断离散数学的合式公式","date":"2019-03-16T16:00:00.000Z","updated":"2020-05-25T08:07:37.558Z","comments":true,"path":"2019/03/17/python-pan-duan-chi-san-shu-xue-de-he-shi-gong-shi/","link":"","permalink":"https://hpu-yz.github.io/2019/03/17/python-pan-duan-chi-san-shu-xue-de-he-shi-gong-shi/","excerpt":"","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;你没有听错，用python程序来解决离散数学的逻辑推理问题，我当我第一次听老师说的时候也很吃惊（再说上学期的Python学的也不咋地…..😩)，但经老师讲解后才知道，使用python解题不仅效率高，而且准确性也很强的，所以还是有必要学习一下的。 判断是否为合式公式&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第一个题目就是判断一个公式是否为合式公式，这也是最基础的，因为只有当在输入的合式公式正确的情况下，才能进一步的运算解题，所以先讲解一下怎么判断合式公式。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先要知道什么是合式公式？(1)原子命题常项或变项是合式公式；(2)如果A是合式公式，则（-A）也是合式公式（- 表示非）；(3)如果A，B是合式公式，则（AB）、（A+B）、（A &lt; B）、（ A ~ B）也是合式公式；(此处 合取 + 析取 &lt; 代表条件 ~ 代表双条件)(4)只有有限次地应用(1)～(3)所包含的命题变元，联结词和括号的符号串才是合式公式。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;个人思路：输入字符串，扫描字符串，把所含的各关联词分区出来，在判断每个关联词使用是否正确比如不合规则的情况：(1) 关联词所处位置不对(2) 关联词的连续使用(3) 括号不匹配(4) ……..程序中可能会有bug，希望大佬们多多指教 这道题应该有很多好的方法，但我不太会用，我这里只能暴力判断了。废话不多说，直接上代码😎12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788import reprint(\"定义‘!’为否定联结词\")print(\"定义‘&amp;’为合取联结词\")print(\"定义‘-’为析取联结词\")print(\"定义‘&gt;’为蕴含联结词\")print(\"定义‘~’为等价联结词\")print(\"如果公式错误，会指出哪里错误；如果公式正确，则什么也不输出\")S =input(\"请输入需要判断的公式：\")T = list(S)if S[0] in ['&amp;','-',')','~','&gt;'] : print(\"不是合式公式\")a ,b,c,d,f =\"!\" , \"&amp;\" ,\"-\" , \"&gt;\" , \"~\"if a in T: Q = [m.start() for m in re.finditer(a, S)] for i in Q : if S[-1] == '!' : print (\"‘!’联结词使用错误，不是合式公式\") break if S[i+1] in ['&amp;','!','-',')','~','&gt;']: print(\"‘!’联结词使用错误，不是合式公式\") break if S[i-1] in ['!']: print(\"‘!’联结词使用错误，不是合式公式！\") breakif b in T: W = [m.start() for m in re.finditer(b, S)] for y in W : if S[-1] == '&amp;' : print (\"‘&amp;’联结词使用错误，不是合式公式\") break if S[y+1] in ['-',')','~','&gt;','&amp;'] : print(\"‘&amp;’联结词使用错误，不是合式公式\") break if S[y-1] in ['-','(','~','&gt;','&amp;'] : print(\"‘&amp;’联结词使用错误，不是合式公式\") breakif c in T: E = [m.start() for m in re.finditer(c, S)] for h in E : if S[-1] == '-' : print(\"‘-’联结词使用错误，不是合式公式\") break if S[h+1] in (')','&gt;','~','-') : print(\"‘-’联结词使用错误，不是合式公式\") break if S[h-1] in ('&amp;','-','&gt;','~') : print(\"‘-’联结词使用错误，不是合式公式\") breakif d in T: R = [m.start() for m in re.finditer(d, S)] for k in R : if S[-1] == '&gt;' : print (\"‘&gt;’联结词使用错误，不是合式公式\") break if S[k+1] in [')','&gt;'] : print(\"‘&gt;’联结词使用错误，不是合式公式\") break if S[k-1] in ['(','&gt;'] : print(\"‘&gt;’联结词使用错误，不是合式公式\") breakif f in T: O = [m.start() for m in re.finditer(f, S)] for v in O : if S[-1] == '~' : print(\"‘~’联结词使用错误，不是合式公式\") break if S[v+1] in [')','~'] : print(\"‘~’联结词使用错误，不是合式公式\") break if S[v-1] in ['(','~'] : print(\"‘~’联结词使用错误，不是合式公式\") breakif '(' or ')' in S : e = 0 klb =[] for i in S : if i == '(': klb.append(i) if i == ')': if len(klb)==0: e=1 break else : klb.pop() if len(klb)!=0 : e = 1 if e==1: print(\"括号使用错误，不是合式公式\") &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我也是小白，程序哪里可能有些不足之处，请多多指教，如果你有更好的解题方法或思路，可以联系我，大家可以一起学习，一起进步的！😊","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://hpu-yz.github.io/tags/python/"}]},{"title":"Hexo博客搭建","slug":"Hexo博客搭建","date":"2019-03-14T16:00:00.000Z","updated":"2020-05-25T08:07:12.113Z","comments":true,"path":"2019/03/15/hexo-bo-ke-da-jian/","link":"","permalink":"https://hpu-yz.github.io/2019/03/15/hexo-bo-ke-da-jian/","excerpt":"","text":"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本篇文章用于介绍Hexo个人博客的搭建过程，这也是我搭建本博客后的第一篇文章，分享一下搭建方法，有兴趣的小伙伴也可以自主搭建一个属于自己的博客！ 首先感谢大家的来访支持！&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;自己本来开始做的是html网页，网页做完之后需要购买服务器和域名（价格不便宜）才能运营，还要定期维护它，对于我个人只是感兴趣做个自己的网页，单单做个html网页就很费力气了，实在是没有精力和时间运营它，所以也没有必要去购买服务器和域名（性价比低），但我做的html网页在我自己的电脑上还是可以运营的（感兴趣的小伙伴可以联系我，一起探讨做html网页的方法）。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其次，现在市面上的博客有很多，如CSDN，博客园等平台，可以直接在上面发表，也有很多优点，但缺点是比较不自由，会受到各种限制和恶心的广告。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所以我选择了直接在github page平台上托管我的博客，这样可以安心写作，也不需要定期维护，而且hexo作为一个快速简洁的博客框架，用它来搭建博客也很简单。我在两天内完成了Hexo-Github个人博客的基本搭建（其实搭建不难，自己脑子笨，做的慢些了……）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在这里记录一下我的第搭建一个博客，并给大家分享一下搭建的流程，以及在搭建过程中遇到的问题和解决方案。如果你也有搭建个人博客的想法，希望能给你一点经验帮助。 大家都是新手，可以分享经验，互相交流学习的。 搭建经历 ❤2019-3-13：初次看到Hexo-Github搭建个人博客，很感兴趣，便开始了自己博客的搭建； ❤2019-3-14：完成了博客的基本搭建（可以运营），并上传了第一篇文章； ❤2019-3-17：博客搭建工作全部完结（主题优化，程序添加，分类管理……）; ❤未完待续…… 搭建步骤1. 安装Git 2. 安装Node.js 3. 安装Hexo 4. 注册Github账号并创建新仓库 5. 生产SSH添加到Github 6. 将Hexo部署到Github 7. 修改主题 8. 优化主题 1.安装Git&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;直接到Git官网下载就行了，右键点击软件使用Git Bash的命令行工具，以后就用这个工具来使用Git 2.安装Node.js&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;安装同Git一样，也是到Node.js官网下载就可以了。 3.安装Hexo&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;使用下载好的Git Bash来下载Hexo，代码如下： 1npm install hexo-cli -g 4.注册Github账号并创建一个新的仓库&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Github是一个很好的开源项目托管平台，大部分人应该都注册的有账号（有账号的小伙伴可以跳过这步），即使没有注册过也没有关系，只需现在注册下就行了，请参考Github基础设置及使用详解，里面的有详细的注册过程,过程也很简单。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;注册过账号后，先创建一个新的仓库（创建的具体方法上面的那个网站也有详细过程，但这里需要注意的是，创建的仓库名一定要是 name.github.io,其中name为你注册的Github的用户名，修改成你自己的。 5.生成SSH添加到Github&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先配置SSH，同样使用Git Bash,输入一下代码： 1ssh-keygen -t rsa -C \"邮件地址\" &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中”邮件地址”是你注册Github时填写的邮箱（输入时双引号要带上），输入后要连续按回车键，再等待回应。它会回复： 1Enter file in which to save the key (/c/Users/lenovo/.ssh/id_rsa): &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;然后进入(/c/Users/lenovo/.ssh/id_rsa.pub),将里面的内容复制下来，再进入Github官网，到GitHub设置-&gt;SSH and GPG keys-&gt;New SSH key，粘贴此处并确定。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;再回到Git Bash,输入一下代码： 1ssh -T git@github.com &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用于测试SSH是否配置好，看到You&#39;ve successfully authenticated, but GitHub does not provide shell access.则说明配置好了，否则无法使用hexo d。 6. 将Hexo部署到Github&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;初始化Hexo12hexo init Blog cd Blog &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Blog可修该城其他名字；cd Blog 指打开Blog文件夹。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;安装Hexo扩展12npm install hexo-deployer-git --savenpm install &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本地调试123hexo cleanhexo g hexo s &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;hexo s是开启本地预览服务，打开浏览器访问 http://localhost:4000 即可看到内容。 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上传到Github Pages&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上传之前，打开站点配置文件_config.yml，位于站点根目录下，修改最后一部分为如下部分。 1234deploy: type: git repository: git@github.com:name/name.github.io.git #name修改为你的Github用户名 branch: master &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上传到Github 123hexo cleanhexo g hexo d 7.修改主题&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主题位置在themes目录下，默认为landscape主题。可从官网https://hexo.io/themes/选择各种下载（得看个人喜好了），推荐NexT主题，以下为安装方法。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;安装过程就一行代码，站点根目录下运行。 1git clone https://github.com/theme-next/hexo-theme-next themes/next &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;然后打开站点配置文件_config.yml，找到theme： landscape，把landscape修改为next就可以了。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;到现在你就已经搭建好了一个属于你自己的博客平台了，赶紧去熟悉熟悉它吧。👍 8.主题优化&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其实刚才我们已经搭建好了，并且也可以使用了，但我们仍可以使我们的博客变得更加好看吸引人，以及添加更多的小功能，使我们有更好的体验，也就是进行主题优化。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;但由于主题优化内容比较多，所以在这就先不讲解了，不用担心，我会在后续的博客文章中持续更新关于如何进行主题优化，使我们的博客变得“高大上”。🤞","categories":[],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://hpu-yz.github.io/tags/Hexo/"},{"name":"博客搭建","slug":"博客搭建","permalink":"https://hpu-yz.github.io/tags/博客搭建/"}]}],"categories":[],"tags":[{"name":"机器学习 - A-Star","slug":"机器学习-A-Star","permalink":"https://hpu-yz.github.io/tags/机器学习-A-Star/"},{"name":"python","slug":"python","permalink":"https://hpu-yz.github.io/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://hpu-yz.github.io/tags/爬虫/"},{"name":"生活","slug":"生活","permalink":"https://hpu-yz.github.io/tags/生活/"},{"name":"动态规划","slug":"动态规划","permalink":"https://hpu-yz.github.io/tags/动态规划/"},{"name":"机器学习","slug":"机器学习","permalink":"https://hpu-yz.github.io/tags/机器学习/"},{"name":"朴素贝叶斯算法","slug":"朴素贝叶斯算法","permalink":"https://hpu-yz.github.io/tags/朴素贝叶斯算法/"},{"name":"分类","slug":"分类","permalink":"https://hpu-yz.github.io/tags/分类/"},{"name":"聚类","slug":"聚类","permalink":"https://hpu-yz.github.io/tags/聚类/"},{"name":"回归","slug":"回归","permalink":"https://hpu-yz.github.io/tags/回归/"},{"name":"深度学习","slug":"深度学习","permalink":"https://hpu-yz.github.io/tags/深度学习/"},{"name":"卷积神经网络（CNN）","slug":"卷积神经网络（CNN）","permalink":"https://hpu-yz.github.io/tags/卷积神经网络（CNN）/"},{"name":"协同过滤（CF）","slug":"协同过滤（CF）","permalink":"https://hpu-yz.github.io/tags/协同过滤（CF）/"},{"name":"推荐系统","slug":"推荐系统","permalink":"https://hpu-yz.github.io/tags/推荐系统/"},{"name":"人工神经网络（ANN）","slug":"人工神经网络（ANN）","permalink":"https://hpu-yz.github.io/tags/人工神经网络（ANN）/"},{"name":"相似度","slug":"相似度","permalink":"https://hpu-yz.github.io/tags/相似度/"},{"name":"矩阵分解","slug":"矩阵分解","permalink":"https://hpu-yz.github.io/tags/矩阵分解/"},{"name":"KNN","slug":"KNN","permalink":"https://hpu-yz.github.io/tags/KNN/"},{"name":"K-means","slug":"K-means","permalink":"https://hpu-yz.github.io/tags/K-means/"},{"name":"数据集","slug":"数据集","permalink":"https://hpu-yz.github.io/tags/数据集/"},{"name":"机器学习 - 梯度下降","slug":"机器学习-梯度下降","permalink":"https://hpu-yz.github.io/tags/机器学习-梯度下降/"},{"name":"OJ","slug":"OJ","permalink":"https://hpu-yz.github.io/tags/OJ/"},{"name":"Hexo","slug":"Hexo","permalink":"https://hpu-yz.github.io/tags/Hexo/"},{"name":"博客搭建","slug":"博客搭建","permalink":"https://hpu-yz.github.io/tags/博客搭建/"}]}