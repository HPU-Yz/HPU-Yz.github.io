<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Best-yz"><title>Python爬虫~豆瓣电影排行榜 · Best-yz</title><meta name="description" content="&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;本篇讲介绍一个简单的Python爬虫案例–爬取豆瓣 TOP250 电影排行榜。
很多朋友在看一部电影前都喜欢先找一下网友们对该片的评价。
说到电影评分的网站，除了国外的 IMDB 和烂番茄，国内要数豆瓣最"><meta name="keywords" content="Flaneur博客，Best-yz博客"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script src="/js/jquery.js"></script><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="https://cdn.jsdelivr.net/gh/HPU-Yz/cdn-speed/medias/logo@2x.png" style="width:127px;"><h3 title><a href="/">Best-yz</a></h3><div class="description"><p>三分热度 <br> Nothing is impossible to a willing heart.</p></div></div></div><ul class="social-links"><li><a href="https://github.com/HPU-Yz"><i class="fa fa-github"></i></a></li><li><a href="mailto:2426545812@qq.com"><i class="fa fa-envelope"></i></a></li><li><a href="http://sighttp.qq.com/authd?IDKEY=da97a3288889b7c35a1e6d914e0cb28bda9a83ec1e77cf7b"><i class="fa fa-qq"></i></a></li></ul><div class="footer"><div class="p"> <span>© 2019 - 2020 </span><i class="fa fa-star"></i><span> Best-yz</span></div><div class="by_farbox"><span>Powered by </span><a href="https://hexo.io/zh-cn/" target="_blank">Hexo </a><span> </span><a href="http://www.beian.miit.gov.cn/" target="_blank">&nbsp;</a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">首页</a></li><li><a href="/archives">归档</a></li><li><a href="/tags">标签</a></li><li><a href="/about">关于</a></li><li><a href="/guestbook">留言</a></li><li><a href="/links">友链</a></li><li><a href="/shuoshuo">说说</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>Python爬虫~豆瓣电影排行榜</a></h3></div><div class="post-content"><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本篇讲介绍一个简单的Python爬虫案例–爬取豆瓣 TOP250 电影排行榜。</p>
<p>很多朋友在看一部电影前都喜欢先找一下网友们对该片的评价。</p>
<p>说到电影评分的网站，除了国外的 IMDB 和烂番茄，国内要数豆瓣最为出名。</p>
<p>主要原因是豆瓣有一套完整的评分和防水军机制 。</p>
<p>在这套机制下，豆瓣评分高的电影不一定是所有人都喜欢的，但是豆瓣评分低的电影，一定是实打实的烂片！</p>
<p>虽然每个人的喜好偏爱不同，但通常豆瓣评分 8 分以上的电影，都是值得一看的。</p>
<p>豆瓣还专门提供了一个 TOP250 的电影链接 -&gt; <a href="https://movie.douban.com/top250" target="_blank" rel="noopener">https://movie.douban.com/top250</a></p>
<h1 id="爬取思路"><a href="#爬取思路" class="headerlink" title="爬取思路"></a>爬取思路</h1><p>爬取的过程很好理解，这里只需要两个过程：<br>① 从服务器上下载所需页面<br>② 解析这个页面，得到自己需要有用的内容</p>
<h3 id="①抓取页面"><a href="#①抓取页面" class="headerlink" title="①抓取页面"></a>①抓取页面</h3><p>有的人可能会利用 urllib 模块实现网络抓取功能。但在 Python 中，有一个更好地替代者——Requests。Requests 简化了 urllib 的诸多冗杂且无意义的操作，并提供了更强大的功能。<br>所以在这里我们使用 Requests 模块的 get() 方法从服务器上来下载这个页面。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">"https://movie.douban.com/top250"</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">"User-Agent"</span>:<span class="string">"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3314.0 Safari/537.36 SE 2.X MetaSr 1.0"</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">res = requests.get(url,headers=headers)</span><br></pre></td></tr></table></figure>
<p>res就是我们需要的这个页面的资源，我们不妨打开来看看是不是</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">"豆瓣电影.txt"</span>,<span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(res.text)</span><br></pre></td></tr></table></figure>
<p>打开文本如下图<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9zbG9tb28ub3NzLWNuLWJlaWppbmcuYWxpeXVuY3MuY29tLzJkZTI1MDMzZTQ0NGY3NTc2ZTRlNDZlYmU2Mjg1YzllLm1kLnBuZw?x-oss-process=image/format,png" alt><br>我们可以看出这确实是当前网页的资源，所以我们就抓取成功了。</p>
<h2 id="②解析页面"><a href="#②解析页面" class="headerlink" title="②解析页面"></a>②解析页面</h2><p>解析网页内容推荐使用 BeautifulSoup 模块，它可以化腐朽为神奇，将一个复杂的网页结构转化为书籍目录的形式供你浏览。<br>例如，我们现在需要解析提取出当前页面的电影名字</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> bs4</span><br><span class="line">soup = bs4.BeautifulSoup(res.text,<span class="string">"html.parser"</span>)</span><br><span class="line">targets = soup.find_all(<span class="string">"div"</span>,class_=<span class="string">"hd"</span>)</span><br><span class="line"><span class="keyword">for</span> each <span class="keyword">in</span> targets:</span><br><span class="line">    print(each.a.span.text)</span><br></pre></td></tr></table></figure>
<p>可以得到如下结果</p>
<blockquote>
<p>肖申克的救赎<br>霸王别姬<br>阿甘正传<br>这个杀手不太冷<br>美丽人生<br>泰坦尼克号<br>千与千寻<br>辛德勒的名单<br> 盗梦空间<br> 忠犬八公的故事<br> 海上钢琴师<br>机器人总动员<br>三傻大闹宝莱坞<br>楚门的世界<br>放牛班的春天<br> 星际穿越<br> 大话西游之大圣娶亲<br> 熔炉<br> 疯狂动物城<br> 无间道<br> 龙猫<br> 教父<br> 当幸福来敲门<br>怦然心动<br>触不可及</p>
</blockquote>
<p>这里你可能就会有疑问，这些数据是怎么得来的呢？<br>我们先来看下 HTML 源代码：<br><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9zbG9tb28ub3NzLWNuLWJlaWppbmcuYWxpeXVuY3MuY29tL2E0MDcxYzNhNmYyZDUzMjVjMWJhYzJhMDQ4OTQ2Zjk4Lm1kLnBuZw?x-oss-process=image/format,png" alt><br>发现每个电影的标题都是位于 <code>&lt;div class=&quot;hd&quot;&gt;...&lt;/div&gt;</code> 标签中的，它的从属关系是：<code>div -&gt; a -&gt; span</code>。</p>
<p>所以我们先调用 find_all() 方法，找到所有 class=”hd” 的 div 标签，然后按照从属关系即可直接取出电影名。<br>同理，我们借用此发方法来解析提取出电影的评分、介绍等需要的信息。</p>
<h2 id="附加问题"><a href="#附加问题" class="headerlink" title="附加问题"></a>附加问题</h2><p>我们刚才解析提取的仅仅是第一页的页面，那么还有第二、第三、第四页……呢？</p>
<p>其实，解决起来也很简单，我们可以使用for循环来对每一页进行上述的两个过程。</p>
<p>但，我们此时又有新的问题，我们不可能每抓取一次，就重新输入下一网页的链接地址，这样很麻烦，效率也不高。</p>
<p> 我们可以分析每一页的链接：</p>
<blockquote>
<p> 第一页：<a href="https://movie.douban.com/top250" target="_blank" rel="noopener">https://movie.douban.com/top250</a><br>第二页：<a href="https://movie.douban.com/top250?start=25" target="_blank" rel="noopener">https://movie.douban.com/top250?start=25</a><br>第三页：<a href="https://movie.douban.com/top250?start=50" target="_blank" rel="noopener">https://movie.douban.com/top250?start=50</a><br>第四页：<a href="https://movie.douban.com/top250?start=75" target="_blank" rel="noopener">https://movie.douban.com/top250?start=75</a><br>第五页：<a href="https://movie.douban.com/top250?start=100" target="_blank" rel="noopener">https://movie.douban.com/top250?start=100</a><br> … …<br> …. …</p>
</blockquote>
<p>我们可以发现这样的规律：<br>每一次的更新的 <code>url = https://movie.douban.com/top250 + &#39;/?start=&#39; + str(25*i)</code>其中<code>i</code>可以表示为页数-1</p>
<p>咦，这个时候，你可能会有疑问，我们怎么知道一共有多少页呢，不能一直for循环无穷吧。</p>
<p>那当然不可能的了，我们可以按第二步解析网页方式来获取页数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">depth = soup.find(<span class="string">'span'</span>,class_=<span class="string">'next'</span>).previous_sibling.previous_sibling.text</span><br></pre></td></tr></table></figure>
<p>注意，这个返回的<code>depth</code>是给字符串形式，需要<code>int()</code></p>
<p>这样结合刚才的过程，就可以迭代每一页了</p>
<h1 id="代码清单"><a href="#代码清单" class="headerlink" title="代码清单"></a>代码清单</h1><p>感兴趣的话，可以试一试哦</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> bs4</span><br><span class="line"></span><br><span class="line"><span class="comment">#抓取网页</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">open_url</span><span class="params">(url)</span>:</span></span><br><span class="line">    headers = &#123;<span class="string">"User-Agent"</span>:<span class="string">"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3314.0 Safari/537.36 SE 2.X MetaSr 1.0"</span>&#125;</span><br><span class="line">    res = requests.get(url,headers=headers)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="comment">#得到总页数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_depth</span><span class="params">(res)</span>:</span></span><br><span class="line">    soup = bs4.BeautifulSoup(res.text,<span class="string">'html.parser'</span>)</span><br><span class="line">    depth = soup.find(<span class="string">'span'</span>,class_=<span class="string">'next'</span>).previous_sibling.previous_sibling.text</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> int(depth)</span><br><span class="line"></span><br><span class="line"><span class="comment">#解析网页，提取内容</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_movies</span><span class="params">(res)</span>:</span></span><br><span class="line">    soup = bs4.BeautifulSoup(res.text,<span class="string">'html.parser'</span>)</span><br><span class="line"></span><br><span class="line">    names = []</span><br><span class="line">    target = soup.find_all(<span class="string">'div'</span>,class_=<span class="string">'hd'</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> target:</span><br><span class="line">        names.append(i.a.span.text)</span><br><span class="line"></span><br><span class="line">    ranks = []</span><br><span class="line">    target = soup.find_all(<span class="string">'span'</span>,class_=<span class="string">'rating_num'</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> target:</span><br><span class="line">        ranks.append(i.text)</span><br><span class="line"></span><br><span class="line">    messages = []</span><br><span class="line">    target = soup.find_all(<span class="string">'div'</span>,class_=<span class="string">'bd'</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> target:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            messages.append(i.p.text.split(<span class="string">'\n'</span>)[<span class="number">1</span>].strip() + i.p.text.split(<span class="string">'\n'</span>)[<span class="number">2</span>].strip())</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    result = []</span><br><span class="line">    length = len(names)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(length):</span><br><span class="line">        result.append(names[i] + <span class="string">','</span> +ranks[i] + <span class="string">','</span> + messages[i] + <span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    host = <span class="string">"https://movie.douban.com/top250"</span></span><br><span class="line">    res = open_url(host)</span><br><span class="line">    depth = find_depth(res)</span><br><span class="line"></span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(depth):</span><br><span class="line">        url = host + <span class="string">'/?start='</span> + str(<span class="number">25</span>*i)</span><br><span class="line">        res = open_url(url)</span><br><span class="line">        result.extend(find_movies(res))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"豆瓣TOP250电影.txt"</span>,<span class="string">'w'</span>,encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> each <span class="keyword">in</span> result:</span><br><span class="line">            f.write(each)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2020-03-20</span><i class="fa fa-tag"></i><a class="tag" href="/tags/python/" title="python">python </a><a class="tag" href="/tags/爬虫/" title="爬虫">爬虫 </a><span class="leancloud_visitors"></span></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href onclick="javascript:join_favorite()" ref="sidebar"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" href="http://twitter.com/home?status=,https://hpu-yz.github.io/2020/03/20/python-pa-chong-pa-qu-dou-ban-top250-dian-ying-pai-xing-bang/,Best-yz,Python爬虫~豆瓣电影排行榜,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2020/04/24/a-star-suan-fa-qiu-jie-n-shu-ma-wen-ti/" title="A-Star算法求解N数码问题">上一篇</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2020/03/18/xiao-wang-zi-zheng-yin-wei-ni-wei-ni-de-mei-gui-hua-fei-liao-shi-jian-zhe-cai-shi-ni-de-mei-gui-bian-de-ru-ci-chong-yao/" title="【小王子】摘录">下一篇</a></li></ul></div><script src="/js/visitors.js"></script><a id="comments"></a><div id="vcomments" style="margin:0 30px;"></div><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//cdn.jsdelivr.net/gh/xcss/valine@latest/dist/Valine.min.js"></script><script>var valine = new Valine({
  el:'#vcomments',
  notify:false || false, 
  verify:false|| false, 
  app_id:'ScFNhrKDxbwGDnwPjE6xGCTm-gzGzoHsz',
  app_key:'Lwnq5xfD0M2LosaGSeDSFeqi',
  placeholder:'念念不忘，必有回响...',
  path: window.location.pathname,
  serverURLs: '',
  visitor:true,
  recordIP:true,
  avatar:'monsterid'
})</script></div></div></div></div><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/add-bookmark.js"></script><script src="/js/baidu-tongji.js"></script></body></html>